{
 "metadata": {
  "name": "",
  "signature": "sha256:d06c83707e2480c127c1863efe0ffbcb1417410b0a10f1022011263234b6b3a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\"\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 7 #7 is best\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 2\n",
      "PCT_VALIDATION  = 0.2\n",
      "MIN_FEAT_FREQ = 5     #15 best so far\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\"\n",
      "STEM = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Dropbox/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            elif w == \"&\":\n",
      "                cw = \"and\"\n",
      "            else:\n",
      "                cw = corrector.correct(w)\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "                sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "235 files found\n",
        "235 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "cor_srtd = sort_by_value(corrections, reverse = True)\n",
      "cor_srtd[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[((\"it's\", 'it'), 46),\n",
        " ((\"don't\", 'dont'), 30),\n",
        " ((\"that's\", 'that'), 28),\n",
        " (('algea', 'algae'), 20),\n",
        " (('zox', 'zo'), 20),\n",
        " ((\"world's\", 'world'), 17),\n",
        " ((\"can't\", 'cant'), 14),\n",
        " (('&', 'and'), 12),\n",
        " (('bleaches', 'bleached'), 12),\n",
        " (('cloral', 'coral'), 10),\n",
        " ((\"coral's\", 'coral'), 9),\n",
        " ((\"isn't\", 'isnt'), 9),\n",
        " ((\"won't\", 'wont'), 9),\n",
        " ((\"they're\", 'there'), 9),\n",
        " (('tempature', 'temperature'), 8),\n",
        " ((\"doesn't\", 'doesnt'), 8),\n",
        " (('tempeture', 'temperature'), 7),\n",
        " (('varys', 'vary'), 6),\n",
        " (('saclike', 'saline'), 6),\n",
        " ((\"there's\", 'there'), 5)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Single char words \"\"\"\n",
      "wds = [(w,f) for w,f in wd_sent_freq.items() if len(w.strip()) == 1 and not w[0].isalpha()]\n",
      "print \"\\n\".join(map(str,sorted(wds, key = lambda (w,f): -f)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('.', 2079)\n",
        "(',', 511)\n",
        "('-', 76)\n",
        "('/', 76)\n",
        "('\"', 71)\n",
        "('?', 62)\n",
        "('3', 34)\n",
        "('(', 29)\n",
        "(')', 28)\n",
        "('%', 26)\n",
        "('\\xc2', 24)\n",
        "('\\xb0', 24)\n",
        "('1', 16)\n",
        "('\\\\', 16)\n",
        "('5', 13)\n",
        "('\\x80', 11)\n",
        "('\\xe2', 11)\n",
        "(';', 10)\n",
        "('+', 6)\n",
        "('2', 6)\n",
        "(\"'\", 6)\n",
        "('\\x93', 6)\n",
        "('\\x99', 5)\n",
        "('0', 4)\n",
        "('!', 4)\n",
        "(':', 4)\n",
        "('9', 2)\n",
        "('=', 2)\n",
        "('6', 1)\n",
        "('7', 1)\n",
        "('8', 1)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "VALID_CHARS = {\".\", \"?\", \"!\", \"=\", \"/\", \":\", \";\", \"&\", \"+\",  \"-\", \"=\",  \"%\", \"'\", \",\", \"\\\\\", \"(\", \")\", \"\\\"\"}\n",
      "\"\"\" Remove bad chars (see above - e.g. '\\x93') \"\"\"\n",
      "removed = set()\n",
      "def valid_wd(wd):\n",
      "    wd = wd.strip()\n",
      "    if len(wd) != 1:\n",
      "        return True\n",
      "    if wd in removed:\n",
      "        return False\n",
      "    if wd.isalpha() or wd.isdigit() or wd in VALID_CHARS:\n",
      "        return True\n",
      "    removed.add(wd)\n",
      "    return False\n",
      "    \n",
      "def filterout_punctuation(sentence):\n",
      "    return filter(lambda (w, tags4word): valid_wd(w), sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "sentences = []\n",
      "\n",
      "i = 0\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        modified_sentence = filterout_punctuation(modified_sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size= WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        sentences.append(sentence)\n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        i += 1\n",
      "        \n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Removed Characters\n",
      "------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(removed))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract Features \"\"\"\n",
      "from WindowFeatures import extract_positional_word_features, extract_word_features\n",
      "from NgramGenerator import compute_ngrams\n",
      "\n",
      "def extract_skip_b4_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[:mid_ix]:\n",
      "        feats[\"_BEFORE: \" + wd + \"|\" + target] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_skip_after_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[mid_ix+1:]:\n",
      "        feats[\"_AFTER: \" + target + \"|\" + wd] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_positional_bigram_features(window, mid_ix, feature_val = 1):\n",
      "    bi_grams = compute_ngrams(window, max_len = 2, min_len = 2)\n",
      "    d = {}\n",
      "    for i, bi_gram in enumerate(bi_grams):\n",
      "        d[\"BI\" + \":\" + str(-mid_ix + i) + \" \" + bi_gram[0] + \" | \" + bi_gram[1]] = feature_val\n",
      "    return d\n",
      "\n",
      "def extract_positional_skip_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for i, wd in enumerate(window):\n",
      "        if i == mid_ix:\n",
      "            continue\n",
      "        a,b = wd,target\n",
      "        if i > mid_ix:\n",
      "            a,b = b,a\n",
      "        feats[\"SKIP:\" + str(-mid_ix + i) + \" \" + a + \" | \" + b] = feature_val\n",
      "    return feats\n",
      "\n",
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "idgen = IdGenerator()\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def extract_features(words):\n",
      "    \n",
      "    if STEM:\n",
      "        words = [stemmer.stem(w) for w in words]\n",
      "    #Extract features for words\n",
      "    \n",
      "    \"\"\" Try only middle word \"\"\"\n",
      "    features = {}\n",
      "    ###\n",
      "    pos_features = extract_positional_word_features(words, MID_IX, feature_val=1)    \n",
      "    word_features  = extract_word_features(words, feature_val=1)\n",
      "    \n",
      "    #DO NOT HELP\n",
      "    #b4_features    = extract_skip_b4_word_features(words, MID_IX, feature_val=1)\n",
      "    #after_features = extract_skip_after_word_features(words, MID_IX, feature_val=1)\n",
      "    #pos_skip_grams = extract_positional_skip_word_features(words, MID_IX,  feature_val = 1)\n",
      "    pos_bi_grams = extract_positional_bigram_features(words, MID_IX, feature_val = 1)\n",
      "\n",
      "    features.update(pos_features)\n",
      "    features.update(word_features)\n",
      "    #features.update(b4_features)\n",
      "    #features.update(after_features)\n",
      "    #features.update(pos_skip_grams)\n",
      "    features.update(pos_bi_grams)\n",
      "    return features.items()\n",
      "\n",
      "def extract_ys_by_code(tags, ysByCode):\n",
      "    for code in all_codes:\n",
      "        ysByCode[code].append(1 if code in tags else 0 )    \n",
      "\n",
      "ix2ys = {}\n",
      "ix2feats = {}\n",
      "feat_counts = defaultdict(int)\n",
      "def tally_features(feats):\n",
      "    for k,v in feats:\n",
      "        feat_counts[k] += 1\n",
      "\n",
      "for i, windows in ix2windows.items():\n",
      "    feats = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    \n",
      "    ix2feats[i] = feats\n",
      "    ix2ys[i] = ysByCode\n",
      "    for window in windows:\n",
      "        # Get the words minus tags\n",
      "        words, tags = zip(*window)                \n",
      "        feat = extract_features(words)\n",
      "        tally_features(feat)\n",
      "        feats.append(feat)\n",
      "        \n",
      "        #Tags for middle word (target)\n",
      "        tags4word = tags[MID_IX]\n",
      "        extract_ys_by_code(tags4word, ysByCode)\n",
      "    assert len(windows) == len(feats)\n",
      "    assert all(map(lambda (k,v): len(v) == len(feats), ysByCode.items()))\n",
      "        \n",
      "\"\"\" Convert sparse dictionary features to sparse arrays \"\"\"\n",
      "ix2xs = {}\n",
      "for i, feature_lists in ix2feats.items():\n",
      "    xs = []\n",
      "    ix2xs[i] = xs\n",
      "    for feats in feature_lists:\n",
      "        x = [(idgen.get_id(f),v) \n",
      "             for f,v in feats \n",
      "             if feat_counts[f] >= MIN_FEAT_FREQ or f.startswith(\"WD:0\" )]\n",
      "        xs.append(x)        \n",
      "\n",
      "num_features = idgen.max_id() + 1\n",
      "print \"Number of features:\", num_features\n",
      "\n",
      "\"\"\" Convert to dense numpy arrays \"\"\"\n",
      "for i in ix2xs.keys():\n",
      "    xs = ix2xs[i]\n",
      "    xs = np.array([matutils.sparse2full(x, num_features) for x in xs])        \n",
      "    ix2xs[i] = xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 12270\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "\n",
      "def count_above(ft_counts, threshold):\n",
      "    above = [ v for k,v in ft_counts.items() if v >= threshold]\n",
      "    return (sum(above), len(above))\n",
      "\n",
      "total_all, cnt_all = count_above(feat_counts, 0)\n",
      "total_above, cnt_above = count_above(feat_counts, MIN_FEAT_FREQ)\n",
      "\n",
      "print \"Counts\"\n",
      "print \"all:     \", cnt_all\n",
      "print \"above:   \", cnt_above\n",
      "print \"% above: \", str(100.0 * cnt_above / float(cnt_all))+ \"%\"\n",
      "\n",
      "print \"\\nTotal Frequency\"\n",
      "print \"all:     \", total_all\n",
      "print \"above:   \", total_above\n",
      "print \"% above: \", str(100.0 * total_above / float(total_all))+ \"%\"\n",
      "\n",
      "#srtd = sort_by_value(feat_counts, reverse = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counts\n",
        "all:      60508\n",
        "above:    12003\n",
        "% above:  19.837046341%\n",
        "\n",
        "Total Frequency\n",
        "all:      677038\n",
        "above:    603782\n",
        "% above:  89.1799278622%\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_window(win):\n",
      "    def set2str(st):\n",
      "        return \"{\" + str(t)[5:-2] + \"}\"\n",
      "    \n",
      "    w, tg = zip(*win)\n",
      "    lens = [max(len(wd),len(set2str(t))) for wd,t in win]\n",
      "    \n",
      "    for i, wd in enumerate(w):\n",
      "        print wd.ljust(lens[i]) , \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "    for i, t in enumerate(tg):\n",
      "        print set2str(t).ljust(lens[i]), \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "def extract_features(window, feat_vals):\n",
      "    feats = [idgen.get_key(i) for i,val in enumerate(feat_vals) if val]\n",
      "    \n",
      "    wd_feats = []\n",
      "    for win in window:\n",
      "        wd, tgs = win\n",
      "        if STEM:\n",
      "            match = filter(lambda feat: \" \" + stemmer.stem(wd) + \" \" in \" \" + feat + \" \", feats)\n",
      "        else:\n",
      "            match = filter(lambda feat: \" \" + wd + \" \" in \" \" + feat + \" \", feats)\n",
      "        wd_feats.append((wd, match))\n",
      "    return wd_feats\n",
      "\n",
      "def print_features(wf):\n",
      "    w_f = wf\n",
      "    for w,ft in w_f:\n",
      "        print w.ljust(10), map(lambda s:s.ljust(10), sorted(ft, key=lambda s:(len(s),s)))\n",
      "    print \"\"\n",
      "\n",
      "#uncomment to verify code output\n",
      "\n",
      "sentence_no = 101\n",
      "print \"Tagged Windows\"\n",
      "for win in ix2windows[sentence_no][:5]:\n",
      "    print_window(win)\n",
      "print \"\"    \n",
      "\n",
      "print \"Features\"\n",
      "def prn_sent_features(sentence_num):\n",
      "    win = ix2windows[sentence_num]\n",
      "    for i in range(len(win)):\n",
      "        print \"[%s]\" % str(i)\n",
      "        wf = extract_features(win[i], ix2xs[sentence_num][i])\n",
      "        print_features(wf)\n",
      "\n",
      "prn_sent_features(sentence_no)\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tagged Windows\n",
        "<START> | <START> | <START> | the | passage | states | \"  | \n",
        "{}      | {}      | {}      | {}  | {}      | {}     | {} | \n",
        "<START> | <START> | the | passage | states | \"  | they | \n",
        "{}      | {}      | {}  | {}      | {}     | {} | {}   | \n",
        "<START> | the | passage | states | \"  | they | have | \n",
        "{}      | {}  | {}      | {}     | {} | {}   | {}   | \n",
        "the | passage | states | \"  | they | have | know | \n",
        "{}  | {}      | {}     | {} | {}   | {}   | {}   | \n",
        "passage | states | \"  | they | have | know | about | \n",
        "{}      | {}     | {} | {}   | {}   | {}   | {}    | \n",
        "\n",
        "Features\n",
        "[0]\n",
        "<START>    ['<START>   ', 'WD:-1 <START>', 'WD:-2 <START>', 'WD:-3 <START>', 'BI:-1 <START> | the', 'BI:-2 <START> | <START>', 'BI:-3 <START> | <START>']\n",
        "<START>    ['<START>   ', 'WD:-1 <START>', 'WD:-2 <START>', 'WD:-3 <START>', 'BI:-1 <START> | the', 'BI:-2 <START> | <START>', 'BI:-3 <START> | <START>']\n",
        "<START>    ['<START>   ', 'WD:-1 <START>', 'WD:-2 <START>', 'WD:-3 <START>', 'BI:-1 <START> | the', 'BI:-2 <START> | <START>', 'BI:-3 <START> | <START>']\n",
        "the        ['the       ', 'WD:0 the  ', 'BI:-1 <START> | the']\n",
        "passage    ['passag    ']\n",
        "states     ['state     ', 'WD:2 state', 'BI:2 state | \"']\n",
        "\"          ['\"         ', 'WD:3 \"    ', 'BI:2 state | \"']\n",
        "\n",
        "[1]\n",
        "<START>    ['<START>   ', 'WD:-2 <START>', 'WD:-3 <START>', 'BI:-2 <START> | the', 'BI:-3 <START> | <START>']\n",
        "<START>    ['<START>   ', 'WD:-2 <START>', 'WD:-3 <START>', 'BI:-2 <START> | the', 'BI:-3 <START> | <START>']\n",
        "the        ['the       ', 'WD:-1 the ', 'BI:-2 <START> | the']\n",
        "passage    ['passag    ', 'WD:0 passag']\n",
        "states     ['state     ', 'WD:1 state', 'BI:1 state | \"']\n",
        "\"          ['\"         ', 'WD:2 \"    ', 'BI:1 state | \"']\n",
        "they       ['they      ', 'WD:3 they ']\n",
        "\n",
        "[2]\n",
        "<START>    ['<START>   ', 'WD:-3 <START>', 'BI:-3 <START> | the']\n",
        "the        ['the       ', 'WD:-2 the ', 'BI:-3 <START> | the']\n",
        "passage    ['passag    ']\n",
        "states     ['state     ', 'WD:0 state', 'BI:0 state | \"']\n",
        "\"          ['\"         ', 'WD:1 \"    ', 'BI:0 state | \"']\n",
        "they       ['they      ', 'WD:2 they ', 'BI:2 they | have']\n",
        "have       ['have      ', 'WD:3 have ', 'BI:2 they | have']\n",
        "\n",
        "[3]\n",
        "the        ['the       ', 'WD:-3 the ']\n",
        "passage    ['passag    ']\n",
        "states     ['state     ', 'WD:-1 state', 'BI:-1 state | \"']\n",
        "\"          ['\"         ', 'WD:0 \"    ', 'BI:-1 state | \"']\n",
        "they       ['they      ', 'WD:1 they ', 'BI:1 they | have']\n",
        "have       ['have      ', 'WD:2 have ', 'BI:1 they | have']\n",
        "know       ['know      ', 'WD:3 know ']\n",
        "\n",
        "[4]\n",
        "passage    ['passag    ']\n",
        "states     ['state     ', 'WD:-2 state', 'BI:-2 state | \"']\n",
        "\"          ['\"         ', 'WD:-1 \"   ', 'BI:-2 state | \"']\n",
        "they       ['they      ', 'WD:0 they ', 'BI:0 they | have']\n",
        "have       ['have      ', 'WD:1 have ', 'BI:0 they | have']\n",
        "know       ['know      ', 'WD:2 know ']\n",
        "about      ['about     ', 'WD:3 about']\n",
        "\n",
        "[5]\n",
        "states     ['state     ', 'WD:-3 state', 'BI:-3 state | \"']\n",
        "\"          ['\"         ', 'WD:-2 \"   ', 'BI:-3 state | \"']\n",
        "they       ['they      ', 'WD:-1 they', 'BI:-1 they | have']\n",
        "have       ['have      ', 'WD:0 have ', 'BI:-1 they | have']\n",
        "know       ['know      ', 'WD:1 know ']\n",
        "about      ['about     ', 'WD:2 about', 'BI:2 about | chang']\n",
        "changes    ['chang     ', 'WD:3 chang', 'BI:2 about | chang']\n",
        "\n",
        "[6]\n",
        "\"          ['\"         ', 'WD:-3 \"   ']\n",
        "they       ['they      ', 'WD:-2 they', 'BI:-2 they | have']\n",
        "have       ['have      ', 'WD:-1 have', 'BI:-2 they | have']\n",
        "know       ['know      ', 'WD:0 know ']\n",
        "about      ['about     ', 'WD:1 about', 'BI:1 about | chang']\n",
        "changes    ['chang     ', 'WD:2 chang', 'BI:2 chang | in', 'BI:1 about | chang']\n",
        "in         ['in        ', 'WD:3 in   ', 'BI:2 chang | in']\n",
        "\n",
        "[7]\n",
        "they       ['they      ', 'WD:-3 they', 'BI:-3 they | have']\n",
        "have       ['have      ', 'WD:-2 have', 'BI:-3 they | have']\n",
        "know       ['know      ', 'WD:-1 know']\n",
        "about      ['about     ', 'WD:0 about', 'BI:0 about | chang']\n",
        "changes    ['chang     ', 'WD:1 chang', 'BI:1 chang | in', 'BI:0 about | chang']\n",
        "in         ['in        ', 'WD:2 in   ', 'BI:2 in | wind', 'BI:1 chang | in']\n",
        "wind       ['wind      ', 'WD:3 wind ', 'BI:2 in | wind']\n",
        "\n",
        "[8]\n",
        "have       ['have      ', 'WD:-3 have']\n",
        "know       ['know      ', 'WD:-2 know']\n",
        "about      ['about     ', 'WD:-1 about', 'BI:-1 about | chang']\n",
        "changes    ['chang     ', 'WD:0 chang', 'BI:0 chang | in', 'BI:-1 about | chang']\n",
        "in         ['in        ', 'WD:1 in   ', 'BI:1 in | wind', 'BI:0 chang | in']\n",
        "wind       ['wind      ', 'WD:2 wind ', 'BI:2 wind | ,', 'BI:1 in | wind']\n",
        ",          [',         ', 'WD:3 ,    ', 'BI:2 wind | ,']\n",
        "\n",
        "[9]\n",
        "know       ['know      ', 'WD:-3 know']\n",
        "about      ['about     ', 'WD:-2 about', 'BI:-2 about | chang']\n",
        "changes    ['chang     ', 'WD:-1 chang', 'BI:-1 chang | in', 'BI:-2 about | chang']\n",
        "in         ['in        ', 'WD:0 in   ', 'BI:0 in | wind', 'BI:-1 chang | in']\n",
        "wind       ['wind      ', 'WD:1 wind ', 'BI:1 wind | ,', 'BI:0 in | wind']\n",
        ",          [',         ', 'WD:2 ,    ', 'BI:1 wind | ,', 'BI:2 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:3 temperatur', 'BI:2 , | temperatur']\n",
        "\n",
        "[10]\n",
        "about      ['about     ', 'WD:-3 about', 'BI:-3 about | chang']\n",
        "changes    ['chang     ', 'WD:-2 chang', 'BI:-2 chang | in', 'BI:-3 about | chang']\n",
        "in         ['in        ', 'WD:-1 in  ', 'BI:-1 in | wind', 'BI:-2 chang | in']\n",
        "wind       ['wind      ', 'WD:0 wind ', 'BI:0 wind | ,', 'BI:-1 in | wind']\n",
        ",          [',         ', 'WD:1 ,    ', 'BI:0 wind | ,', 'BI:1 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:2 temperatur', 'BI:1 , | temperatur', 'BI:2 temperatur | .']\n",
        ".          ['.         ', 'WD:3 .    ', 'BI:2 temperatur | .']\n",
        "\n",
        "[11]\n",
        "changes    ['chang     ', 'WD:-3 chang', 'BI:-3 chang | in']\n",
        "in         ['in        ', 'WD:-2 in  ', 'BI:-2 in | wind', 'BI:-3 chang | in']\n",
        "wind       ['wind      ', 'WD:-1 wind', 'BI:-1 wind | ,', 'BI:-2 in | wind']\n",
        ",          [',         ', 'WD:0 ,    ', 'BI:-1 wind | ,', 'BI:0 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:1 temperatur', 'BI:0 , | temperatur', 'BI:1 temperatur | .']\n",
        ".          ['.         ', 'WD:2 .    ', 'BI:2 . | \"', 'BI:1 temperatur | .']\n",
        "\"          ['\"         ', 'WD:3 \"    ', 'BI:2 . | \"']\n",
        "\n",
        "[12]\n",
        "in         ['in        ', 'WD:-3 in  ', 'BI:-3 in | wind']\n",
        "wind       "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['wind      ', 'WD:-2 wind', 'BI:-2 wind | ,', 'BI:-3 in | wind']\n",
        ",          [',         ', 'WD:-1 ,   ', 'BI:-2 wind | ,', 'BI:-1 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:0 temperatur', 'BI:0 temperatur | .', 'BI:-1 , | temperatur']\n",
        ".          ['.         ', 'WD:1 .    ', 'BI:1 . | \"', 'BI:0 temperatur | .']\n",
        "\"          ['\"         ', 'WD:2 \"    ', 'BI:1 . | \"', 'BI:2 \" | <END>']\n",
        "<END>      ['<END>     ', 'WD:3 <END>', 'BI:2 \" | <END>']\n",
        "\n",
        "[13]\n",
        "wind       ['wind      ', 'WD:-3 wind', 'BI:-3 wind | ,']\n",
        ",          [',         ', 'WD:-2 ,   ', 'BI:-3 wind | ,', 'BI:-2 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:-1 temperatur', 'BI:-1 temperatur | .', 'BI:-2 , | temperatur']\n",
        ".          ['.         ', 'WD:0 .    ', 'BI:0 . | \"', 'BI:-1 temperatur | .']\n",
        "\"          ['\"         ', 'WD:1 \"    ', 'BI:0 . | \"', 'BI:1 \" | <END>']\n",
        "<END>      ['<END>     ', 'WD:2 <END>', 'WD:3 <END>', 'BI:1 \" | <END>', 'BI:2 <END> | <END>']\n",
        "<END>      ['<END>     ', 'WD:2 <END>', 'WD:3 <END>', 'BI:1 \" | <END>', 'BI:2 <END> | <END>']\n",
        "\n",
        "[14]\n",
        ",          [',         ', 'WD:-3 ,   ', 'BI:-3 , | temperatur']\n",
        "temperatures ['temperatur', 'WD:-2 temperatur', 'BI:-2 temperatur | .', 'BI:-3 , | temperatur']\n",
        ".          ['.         ', 'WD:-1 .   ', 'BI:-1 . | \"', 'BI:-2 temperatur | .']\n",
        "\"          ['\"         ', 'WD:0 \"    ', 'BI:-1 . | \"', 'BI:0 \" | <END>']\n",
        "<END>      ['<END>     ', 'WD:1 <END>', 'WD:2 <END>', 'WD:3 <END>', 'BI:0 \" | <END>', 'BI:1 <END> | <END>', 'BI:2 <END> | <END>']\n",
        "<END>      ['<END>     ', 'WD:1 <END>', 'WD:2 <END>', 'WD:3 <END>', 'BI:0 \" | <END>', 'BI:1 <END> | <END>', 'BI:2 <END> | <END>']\n",
        "<END>      ['<END>     ', 'WD:1 <END>', 'WD:2 <END>', 'WD:3 <END>', 'BI:0 \" | <END>', 'BI:1 <END> | <END>', 'BI:2 <END> | <END>']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split the Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_xs_ys(ixs,ixTOxs,ixTOys):\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    for i in ixs:\n",
      "        xs_tmp = ixTOxs[i]\n",
      "        xs.extend(xs_tmp)\n",
      "        ysByCode_tmp = ixTOys[i]\n",
      "        for code in all_codes:\n",
      "            ysByCode[code].extend(ysByCode_tmp[code])\n",
      "    return (np.array(xs), ysByCode)\n",
      "\n",
      "num_train = int(len(sentences) * (1.0 - PCT_VALIDATION))\n",
      "\n",
      "ixtest  = ix2sents.keys()[:num_train]\n",
      "ixvalid = ix2sents.keys()[num_train:]\n",
      "\n",
      "# Extract flattened windows for training data as xs and ys\n",
      "x_t, yByCode_t = extract_xs_ys(ixtest,ix2xs, ix2ys)\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"#Sentences : \" + str(len(sentences))\n",
      "print \"\"\n",
      "\n",
      "all_codes = sorted(all_codes, key= lambda s :(len(s), s))\n",
      "for code in all_codes:\n",
      "    print code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#Sentences : 2143\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "11\n",
        "12\n",
        "13\n",
        "14\n",
        "50\n",
        "5b\n",
        "it\n",
        "other\n",
        "Causer\n",
        "Result\n",
        "Anaphor\n",
        "explicit\n",
        "rhetorical\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train\n",
      "====="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TRAIN \"\"\"\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "map_svm = lambda y: -1 if y < 0 else 1\n",
      "map_reg = lambda y: y\n",
      "\n",
      "#map_y = map_svm\n",
      "map_y = map_reg\n",
      "\n",
      "#cls = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, criterion=\"entropy\")\n",
      "#cls = DecisionTreeClassifier(criterion=\"entropy\")\n",
      "#cls = LogisticRegression()\n",
      "#cls = RidgeClassifier()\n",
      "#cls = LinearSVC()\n",
      "#cls = KNeighborsClassifier(n_neighbors=5) # TOO SLOW!\n",
      "#cls = LDA()\n",
      "#cls = SVC()\n",
      "#cls = RandomForestClassifier(n_jobs=-1, max_depth=100, n_estimators=10)\n",
      "\n",
      "print \"Starting Training\"\n",
      "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "def train(codes, xs, yByCode, fn_create_cls):\n",
      "    code2classifier = {}\n",
      "    for code in codes:\n",
      "        print \"Training for :\", code   \n",
      "        cls = fn_create_cls()\n",
      "        code2classifier[code] = cls\n",
      "        ys = np.asarray(yByCode_t[code])    \n",
      "        ys = map(map_y, ys)\n",
      "        cls.fit(xs, ys)\n",
      "    return code2classifier\n",
      "\n",
      "code2cls = train(all_codes, x_t, yByCode_t, LinearSVC)\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Training\n",
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classify\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Get sentence level classification performance \"\"\"\n",
      "def test_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    cls = codeToClassifier[code]\n",
      "    \n",
      "    try:\n",
      "        cls.n_jobs = 1\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    act_ys  = []\n",
      "    pred_ys = []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys.append(max(ys))\n",
      "        pred_ys.append(max(pred))\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    print \"code:      \", code\n",
      "    print \"recall:    \", r\n",
      "    print \"precision: \", p\n",
      "    print \"f1:        \", f1\n",
      "    print \"accuracy:  \", a\n",
      "    print \"sentences: \", num_codes\n",
      "    print \"\"\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "print \"\"\n",
      "print \"total sent:\", len(ixvalid)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "total sent: 536\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training Data Performance\n",
      "-------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(codes, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    td_metrics = []\n",
      "    for c in codes:\n",
      "        cls = codeToClassifier[c]\n",
      "        td_metrics.append(test_for_code(c, ixs, ixToXs, ixToYs, codeToClassifier))\n",
      "    td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
      "    print type(cls), td_wt_mn_prfa\n",
      "    return td_wt_mn_prfa\n",
      "\n",
      "print \"Training Data: \"\n",
      "metrics = test(all_codes, ixtest, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     1.0\n",
        "precision:  0.993939393939\n",
        "f1:         0.996960486322\n",
        "accuracy:   0.999377722464\n",
        "sentences:  164\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.928571428571\n",
        "precision:  0.962962962963\n",
        "f1:         0.945454545455\n",
        "accuracy:   0.998133167393\n",
        "sentences:  28\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.995169082126\n",
        "precision:  0.990384615385\n",
        "f1:         0.992771084337\n",
        "accuracy:   0.998133167393\n",
        "sentences:  207\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  34\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     1.0\n",
        "precision:  0.942028985507\n",
        "f1:         0.970149253731\n",
        "accuracy:   0.997510889857\n",
        "sentences:  65\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  23\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  109\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.97619047619\n",
        "precision:  1.0\n",
        "f1:         0.987951807229\n",
        "accuracy:   0.999377722464\n",
        "sentences:  42\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  18\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     1.0\n",
        "precision:  0.964285714286\n",
        "f1:         0.981818181818\n",
        "accuracy:   0.998755444928\n",
        "sentences:  54\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     1.0\n",
        "precision:  0.818181818182\n",
        "f1:         0.9\n",
        "accuracy:   0.998755444928\n",
        "sentences:  9\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.998349834983\n",
        "precision:  0.991803278689\n",
        "f1:         0.995065789474\n",
        "accuracy:   0.996266334785\n",
        "sentences:  606\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "recall:     1.0\n",
        "precision:  0.875\n",
        "f1:         0.933333333333\n",
        "accuracy:   0.999377722464\n",
        "sentences:  7\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  2\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "recall:     0.857142857143\n",
        "precision:  1.0\n",
        "f1:         0.923076923077\n",
        "accuracy:   0.990665836963\n",
        "sentences:  105\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "recall:     0.978260869565\n",
        "precision:  0.945945945946\n",
        "f1:         0.961832061069\n",
        "accuracy:   0.984443061605\n",
        "sentences:  322\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "recall:     0.897590361446\n",
        "precision:  0.943037974684\n",
        "f1:         0.91975308642\n",
        "accuracy:   0.967641568139\n",
        "sentences:  332\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "recall:     0.955056179775\n",
        "precision:  0.988372093023\n",
        "f1:         0.971428571429\n",
        "accuracy:   0.996888612321\n",
        "sentences:  89\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.98250728863\n",
        "precision:  0.979651162791\n",
        "f1:         0.981077147016\n",
        "accuracy:   0.991910392035\n",
        "sentences:  343\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "recall:     0.971698113208\n",
        "precision:  0.990384615385\n",
        "f1:         0.980952380952\n",
        "accuracy:   0.997510889857\n",
        "sentences:  106\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.9722, Precision: 0.9765, F1: 0.9740, Accuracy: 0.9913, Codes:  2665\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validation Data Performance\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data: \"\n",
      "test(all_codes, ixvalid, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.879518072289\n",
        "precision:  0.669724770642\n",
        "f1:         0.760416666667\n",
        "accuracy:   0.914179104478\n",
        "sentences:  83\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.605263157895\n",
        "precision:  0.821428571429\n",
        "f1:         0.69696969697\n",
        "accuracy:   0.962686567164\n",
        "sentences:  38\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.808695652174\n",
        "precision:  0.7265625\n",
        "f1:         0.765432098765\n",
        "accuracy:   0.893656716418\n",
        "sentences:  115\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.8\n",
        "precision:  1.0\n",
        "f1:         0.888888888889\n",
        "accuracy:   0.992537313433\n",
        "sentences:  20\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.772727272727\n",
        "precision:  0.641509433962\n",
        "f1:         0.701030927835\n",
        "accuracy:   0.945895522388\n",
        "sentences:  44\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  0.625\n",
        "f1:         0.769230769231\n",
        "accuracy:   0.994402985075\n",
        "sentences:  5\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.8\n",
        "precision:  0.592592592593\n",
        "f1:         0.68085106383\n",
        "accuracy:   0.944029850746\n",
        "sentences:  40\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.666666666667\n",
        "precision:  1.0\n",
        "f1:         0.8\n",
        "accuracy:   0.992537313433\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.545454545455\n",
        "precision:  1.0\n",
        "f1:         0.705882352941\n",
        "accuracy:   0.990671641791\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.764705882353\n",
        "precision:  0.684210526316\n",
        "f1:         0.722222222222\n",
        "accuracy:   0.981343283582\n",
        "sentences:  17\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     0.636363636364\n",
        "precision:  0.777777777778\n",
        "f1:         0.7\n",
        "accuracy:   0.988805970149\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.979253112033\n",
        "precision:  0.940239043825\n",
        "f1:         0.959349593496\n",
        "accuracy:   0.962686567164\n",
        "sentences:  241\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "recall:     0.272727272727\n",
        "precision:  0.375\n",
        "f1:         0.315789473684\n",
        "accuracy:   0.975746268657\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "recall:     0.0\n",
        "precision:  0.0\n",
        "f1:         0.0\n",
        "accuracy:   1.0\n",
        "sentences:  0\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "recall:     0.075\n",
        "precision:  0.24\n",
        "f1:         0.114285714286\n",
        "accuracy:   0.826492537313\n",
        "sentences:  80\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "recall:     0.708860759494\n",
        "precision:  0.535885167464\n",
        "f1:         0.610354223433\n",
        "accuracy:   0.733208955224\n",
        "sentences:  158\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "recall:     0.59375\n",
        "precision:  0.552325581395\n",
        "f1:         0.572289156627\n",
        "accuracy:   0.735074626866\n",
        "sentences:  160\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "recall:     0.145833333333\n",
        "precision:  0.466666666667\n",
        "f1:         0.222222222222\n",
        "accuracy:   0.908582089552\n",
        "sentences:  48\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.645962732919\n",
        "precision:  0.587570621469\n",
        "f1:         0.615384615385\n",
        "accuracy:   0.757462686567\n",
        "sentences:  161\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "recall:     0.205882352941\n",
        "precision:  0.518518518519\n",
        "f1:         0.294736842105\n",
        "accuracy:   0.875\n",
        "sentences:  68\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.6704, Precision: 0.6529, F1: 0.6472, Accuracy: 0.8595, Codes:  1323\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "Recall: 0.6704, Precision: 0.6529, F1: 0.6472, Accuracy: 0.8595, Codes:  1323"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Window - 5, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LDA:      Recall: 0.8927, Precision: 0.6974, F1: 0.7675, Accuracy: 0.8823, Codes:   792\n",
      "LinSVC:   Recall: 0.7601, Precision: 0.7655, F1: 0.7563, Accuracy: 0.9048, Codes:   792\n",
      "DT:       Recall: 0.7462, Precision: 0.6890, F1: 0.7063, Accuracy: 0.8766, Codes:   792\n",
      "RidgeClf: Recall: 0.6843, Precision: 0.8359, F1: 0.6874, Accuracy: 0.9036, Codes:   795\n",
      "\n",
      "LinSVC\n",
      "Window - 7, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LinSVC: Recall: 0.7937, Precision: 0.7522, F1: 0.7677, Accuracy: 0.9037, Codes:   795\n",
      "\n",
      "Window - 9, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.7887, Precision: 0.7338, F1: 0.7555, Accuracy: 0.8929, Codes:   795\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.8058, Precision: 0.7559, F1: 0.7756, Accuracy: 0.9046, Codes:   798\n",
      "\n",
      "-- Starting adding new features, messing with feature freq\n",
      "Window - 5, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8008, Precision: 0.7369, F1: 0.7625, Accuracy: 0.9008, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8120, Precision: 0.7535, F1: 0.7779, Accuracy: 0.9066, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - ***15***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8145, Precision: 0.7460, F1: 0.7744, Accuracy: 0.9040, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 20\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7982, Precision: 0.7496, F1: 0.7685, Accuracy: 0.9025, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 25\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8070, Precision: 0.7485, F1: 0.7732, Accuracy: 0.9047, Codes:   798\n",
      "\n",
      "***\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 5 ***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "    + Positional BI-GRAMS ****\n",
      "Recall: 0.8145, Precision: 0.7642, F1: 0.7831, Accuracy: 0.9070, Codes:   798\n",
      "***\n",
      "\n",
      "Logistic Regression\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7306, Precision: 0.8298, F1: 0.7672, Accuracy: 0.9150, Codes:   798\n",
      "\n",
      "Window - 9, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7206, Precision: 0.8232, F1: 0.7580, Accuracy: 0.9121, Codes:   798\n",
      "\n",
      "RF\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit')\n",
      "Recall: 0.6028, Precision: 0.8071, F1: 0.6570, Accuracy: 0.8978, Codes:   798"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"\"\" TODO - Peter\\Simon 5.14.2014\n",
      "Match Explicit + Cause or Result (or both)\n",
      "Try reading this: http://ceur-ws.org/Vol-1109/paper4.pdf\n",
      "Read Peter's paper\n",
      "\n",
      "1. <s>Try removing commas and '\"''s and other punctuation</s>\n",
      "2. <s>Try skip gram features (Peter)</s>\n",
      "3. Dependency parse \n",
      "    - span of words for the explicit and the concept\n",
      "    - find any depencencies tha join those two groups\n",
      "    - dependency type\n",
      "        - NSUBJ or PREP_TO or CONJ_AND\n",
      "        - OR advmod, conj_and, dobj, prep_of, prep_in\n",
      "        - OR acomp,  advmod\n",
      "4. <s>Read Peter's latest paper </s>\n",
      "5. Read related papers - Semeval 2007 (or close), Rink et al, Girju et al, etc\n",
      "5. <s>Try training a second classifier based on the output of the first including the predictions for the previous and next word</s>\n",
      "\"\"\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train Stacked Classifier\n",
      "========================"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Create Data Using Previous Classifier \"\"\"\n",
      "ix2newxs = {}\n",
      "ix2newys = {} #dict to dict to list\n",
      "for i,xs in ix2xs.items():\n",
      "    \n",
      "    tmp_xs = []\n",
      "    tmp_ys = []\n",
      "    tmp_ys_by_code = defaultdict(list)\n",
      "    for code in all_codes:\n",
      "        \n",
      "        cls = code2cls[code]\n",
      "        pred = cls.predict(xs)\n",
      "        #pred = cls.decision_function(xs)\n",
      "        tmp_xs.append(max(pred))\n",
      "        tmp_ys_by_code[code] = max(ix2ys[i][code])\n",
      "    ix2newxs[i] = np.array([tmp_xs])\n",
      "    ix2newys[i] = tmp_ys_by_code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "newx_t, yByCode_t = extract_xs_ys(ixtest,ix2newxs, ix2ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.dummy import DummyClassifier\n",
      "#new_code2cls = train(all_codes, newx_t, yByCode_t, DummyClassifier)\n",
      "new_code2cls = train(all_codes, newx_t, yByCode_t, DecisionTreeClassifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test Data #2: \"\n",
      "metrics = test(reg_codes, ixtest, ix2newxs, ix2ys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test Data #2: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  164\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.892857142857\n",
        "precision:  1.0\n",
        "f1:         0.943396226415\n",
        "accuracy:   0.998133167393\n",
        "sentences:  28\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.995169082126\n",
        "precision:  0.990384615385\n",
        "f1:         0.992771084337\n",
        "accuracy:   0.998133167393\n",
        "sentences:  207\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  34\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  65\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  23\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  109\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  42\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  18\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     1.0\n",
        "precision:  0.981818181818\n",
        "f1:         0.990825688073\n",
        "accuracy:   0.999377722464\n",
        "sentences:  54\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     1.0\n",
        "precision:  0.9\n",
        "f1:         0.947368421053\n",
        "accuracy:   0.999377722464\n",
        "sentences:  9\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.996699669967\n",
        "precision:  1.0\n",
        "f1:         0.998347107438\n",
        "accuracy:   0.998755444928\n",
        "sentences:  606\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.98833819242\n",
        "precision:  1.0\n",
        "f1:         0.994134897361\n",
        "accuracy:   0.997510889857\n",
        "sentences:  343\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.9941, Precision: 0.9977, F1: 0.9958, Accuracy: 0.9988, Codes:  1702\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data #2: \"\n",
      "metrics = test(reg_codes, ixvalid, ix2newxs, ix2ys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data #2: \n",
        "code:       1\n",
        "recall:     0.879518072289\n",
        "precision:  0.657657657658\n",
        "f1:         0.752577319588\n",
        "accuracy:   0.910447761194\n",
        "sentences:  83\n",
        "\n",
        "code:       2\n",
        "recall:     0.657894736842\n",
        "precision:  0.694444444444\n",
        "f1:         0.675675675676\n",
        "accuracy:   0.955223880597\n",
        "sentences:  38\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.8\n",
        "precision:  0.747967479675\n",
        "f1:         0.773109243697\n",
        "accuracy:   0.899253731343\n",
        "sentences:  115\n",
        "\n",
        "code:       4\n",
        "recall:     0.8\n",
        "precision:  1.0\n",
        "f1:         0.888888888889\n",
        "accuracy:   0.992537313433\n",
        "sentences:  20\n",
        "\n",
        "code:       5\n",
        "recall:     0.75\n",
        "precision:  0.702127659574\n",
        "f1:         0.725274725275\n",
        "accuracy:   0.953358208955\n",
        "sentences:  44\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  0.555555555556\n",
        "f1:         0.714285714286\n",
        "accuracy:   0.992537313433\n",
        "sentences:  5\n",
        "\n",
        "code:       7\n",
        "recall:     0.8\n",
        "precision:  0.542372881356\n",
        "f1:         0.646464646465\n",
        "accuracy:   0.934701492537\n",
        "sentences:  40\n",
        "\n",
        "code:       11\n",
        "recall:     0.75\n",
        "precision:  1.0\n",
        "f1:         0.857142857143\n",
        "accuracy:   0.994402985075\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.636363636364\n",
        "precision:  0.7\n",
        "f1:         0.666666666667\n",
        "accuracy:   0.986940298507\n",
        "sentences:  11\n",
        "\n",
        "code:       13\n",
        "recall:     0.529411764706\n",
        "precision:  0.9\n",
        "f1:         0.666666666667\n",
        "accuracy:   0.983208955224\n",
        "sentences:  17\n",
        "\n",
        "code:       14\n",
        "recall:     0.727272727273\n",
        "precision:  0.888888888889\n",
        "f1:         0.8\n",
        "accuracy:   0.992537313433\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.96265560166\n",
        "precision:  0.94693877551\n",
        "f1:         0.954732510288\n",
        "accuracy:   0.958955223881\n",
        "sentences:  241\n",
        "\n",
        "code:       explicit\n",
        "recall:     0.639751552795\n",
        "precision:  0.556756756757\n",
        "f1:         0.595375722543\n",
        "accuracy:   0.738805970149\n",
        "sentences:  161\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.8070, Precision: 0.7581, F1: 0.7768, Accuracy: 0.9021, Codes:   798\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.arange(10)\n",
      "b = a + 10\n",
      "np.concatenate((a,b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
        "       17, 18, 19])"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}