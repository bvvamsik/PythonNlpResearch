{
 "metadata": {
  "name": "",
  "signature": "sha256:1e0a69deae7d89b8daaa2ee94e39b4a220eaf8ac41d78b2ba97a165a13c8a425"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Sequential Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\"\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 5\n",
      "\n",
      "MIN_SENTENCE_FREQ = 1\n",
      "MIN_FEAT_FREQ = 10     #10 best so far\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "STEM = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Dropbox/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "print \"Running Spelling Correction..\"\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            else:\n",
      "                cw = corrector.correct(w)\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "                sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "            \n",
      "numeric_codes = set([c for c in all_codes if not c.isalpha()])\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "235 files found\n",
        "235 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Running Spelling Correction..\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Verify Spelling Corrections\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "cor_srtd = sort_by_value(corrections, reverse = True)\n",
      "cor_srtd[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[((\"it's\", 'it'), 46),\n",
        " ((\"don't\", 'dont'), 30),\n",
        " ((\"that's\", 'that'), 28),\n",
        " (('algea', 'algae'), 20),\n",
        " (('zox', 'zo'), 20),\n",
        " ((\"world's\", 'world'), 17),\n",
        " ((\"can't\", 'cant'), 14),\n",
        " (('bleaches', 'bleached'), 12),\n",
        " (('cloral', 'coral'), 10),\n",
        " ((\"coral's\", 'coral'), 9),\n",
        " ((\"isn't\", 'isnt'), 9),\n",
        " ((\"won't\", 'wont'), 9),\n",
        " ((\"they're\", 'there'), 9),\n",
        " (('tempature', 'temperature'), 8),\n",
        " ((\"doesn't\", 'doesnt'), 8),\n",
        " (('tempeture', 'temperature'), 7),\n",
        " (('varys', 'vary'), 6),\n",
        " (('saclike', 'saline'), 6),\n",
        " ((\"there's\", 'there'), 5),\n",
        " ((\"i'm\", 'iam'), 5)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Corpus in CRF Format (list of list of tuples(word,tag))\n",
      "--------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sentences_by_code = defaultdict(list)\n",
      "\n",
      "#store raw sentence\n",
      "ix2sentence = {}\n",
      "ix2tags = {}\n",
      "\n",
      "#TODO Try IOBE encoding\n",
      "out_tag = 'O'\n",
      "ix = -1\n",
      "#Keep untagged sentences for testing\n",
      "sentences = []\n",
      "tags4sentences = []\n",
      "\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        ix+= 1\n",
      "        \n",
      "        sent = [(w,t) for (w,t) in sentence if wd_sent_freq[w] >= MIN_SENTENCE_FREQ]\n",
      "        words, tags = zip(*sent)\n",
      "        \n",
      "        ix2sentence[ix] = words\n",
      "        sentences.append(words)\n",
      "        \n",
      "        utags = set(flatten(tags))\n",
      "        tags4sentences.append(utags)\n",
      "        \n",
      "        for code in utags:\n",
      "            tagged = []\n",
      "            for w, tags in sentence:\n",
      "                tagged.append((w, code if code in tags else out_tag))\n",
      "            tagged_sentences_by_code[code].append(tagged)\n",
      "        \n",
      "        no_tags = [(w,out_tag) for w in words]\n",
      "        missing_tags = all_codes - utags\n",
      "        for code in missing_tags:\n",
      "            tagged_sentences_by_code[code].append(no_tags)\n",
      "\n",
      "#To numpy so we can filter using indexes\n",
      "for code in all_codes:\n",
      "    sents = tagged_sentences_by_code[code]\n",
      "    tagged_sentences_by_code[code] = np.asarray(sents)\n",
      "\n",
      "sentences = np.asarray(sentences)\n",
      "tags4sentences = np.asarray(tags4sentences)\n",
      "\n",
      "def tags_to_binary_matches(tags_lst, code):\n",
      "    return np.asarray([1 if code in tags else 0 for tags in tags_lst])\n",
      "\n",
      "binary_lbls_for_code = {}\n",
      "for code in all_codes:\n",
      "    binary_lbls_for_code[code] = tags_to_binary_matches(tags4sentences, code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Verify all codes same length, types are the same (np.array of lists of tuples of (word,tag) pairs)\n",
      "for code in sorted(all_codes):\n",
      "    print code.ljust(20), len(tagged_sentences_by_code[code]), \\\n",
      "      type(tagged_sentences_by_code[code]), \\\n",
      "            type(tagged_sentences_by_code[code][0]), \\\n",
      "                type(tagged_sentences_by_code[code][0][0]), \\\n",
      "                    type(tagged_sentences_by_code[code][0][0][0]),\\\n",
      "                        type(tagged_sentences_by_code[code][0][0][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "11                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "12                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "13                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "14                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "2                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "3                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "4                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "5                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "50                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "5b                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "6                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "7                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "Anaphor              2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "explicit             2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "it                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "other                2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
        "rhetorical           2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "WINDOW_OFFSET = int(WINDOW_SIZE / 2)\n",
      "PAD_START = [\"START\" for i in range(WINDOW_OFFSET)]\n",
      "PAD_END = [\"END\" for i in range(WINDOW_OFFSET)]\n",
      "POS_TAG = 'x'\n",
      "\n",
      "def extract_features(words, index):\n",
      "    \"\"\" Takes a list of string and the index for the word in the list\n",
      "    \"\"\"\n",
      "    \n",
      "    wds = PAD_START + list(words[::]) + PAD_END\n",
      "    ix_offset = index + WINDOW_OFFSET\n",
      "    start = ix_offset - WINDOW_OFFSET\n",
      "    end   = start + WINDOW_SIZE\n",
      "    \n",
      "    feats = {}\n",
      "    for i in range(start, end):\n",
      "        word = wds[i]\n",
      "        if STEM:\n",
      "            word = stemmer.stem(word)\n",
      "        feats[\"WD\" + str(i - ix_offset) + \":\" + word] = POS_TAG\n",
      "    \n",
      "    #word_features = extract_word_features(words, feature_val=1)\n",
      "    #features.update(word_features)    \n",
      "    return feats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test Feature Exractor\n",
      "---------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def comparator(kvp):\n",
      "    k,v = kvp\n",
      "    num = (k[2:]).split(\":\")[0]\n",
      "    return int(num)\n",
      "\n",
      "def test_feature_extractor_on_sentence(extractor, sent):\n",
      "    sent = sent.split(\" \")\n",
      "    for i in range(len(sent)):\n",
      "        print sent[i].ljust(10),\n",
      "        s = sorted(extractor(sent, i).items(), key = comparator)\n",
      "        print map(lambda item: str(item).ljust(15),zip(*s)[0])\n",
      "\n",
      "sent1 = \"the cat sat on the mat\"\n",
      "sent2 = \"coral bleaching\"\n",
      "sent3 = \"president obama approached the senate, ...\"\n",
      "test_feature_extractor_on_sentence(extract_features, sent1)\n",
      "print \"\"\n",
      "test_feature_extractor_on_sentence(extract_features, sent2)\n",
      "print \"\"\n",
      "test_feature_extractor_on_sentence(extract_features, sent3)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the        ['WD-2:START     ', 'WD-1:START     ', 'WD0:the        ', 'WD1:cat        ', 'WD2:sat        ']\n",
        "cat        ['WD-2:START     ', 'WD-1:the       ', 'WD0:cat        ', 'WD1:sat        ', 'WD2:on         ']\n",
        "sat        ['WD-2:the       ', 'WD-1:cat       ', 'WD0:sat        ', 'WD1:on         ', 'WD2:the        ']\n",
        "on         ['WD-2:cat       ', 'WD-1:sat       ', 'WD0:on         ', 'WD1:the        ', 'WD2:mat        ']\n",
        "the        ['WD-2:sat       ', 'WD-1:on        ', 'WD0:the        ', 'WD1:mat        ', 'WD2:END        ']\n",
        "mat        ['WD-2:on        ', 'WD-1:the       ', 'WD0:mat        ', 'WD1:END        ', 'WD2:END        ']\n",
        "\n",
        "coral      ['WD-2:START     ', 'WD-1:START     ', 'WD0:coral      ', 'WD1:bleaching  ', 'WD2:END        ']\n",
        "bleaching  ['WD-2:START     ', 'WD-1:coral     ', 'WD0:bleaching  ', 'WD1:END        ', 'WD2:END        ']\n",
        "\n",
        "president  ['WD-2:START     ', 'WD-1:START     ', 'WD0:president  ', 'WD1:obama      ', 'WD2:approached ']\n",
        "obama      ['WD-2:START     ', 'WD-1:president ', 'WD0:obama      ', 'WD1:approached ', 'WD2:the        ']\n",
        "approached ['WD-2:president ', 'WD-1:obama     ', 'WD0:approached ', 'WD1:the        ', 'WD2:senate,    ']\n",
        "the        ['WD-2:obama     ', 'WD-1:approached', 'WD0:the        ', 'WD1:senate,    ', 'WD2:...        ']\n",
        "senate,    ['WD-2:approached', 'WD-1:the       ', 'WD0:senate,    ', 'WD1:...        ', 'WD2:END        ']\n",
        "...        ['WD-2:the       ', 'WD-1:senate,   ', 'WD0:...        ', 'WD1:END        ', 'WD2:END        ']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training\n",
      "========"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag.crf import MalletCRF\n",
      "\n",
      "n = len(sentences)\n",
      "ixs = range(n)\n",
      "num_train = int((1 - PCT_VALIDATION) * n)\n",
      "ix_train = ixs[:num_train]\n",
      "ix_valid = ixs[num_train:]\n",
      "print \"#train\", len(ix_train), \"#validation\", len(ix_valid), \"#combined\", len(ix_train + ix_valid), \"#total\", len(ixs)\n",
      "\n",
      "code2tagger = {}\n",
      "for code in sorted(all_codes):\n",
      "    \n",
      "    data = tagged_sentences_by_code[code]\n",
      "    #Training Data\n",
      "    train = data[ix_train]\n",
      "    print \"Training on\", code\n",
      "    tagger = MalletCRF.train(feature_detector= extract_features, corpus=train, \n",
      "                  filename=None, weight_groups=None, gaussian_variance=1, default_label='O',\n",
      "                  transduction_type='VITERBI', max_iterations=500,\n",
      "                  add_start_state=True, add_end_state=True, trace=0)\n",
      "    code2tagger[code] = tagger\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#train 1608 #validation 536 #combined 2144 #total 2144\n",
        "Training on 11\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training on"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "\n",
      "def test_for_code(code, ixs):\n",
      "    #Validation Data\n",
      "    valid_sentences = sentences[ixs]\n",
      "    act_ys = binary_lbls_for_code[code][ixs]\n",
      "    \n",
      "    tagger = code2tagger[code]\n",
      "    pred_tags = tagger.batch_tag(valid_sentences)\n",
      "    tags_list = map(lambda tag_sent: set(zip(*tag_sent)[1]) , pred_tags)\n",
      "    pred_ys = tags_to_binary_matches(tags_list, code)\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    print \"code:      \", code\n",
      "    print \"recall:    \", r\n",
      "    print \"precision: \", p\n",
      "    print \"f1:        \", f1\n",
      "    print \"accuracy:  \", a\n",
      "    print \"sentences: \", num_codes\n",
      "    print \"\"\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "print \"\"\n",
      "print \"total sent:\", len(ix_valid)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "total sent: 536\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training Data Performance\n",
      "-------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "td_metrics = []\n",
      "for c in sorted(reg_codes):\n",
      "    td_metrics.append(test_for_code(c, ix_train))\n",
      "\n",
      "td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
      "print \"Training Data: \"\n",
      "print td_wt_mn_prfa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "code:       11\n",
        "recall:     0.666666666667\n",
        "precision:  1.0\n",
        "f1:         0.8\n",
        "accuracy:   0.991293532338\n",
        "sentences:  42\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.814814814815\n",
        "precision:  0.814814814815\n",
        "f1:         0.814814814815\n",
        "accuracy:   0.987562189055\n",
        "sentences:  54\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.833333333333\n",
        "precision:  1.0\n",
        "f1:         0.909090909091\n",
        "accuracy:   0.998134328358\n",
        "sentences:  18\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     0.444444444444\n",
        "precision:  0.8\n",
        "f1:         0.571428571429\n",
        "accuracy:   0.996268656716\n",
        "sentences:  9\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.479651162791\n",
        "precision:  0.820895522388\n",
        "f1:         0.605504587156\n",
        "accuracy:   0.866293532338\n",
        "sentences:  344\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.960461285008\n",
        "precision:  0.949511400651\n",
        "f1:         0.954954954955\n",
        "accuracy:   0.9657960199\n",
        "sentences:  607\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.94512195122\n",
        "precision:  0.933734939759\n",
        "f1:         0.939393939394\n",
        "accuracy:   0.987562189055\n",
        "sentences:  164\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.927536231884\n",
        "precision:  0.914285714286\n",
        "f1:         0.920863309353\n",
        "accuracy:   0.97947761194\n",
        "sentences:  207\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.907692307692\n",
        "precision:  0.766233766234\n",
        "f1:         0.830985915493\n",
        "accuracy:   0.985074626866\n",
        "sentences:  65\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.882352941176\n",
        "precision:  0.909090909091\n",
        "f1:         0.89552238806\n",
        "accuracy:   0.995646766169\n",
        "sentences:  34\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.761467889908\n",
        "precision:  0.855670103093\n",
        "f1:         0.805825242718\n",
        "accuracy:   0.975124378109\n",
        "sentences:  109\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     0.739130434783\n",
        "precision:  1.0\n",
        "f1:         0.85\n",
        "accuracy:   0.996268656716\n",
        "sentences:  23\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.642857142857\n",
        "precision:  0.857142857143\n",
        "f1:         0.734693877551\n",
        "accuracy:   0.991915422886\n",
        "sentences:  28\n",
        "\n",
        "Training Data: \n",
        "Recall: 0.8175, Precision: 0.8998, F1: 0.8475, Accuracy: 0.9541, Codes:  1704\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validation Data Performance\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vd_metrics = []\n",
      "for c in sorted(reg_codes):\n",
      "    vd_metrics.append(test_for_code(c, ix_valid))\n",
      "\n",
      "vd_wt_mn_prfa = weighted_mean_rpfa(vd_metrics)\n",
      "print \"Validation Data:\"\n",
      "print vd_wt_mn_prfa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "code:       1\n",
        "recall:     0.867469879518\n",
        "precision:  0.679245283019\n",
        "f1:         0.761904761905\n",
        "accuracy:   0.916044776119\n",
        "sentences:  83\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.538461538462\n",
        "precision:  1.0\n",
        "f1:         0.7\n",
        "accuracy:   0.988805970149\n",
        "sentences:  13\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.363636363636\n",
        "precision:  1.0\n",
        "f1:         0.533333333333\n",
        "accuracy:   0.986940298507\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.352941176471\n",
        "precision:  0.666666666667\n",
        "f1:         0.461538461538\n",
        "accuracy:   0.973880597015\n",
        "sentences:  17\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     0.363636363636\n",
        "precision:  0.8\n",
        "f1:         0.5\n",
        "accuracy:   0.985074626866\n",
        "sentences:  11\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.447368421053\n",
        "precision:  0.85\n",
        "f1:         0.586206896552\n",
        "accuracy:   0.955223880597\n",
        "sentences:  38\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.721739130435\n",
        "precision:  0.754545454545\n",
        "f1:         0.737777777778\n",
        "accuracy:   0.889925373134\n",
        "sentences:  115\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.7\n",
        "precision:  0.933333333333\n",
        "f1:         0.8\n",
        "accuracy:   0.986940298507\n",
        "sentences:  20\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.704545454545\n",
        "precision:  0.756097560976\n",
        "f1:         0.729411764706\n",
        "accuracy:   0.957089552239\n",
        "sentences:  44\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.979166666667\n",
        "precision:  0.971074380165\n",
        "f1:         0.97510373444\n",
        "accuracy:   0.977611940299\n",
        "sentences:  240\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     0.4\n",
        "precision:  1.0\n",
        "f1:         0.571428571429\n",
        "accuracy:   0.994402985075\n",
        "sentences:  5\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.65\n",
        "precision:  0.684210526316\n",
        "f1:         0.666666666667\n",
        "accuracy:   0.951492537313\n",
        "sentences:  40\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.304347826087\n",
        "precision:  0.604938271605\n",
        "f1:         0.404958677686\n",
        "accuracy:   0.731343283582\n",
        "sentences:  161\n",
        "\n",
        "Validation Data:\n",
        "Recall: 0.6892, Precision: 0.7949, F1: 0.7212, Accuracy: 0.9061, Codes:   798\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**CRF** - MIN_SENT_FREQ = 1, STEM = False, PCT_VALIDATION = 0.25  \n",
      "Validation Data:\n",
      "Recall: 0.6892, Precision: 0.7949, **F1: 0.7212**, Accuracy: 0.9061, Codes:   798"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TODO**  \n",
      "Try training it on all codes rather than one at a time, ignoring where multiple (so ignore word codes, restrict to codes with a number in them somewhere).  \n",
      "Try IOBE encoding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}