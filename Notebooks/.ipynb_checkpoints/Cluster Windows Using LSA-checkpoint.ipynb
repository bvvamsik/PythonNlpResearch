{
 "metadata": {
  "name": "",
  "signature": "sha256:5eb0530791e08a8b917ce695c206e5712f14c99509d5aa09bcd5d99a5d3a3cf9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from ProcessEssays import process_sentences\n",
      "from Decorators import timeit, memoize\n",
      "\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "WINDOW_SIZE = 5\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ   = 2       #i.e. df\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = False\n",
      "REPLACE_NUMS        = True    # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 5\n",
      "NUM_LSA_TOPICS      = 300\n",
      "\n",
      "assert WINDOW_SIZE >= MIN_SENTENCE_LENGTH"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load Data\n",
      "========="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wrap in decorated fn for timing\n",
      "@timeit\n",
      "def load_sentences(essays):\n",
      "    return process_sentences(essays, \n",
      "                              min_df = MIN_SENTENCE_FREQ, \n",
      "                              spelling_correct = SPELLING_CORRECT, \n",
      "                              replace_nums = REPLACE_NUMS, \n",
      "                              stem=STEM)\n",
      "# load from disk\n",
      "essays = load_bratt_essays()\n",
      "lbl_sentences = load_sentences(essays)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
        "297 files found\n",
        "297 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timeit: load_sentences(...) took 15.23 secs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences, lbls = [], []\n",
      "\n",
      "#unzip for convenience\n",
      "for lbl_sentence in lbl_sentences:\n",
      "    wds, tags = zip(*lbl_sentence)\n",
      "    sentences.append(wds)\n",
      "    lbls.append(tags)\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split Data into Windows of Regular Length\n",
      "========================================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" NOTE - some windows will be less than WINDOW_SIZE length \"\"\"\n",
      "ix2windows = dict()\n",
      "windows = []\n",
      "for i, sentence in enumerate(sentences):\n",
      "    if len(sentence) < MIN_SENTENCE_LENGTH:\n",
      "        continue\n",
      "\n",
      "    wins = split_into_windows(sentence, WINDOW_SIZE)\n",
      "    ix2windows[i] = wins\n",
      "\n",
      "    str_wins = map(lambda win: \" \".join(win), wins)\n",
      "    windows.extend(str_wins)\n",
      "    \n",
      "for i, w in enumerate(ix2windows[0]):\n",
      "    print i, len(w), [wd.ljust(10) for wd in w]\n",
      "print \"\"\n",
      "for w in windows[0:10]:\n",
      "    print len(w.split(\" \")), w \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 5 ['the       ', 'coral     ', 'bleaching ', 'is        ', 'a         ']\n",
        "1 5 ['coral     ', 'bleaching ', 'is        ', 'a         ', 'different ']\n",
        "2 5 ['bleaching ', 'is        ', 'a         ', 'different ', 'type      ']\n",
        "3 5 ['is        ', 'a         ', 'different ', 'type      ', 'they      ']\n",
        "4 5 ['a         ', 'different ', 'type      ', 'they      ', 'are       ']\n",
        "5 5 ['different ', 'type      ', 'they      ', 'are       ', 'bleached  ']\n",
        "6 5 ['type      ', 'they      ', 'are       ', 'bleached  ', 'and       ']\n",
        "7 5 ['they      ', 'are       ', 'bleached  ', 'and       ', 'coral     ']\n",
        "8 5 ['are       ', 'bleached  ', 'and       ', 'coral     ', 'bleaching ']\n",
        "9 5 ['bleached  ', 'and       ', 'coral     ', 'bleaching ', '.         ']\n",
        "\n",
        "5 the coral bleaching is a\n",
        "5 coral bleaching is a different\n",
        "5 bleaching is a different type\n",
        "5 is a different type they\n",
        "5 a different type they are\n",
        "5 different type they are bleached\n",
        "5 type they are bleached and\n",
        "5 they are bleached and coral\n",
        "5 are bleached and coral bleaching\n",
        "5 bleached and coral bleaching .\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(use_idf = True, ngram_range = (1, 1), min_df = 1, binary=False, norm =\"l2\")\n",
      "window_vectors = np.asarray(vectorizer.fit_transform(windows).todense())\n",
      "\n",
      "print window_vectors.shape\n",
      "print window_vectors[0][window_vectors[0] > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(33933, 1050)\n",
        "[ 0.57711243  0.46968686  0.57287557  0.3437282 ]\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec_lens = np.asarray(map(np.linalg.norm,window_vectors))\n",
      "print \"MEAN, MIN, MAX\", np.mean(vec_lens), np.min(vec_lens), np.max(vec_lens)\n",
      "\n",
      "print \"0 length vectors       \", np.asarray(windows)[np.where(vec_lens == 0.0)]\n",
      "print \"Not unit length vectors\", np.asarray(windows)[np.where(vec_lens < 0.9)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MEAN, MIN, MAX 0.999941060325 0.0 1.0\n",
        "0 length vectors        ['a ? ? ? a' '0 . 0 f .']\n",
        "Not unit length vectors ['a ? ? ? a' '0 . 0 f .']\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import matutils\n",
      "\n",
      "# Compute id2 word mapping needed by gensim\n",
      "wd2id = dict(vectorizer.vocabulary_.items())\n",
      "def swap((k,v)):\n",
      "    return (v,k)\n",
      "id2wd = dict(map(swap, wd2id.items()))\n",
      "\n",
      "#window_vectors_fltrd = np.asarray([w for w in window_vectors if max(w) > 0.0])\n",
      "#corpus = list(matutils.Scipy2Corpus(window_vectors_fltrd))\n",
      "corpus = list(matutils.Scipy2Corpus(window_vectors))\n",
      "\n",
      "print corpus[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(122, 0.49804294467750176), (203, 0.40533562562848852), (252, 0.5858653167062704), (456, 0.49438657600503921)]\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import LsiModel\n",
      "import logging\n",
      "\n",
      "lsa = LsiModel(corpus = corpus, id2word = id2wd, num_topics = NUM_LSA_TOPICS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsa_vectors = matutils.corpus2dense(lsa[corpus], num_terms = NUM_LSA_TOPICS).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert len(lsa_vectors) == len(windows), \"lsa vectors must equal window length\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 178,
       "text": [
        "(33933, 33933)"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TODO clustering \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}