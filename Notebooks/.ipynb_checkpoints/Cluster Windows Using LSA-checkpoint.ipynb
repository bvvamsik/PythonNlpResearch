{
 "metadata": {
  "name": "",
  "signature": "sha256:7181bb789a942e5af9840bb6d460bd53d57983e921ddcd1bdb17e0b1ca8e87b5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from ProcessEssays import process_sentences\n",
      "from Decorators import timeit, memoize\n",
      "\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "WINDOW_SIZE = 5\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ   = 5       #i.e. df. Note this is calculated BEFORE creating windows\n",
      "REMOVE_INFREQUENT   = True    # if false, infrequent words are replaced with \"INFREQUENT\"\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = True\n",
      "REPLACE_NUMS        = True    # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 5\n",
      "NUM_LSA_TOPICS      = 30\n",
      "REMOVE_STOP_WORDS   = True\n",
      "REMOVE_PUNCTUATION  = True\n",
      "\n",
      "assert WINDOW_SIZE >= MIN_SENTENCE_LENGTH"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load Data\n",
      "========="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wrap in decorated fn for timing\n",
      "@timeit\n",
      "def load_sentences(essays):\n",
      "    return process_sentences(essays, \n",
      "                              min_df = MIN_SENTENCE_FREQ, \n",
      "                              remove_infrequent = REMOVE_INFREQUENT,\n",
      "                              spelling_correct = SPELLING_CORRECT, \n",
      "                              replace_nums = REPLACE_NUMS, \n",
      "                              stem=STEM,\n",
      "                              remove_stop_words=REMOVE_STOP_WORDS,\n",
      "                              remove_punctuation=REMOVE_PUNCTUATION)\n",
      "# load from disk\n",
      "essays = load_bratt_essays()\n",
      "lbl_sentences = load_sentences(essays)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
        "297 files found\n",
        "297 essays processed"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences, lbls = [], []\n",
      "\n",
      "#unzip for convenience\n",
      "unique_wds = set()\n",
      "unique_tags = set()\n",
      "\n",
      "for lbl_sentence in lbl_sentences:\n",
      "    wds, tags = zip(*lbl_sentence)\n",
      "    \n",
      "    unique_wds.update(wds)\n",
      "    for stag in tags:\n",
      "        unique_tags.update(stag)\n",
      "    \n",
      "    sentences.append(wds)\n",
      "    lbls.append(tags)\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split Data into Windows of Regular Length\n",
      "========================================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" NOTE - some windows will be less than WINDOW_SIZE length \"\"\"\n",
      "from IterableFP import flatten\n",
      "\n",
      "ix2windows = dict()\n",
      "windows, window_lbls = [], []\n",
      "for i, sentence in enumerate(sentences):\n",
      "    if len(sentence) < MIN_SENTENCE_LENGTH:\n",
      "        continue\n",
      "\n",
      "    wins = split_into_windows(sentence, WINDOW_SIZE)\n",
      "    win_lbls = split_into_windows(lbls[i], WINDOW_SIZE)\n",
      "\n",
      "    ix2windows[i] = wins\n",
      "\n",
      "    str_wins = map(lambda win: \" \".join(win), wins)\n",
      "    windows.extend(str_wins)\n",
      "    window_lbls.extend(win_lbls)\n",
      "    \n",
      "for i, w in enumerate(ix2windows[0]):\n",
      "    print i, len(w), [wd.ljust(10) for wd in w]\n",
      "print \"\"\n",
      "for w in windows[0:10]:\n",
      "    print len(w.split(\" \")), w\n",
      "    \n",
      "window_lbls = map(lambda lst: set(flatten(lst)), window_lbls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(use_idf = True, ngram_range = (1, 3), min_df = WINDOW_SIZE*2, binary=False, norm =\"l2\")\n",
      "window_vectors = np.asarray(vectorizer.fit_transform(windows).todense())\n",
      "\n",
      "print window_vectors.shape\n",
      "print window_vectors[0][window_vectors[0] > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Ensure resulting vectors are unit length \"\"\"\n",
      "vec_lens = np.asarray(map(np.linalg.norm,window_vectors))\n",
      "print \"MEAN, MIN, MAX\", np.mean(vec_lens), np.min(vec_lens), np.max(vec_lens)\n",
      "\n",
      "print \"0 length vectors       \", np.asarray(windows)[np.where(vec_lens == 0.0)]\n",
      "print \"Not unit length vectors\", np.asarray(windows)[np.where(vec_lens < 0.9)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import matutils\n",
      "\n",
      "# Compute id2 word mapping needed by gensim\n",
      "wd2id = dict(vectorizer.vocabulary_.items())\n",
      "def swap((k,v)):\n",
      "    return (v,k)\n",
      "id2wd = dict(map(swap, wd2id.items()))\n",
      "\n",
      "#window_vectors_fltrd = np.asarray([w for w in window_vectors if max(w) > 0.0])\n",
      "#corpus = list(matutils.Scipy2Corpus(window_vectors_fltrd))\n",
      "corpus = list(matutils.Scipy2Corpus(window_vectors))\n",
      "print corpus[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import LdaModel, LsiModel\n",
      "\n",
      "def extract_vectors(corpus, model, topics):\n",
      "    return matutils.corpus2dense(model[corpus], num_terms = topics).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsa = LsiModel(corpus = corpus, id2word = id2wd, num_topics = NUM_LSA_TOPICS)\n",
      "lsa_vectors = extract_vectors(corpus, lsa, NUM_LSA_TOPICS)\n",
      "assert len(lsa_vectors) == len(windows), \"lsa vectors must equal window length\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = LdaModel(corpus = corpus, id2word = id2wd, num_topics = NUM_LSA_TOPICS)\n",
      "lda_vectors = extract_vectors(corpus, lda, NUM_LSA_TOPICS)\n",
      "assert len(lda_vectors) == len(windows), \"lda vectors must equal window length\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "def group_clusters(cluster_labels):\n",
      "    clusters = defaultdict(list)\n",
      "    clusters_tags = defaultdict(lambda : defaultdict(int))\n",
      "    for i, lbl in enumerate(cluster_labels):\n",
      "        clusters[lbl].append(windows[i])\n",
      "        if len( window_lbls[i] ) == 0:\n",
      "            clusters_tags[lbl][\"-\"] += 1\n",
      "        else:\n",
      "            for tag in window_lbls[i]:\n",
      "                clusters_tags[lbl][tag] += 1\n",
      "    srtd = map(sorted, clusters.values())\n",
      "    # returns a list of lists of clustered documents, and the counts of the cluster tags\n",
      "    return (clusters, clusters_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import AffinityPropagation, KMeans, DBSCAN, MeanShift, SpectralClustering, Ward"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmclusterer = KMeans(300)\n",
      "print kmclusterer\n",
      "cls = kmclusterer.fit_predict(lsa_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dsclusterer = DBSCAN(metric='euclidean', eps=0.5) # eps should be about 0.1 for LDA\n",
      "print dsclusterer\n",
      "cls = dsclusterer.fit_predict(lsa_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msclusterer = MeanShift(cluster_all = False, min_bin_freq =5, bin_seeding=True) \n",
      "print msclusterer\n",
      "cls = msclusterer.fit_predict(lda_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scclusterer = SpectralClustering(n_clusters = 20)\n",
      "print scclusterer\n",
      "db_lsa_vectors = np.asarray(lsa_vectors, dtype=np.double)\n",
      "db_lda_vectors = np.asarray(lda_vectors, dtype=np.double)\n",
      "cls = scclusterer.fit_predict(db_lda_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wdclusterer = Ward(n_clusters = 20) \n",
      "print wdclusterer\n",
      "cls = wdclusterer.fit_predict(lda_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Entropy import entropy as calc_ent\n",
      "\n",
      "@memoize\n",
      "def regular_tag(tag):\n",
      "    return tag[0].isdigit() or tag == \"-\"\n",
      "\n",
      "regular_tags = set(filter(regular_tag, unique_tags))\n",
      "regular_tags.add(\"-\")\n",
      "special_tags = set(filter(lambda t: not regular_tag(t), unique_tags))\n",
      "print regular_tags\n",
      "\n",
      "def print_tuple(tpl,total):\n",
      "    tag, cnt = tpl\n",
      "    return str(round(cnt/float(total),2)).ljust(4,'0') + \":\" + (\"[%s]\" % tag).rjust(5)\n",
      "\n",
      "def filter_items(tally, lbls):\n",
      "    return filter(lambda (k,v): k in lbls, tally.items())\n",
      "\n",
      "def filter_and_sort(tally):\n",
      "    return sorted(filter_items(tally, regular_tags), key = lambda (k,v):-v)\n",
      "\n",
      "def print_cluster_distribution(tally):\n",
      "    srtd = filter_and_sort(tally)\n",
      "    if not srtd:\n",
      "        total = len(tally.items())\n",
      "        clstr = \"Empty\"\n",
      "    else:\n",
      "        total = sum(zip(*srtd)[1])\n",
      "        prn = lambda tpl: print_tuple(tpl, total)\n",
      "        clstr = \", \".join(map(prn, srtd)[0:min(8, len(srtd))])\n",
      "    print str(total).ljust(5), clstr\n",
      "    return srtd\n",
      "    \n",
      "def compute_entropy(items):\n",
      "    num_items = 0.0\n",
      "    for k, cnt in items:\n",
      "        num_items += float(cnt)\n",
      "\n",
      "    entropy = 0.0\n",
      "    for k, cnt in items:\n",
      "        if cnt <= 0.0:\n",
      "            continue\n",
      "        p = float(cnt) / num_items\n",
      "        assert p <= 1.0\n",
      "        entropy += -1.0 * (p * math.log(p, 2.0))\n",
      "    return entropy\n",
      "\n",
      "def compute_clusters_entropy(cluster_tags, target_lbls):\n",
      "    max_entropy = math.log(len(target_lbls),2)\n",
      "    total_count = 0.0\n",
      "    wt_entropy = 0.0\n",
      "    for i, (key, tally) in enumerate(cluster_tags.items()):\n",
      "        fltrd = filter_items(tally, target_lbls)\n",
      "        if fltrd:\n",
      "            num_items = sum(zip(*fltrd)[1])                \n",
      "            total_count += num_items\n",
      "            entropy = 0.0\n",
      "            for k, cnt in fltrd:\n",
      "                if cnt == 0.0:\n",
      "                    continue\n",
      "                p = float(cnt) / num_items\n",
      "                entropy += -1.0 * (p * math.log(p, 2.0))\n",
      "            # normalize by the number of items (the higher the number of items, the larger the max value)\n",
      "            entropy = entropy / max_entropy\n",
      "            assert entropy <= 1, \"bad ent - \" + key + \" : \" + str(entropy)\n",
      "            wt_entropy += entropy * num_items\n",
      "    return wt_entropy / total_count\n",
      "\n",
      "clusters, cluster_tags = group_clusters(cls)\n",
      "\n",
      "print \"#Clusters:\", len(cluster_tags), \"#Windows\", len(windows)\n",
      "print \"Entropy\", compute_clusters_entropy(cluster_tags, regular_tags)\n",
      "\n",
      "for key in cluster_tags.keys()[:20]:\n",
      "    print str(key).rjust(3,'0'),\n",
      "    print_cluster_distribution(cluster_tags[key])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['11', '13', '12', '14', '-', '50', '1', '3', '2', '5', '4', '7', '6', '5b'])\n",
        "#Clusters: 30 #Windows 14293\n",
        "Entropy 0.51539257441\n",
        "000 88    1.00:  [-]\n",
        "001 1265  0.79: [50], 0.07:  [-], 0.05:  [3], 0.04:  [1], 0.03:  [7], 0.01:  [6], 0.01:  [5], 0.00: [13]\n",
        "002 179   0.91: [50], 0.03:  [3], 0.03:  [-], 0.02:  [1], 0.01:  [7]\n",
        "003 315   0.99: [50], 0.01:  [-]\n",
        "004 393   0.62:  [3], 0.13:  [4], 0.08:  [-], 0.06: [50], 0.04:  [2], 0.04:  [1], 0.02:  [5], 0.01:  [6]\n",
        "005 4170  0.51:  [-], 0.13: [50], 0.07:  [1], 0.07:  [3], 0.05:  [7], 0.03:  [2], 0.03: [11], 0.02: [13]\n",
        "006 176   0.68: [14], 0.18:  [-], 0.08: [13], 0.03:  [5], 0.02:  [3], 0.01:  [1]\n",
        "007 292   0.68: [50], 0.13:  [-], 0.07:  [7], 0.03:  [1], 0.02:  [3], 0.02: [11], 0.02:  [5], 0.01:  [6]\n",
        "008 615   0.56:  [1], 0.20:  [-], 0.13:  [3], 0.07: [50], 0.02:  [2], 0.01:  [7], 0.00:  [6]\n",
        "009 592   0.54:  [5], 0.20:  [-], 0.08:  [3], 0.07:  [4], 0.03: [14], 0.03:  [7], 0.03: [50], 0.01: [5b]\n",
        "010 324   0.60:  [-], 0.33:  [3], 0.02:  [5], 0.02: [50], 0.02:  [2], 0.01:  [1]\n",
        "011 196   0.29:  [1], 0.27:  [2], 0.23:  [-], 0.19:  [3], 0.02: [50]\n",
        "012 805   0.28:  [3], 0.26:  [1], 0.19:  [-], 0.10: [50], 0.05:  [2], 0.03: [11], 0.03: [13], 0.02:  [7]\n",
        "013 114   0.65:  [1], 0.18:  [-], 0.09:  [3], 0.04: [50], 0.04:  [2]\n",
        "014 444   0.36: [13], 0.23: [12], 0.16:  [-], 0.12: [11], 0.07: [14], 0.02:  [3], 0.01: [50], 0.01:  [1]\n",
        "015 347   0.61:  [-], 0.17: [50], 0.12:  [1], 0.07:  [3], 0.01:  [7], 0.01: [13], 0.01:  [2], 0.00:  [5]\n",
        "016 316   0.52:  [4], 0.19:  [-], 0.13:  [3], 0.13:  [5], 0.03: [50], 0.01: [14]\n",
        "017 786   0.43:  [7], 0.34:  [-], 0.12: [50], 0.05:  [5], 0.02: [5b], 0.01:  [3], 0.01:  [6], 0.01:  [1]\n",
        "018 226   0.65: [50], 0.20:  [-], 0.12:  [7], 0.02:  [1], 0.01:  [6]\n",
        "019 239   0.67: [50], 0.21:  [7], 0.13:  [-], 0.00:  [5]\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Tally import tally\n",
      "\n",
      "def cluster_summary(cluster):\n",
      "    tally = defaultdict(int)\n",
      "    for win in cluster:\n",
      "        splt = win.split(\" \")\n",
      "        for item in splt:\n",
      "            tally[item]+=1\n",
      "    srtd = sorted(tally.items(), key = lambda (k,v): -v)\n",
      "    return \" \".join(zip(*srtd[0:WINDOW_SIZE*2])[0])\n",
      "\n",
      "cnt_below = 0\n",
      "cnt_above = 0\n",
      "tags = defaultdict(int)\n",
      "for key in cluster_tags.keys()[:]:\n",
      "    cluster = clusters[key]\n",
      "    if len(cluster) < 50:\n",
      "        cnt_below += 1\n",
      "    else:\n",
      "        cnt_above += 1\n",
      "        print key, \n",
      "        top_tags = print_cluster_distribution(cluster_tags[key])\n",
      "        if top_tags:\n",
      "            tags[zip(*top_tags)[0][0]] += 1\n",
      "        print len(cluster), \"items, Summary:\", cluster_summary(cluster)\n",
      "        #for k,v in sorted(tally(cluster).items(), key = lambda (k,v): -v):\n",
      "        #    print key, v, k\n",
      "        print \"\"\n",
      "\n",
      "print cnt_below, \"below\"\n",
      "print cnt_above, \"above\"\n",
      "print sorted(tags.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 230   0.49:  [-], 0.18:  [3], 0.07:  [1], 0.06:  [2], 0.04:  [5], 0.03: [11], 0.03: [13], 0.03: [50]\n",
        "213 items, Summary: water , . make coral colder move warmer come salti\n",
        "\n",
        "4 152   1.00: [50]\n",
        "152 items, Summary: rate differ bleach vari coral ,\n",
        "\n",
        "6 120   0.52:  [1], 0.31:  [-], 0.07: [50], 0.06:  [2], 0.03:  [3], 0.01: [11]\n",
        "109 items, Summary: wind , direct . weaken blow revers alter strength warm\n",
        "\n",
        "7 180   0.57:  [5], 0.20:  [-], 0.07:  [4], 0.04:  [3], 0.04: [14], 0.04: [50], 0.03:  [7], 0.01: [5b]\n",
        "155 items, Summary: photosynthesi , coral . go make disrupt use process decreas\n",
        "\n",
        "8 58    0.55: [50], 0.29:  [-], 0.12:  [7], 0.03:  [1]\n",
        "51 items, Summary: color lose coral . bleach , phenomenon event \" start\n",
        "\n",
        "9 80    0.76: [14], 0.16:  [-], 0.04: [13], 0.03:  [5], 0.01:  [1]\n",
        "79 items, Summary: coral keep healthi balanc . requir necessari threaten , salin\n",
        "\n",
        "10 116   0.85:  [-], 0.05: [50], 0.03:  [3], 0.03:  [7], 0.02:  [4], 0.02:  [6], 0.01: [13]\n",
        "124 items, Summary: one . , thing stay place tend stressor environment exampl\n",
        "\n",
        "12 84    0.45:  [-], 0.25: [50], 0.07:  [1], 0.06:  [3], 0.05: [11], 0.05:  [7], 0.04:  [6], 0.02:  [5]\n",
        "76 items, Summary: happen coral . bleach , ( temperatur die world think\n",
        "\n",
        "13 148   0.82: [50], 0.07:  [3], 0.06:  [1], 0.02:  [7], 0.01: [11], 0.01: [14], 0.01:  [2]\n",
        "122 items, Summary: coral caus bleach . thing , also temperatur chang could\n",
        "\n",
        "19 149   1.00: [50]\n",
        "149 items, Summary: rate time vari differ bleach\n",
        "\n",
        "23 39    0.41: [50], 0.38:  [-], 0.05:  [6], 0.03: [13], 0.03:  [1], 0.03:  [3], 0.03:  [5], 0.03:  [7]\n",
        "61 items, Summary: reason coral mani , thing . anoth effect one rate\n",
        "\n",
        "25 339   0.51:  [-], 0.18: [50], 0.11:  [7], 0.03:  [6], 0.03: [14], 0.03: [5b], 0.03:  [5], 0.02: [13]\n",
        "333 items, Summary: coral , . diseas \" vulner zooxanthalla energi starvat surviv\n",
        "\n",
        "26 273   0.84: [50], 0.07:  [-], 0.03:  [3], 0.02:  [7], 0.02:  [1], 0.01: [13], 0.01:  [2], 0.00: [11]\n",
        "251 items, Summary: coral bleach . , seriou report lead problem countri event\n",
        "\n",
        "27 87    0.76:  [3], 0.09:  [4], 0.08:  [-], 0.03: [50], 0.02:  [1], 0.01:  [5]\n",
        "73 items, Summary: increas temperatur water 0 , ocean coral solubl f -\n",
        "\n",
        "28 73    0.55:  [-], 0.27:  [7], 0.11: [50], 0.03: [5b], 0.01: [11], 0.01:  [3], 0.01:  [5]\n",
        "68 items, Summary: alga , coral zooxanthella . relationship zooxanthalla ( surviv weather\n",
        "\n",
        "29 86    0.38: [13], 0.37:  [-], 0.08: [12], 0.07: [11], 0.05: [14], 0.02:  [1], 0.02:  [3]\n",
        "75 items, Summary: ocean salin sensit . , coral also drop chang \"\n",
        "\n",
        "39 67    0.51:  [6], 0.28:  [7], 0.13:  [-], 0.03:  [3], 0.03: [5b], 0.01: [50]\n",
        "61 items, Summary: coral environ stress forc eject may lot food , mani\n",
        "\n",
        "42 90    0.64:  [1], 0.11:  [3], 0.11:  [2], 0.03: [11], 0.03: [50], 0.02: [13], 0.02:  [-], 0.02:  [7]\n",
        "67 items, Summary: chang wind direct , weaken . water either time go\n",
        "\n",
        "43 55    1.00:  [-]\n",
        "57 items, Summary: anim call tini polyp made . coral , realli actual\n",
        "\n",
        "45 81    0.60:  [-], 0.28: [50], 0.06:  [1], 0.02:  [7], 0.01: [13], 0.01:  [3]\n",
        "83 items, Summary: time differ bleach . , coral depend take everi long\n",
        "\n",
        "47 119   0.65:  [-], 0.13: [13], 0.09: [50], 0.04: [11], 0.04:  [1], 0.03:  [3], 0.01:  [2]\n",
        "120 items, Summary: ocean coral 0 . , salt / cover caus bleach\n",
        "\n",
        "48 205   0.53:  [3], 0.18:  [1], 0.11:  [-], 0.09: [50], 0.05:  [2], 0.02: [13], 0.01:  [4], 0.00:  [5]\n",
        "148 items, Summary: water temperatur , . high higher tradewind coral weaker low\n",
        "\n",
        "49 90    0.78:  [3], 0.07: [50], 0.06:  [1], 0.04:  [5], 0.03:  [-], 0.01: [13], 0.01:  [6]\n",
        "74 items, Summary: water chang temperatur . sensit , photosynthesi coral bleach wind\n",
        "\n",
        "51 74    0.81: [50], 0.09:  [1], 0.07:  [3], 0.03:  [7]\n",
        "60 items, Summary: coral bleach affect . thing , rate strength neg temperatur\n",
        "\n",
        "52 56    0.46:  [-], 0.23: [50], 0.11:  [3], 0.07:  [5], 0.05:  [7], 0.04:  [4], 0.04: [5b]\n",
        "51 items, Summary: coral get bleach , make nutrient . diseas enough energi\n",
        "\n",
        "53 49    0.67:  [-], 0.18: [50], 0.06:  [3], 0.04:  [1], 0.02:  [2], 0.02:  [7]\n",
        "52 items, Summary: year 0000 everi , water bleach sever coral differ \"\n",
        "\n",
        "54 103   0.41:  [-], 0.13: [50], 0.13:  [1], 0.12:  [2], 0.06:  [7], 0.05:  [3], 0.05:  [5], 0.02: [11]\n",
        "94 items, Summary: caus , shift major world eastward wind . death problem\n",
        "\n",
        "56 84    0.58:  [1], 0.29:  [-], 0.06:  [3], 0.02: [50], 0.02:  [2], 0.01:  [7], 0.01:  [6]\n",
        "77 items, Summary: wind trade , . alter \" weaker strength articl coral\n",
        "\n",
        "57 110   0.43:  [3], 0.32:  [-], 0.10:  [1], 0.08: [50], 0.02: [11], 0.02:  [4], 0.01: [13], 0.01:  [2]\n",
        "95 items, Summary: temperatur , . coral high link rise could level variat\n",
        "\n",
        "61 115   0.53:  [2], 0.24:  [1], 0.17:  [-], 0.03: [50], 0.02:  [3]\n",
        "103 items, Summary: east west wind travel blow . direct go , complet\n",
        "\n",
        "67 57    0.51:  [-], 0.21:  [1], 0.14: [50], 0.11:  [3], 0.04:  [2]\n",
        "58 items, Summary: pacif ocean . water wind notic , bleach coral might\n",
        "\n",
        "70 61    0.87:  [-], 0.07: [50], 0.02: [13], 0.02:  [1], 0.02:  [3], 0.02:  [7]\n",
        "73 items, Summary: differ rate vari time . , ? think process text\n",
        "\n",
        "73 73    0.60: [14], 0.21:  [-], 0.15: [13], 0.03:  [3], 0.01:  [5]\n",
        "66 items, Summary: keep balanc threaten coral necessari requir delic salin healthi ,\n",
        "\n",
        "78 59    0.78:  [-], 0.10: [50], 0.07:  [3], 0.02: [14], 0.02:  [4], 0.02:  [7]\n",
        "59 items, Summary: reef seriou coral world impact problem - build . ,\n",
        "\n",
        "80 58    0.36: [50], 0.24:  [-], 0.14:  [7], 0.09: [14], 0.05:  [5], 0.03:  [1], 0.03:  [6], 0.02: [11]\n",
        "50 items, Summary: caus coral , unhealthi go . becom stress healthi sometim\n",
        "\n",
        "81 49    0.43:  [-], 0.37:  [1], 0.12:  [3], 0.08: [50]\n",
        "50 items, Summary: time chang differ wind . temperatur due rate , year\n",
        "\n",
        "85 93    0.40: [13], 0.24: [11], 0.13: [12], 0.11: [14], 0.10:  [-], 0.01: [50], 0.01:  [3], 0.01:  [7]\n",
        "55 items, Summary: salin drop , storm coral threaten hurrican level fresh .\n",
        "\n",
        "86 74    0.46:  [-], 0.27: [50], 0.23:  [7], 0.01:  [4], 0.01:  [3], 0.01:  [5]\n",
        "69 items, Summary: coral die . , becom white bleach make live start\n",
        "\n",
        "87 115   0.92:  [-], 0.03: [5b], 0.02: [50], 0.02:  [5], 0.01:  [7]\n",
        "117 items, Summary: polyp coral bodi anim , made . live coloni plant\n",
        "\n",
        "88 59    0.53:  [-], 0.46:  [3], 0.02:  [5]\n",
        "59 items, Summary: 00 - f . % , ) ( \\ coral\n",
        "\n",
        "89 97    0.66:  [-], 0.11: [50], 0.06: [13], 0.04:  [3], 0.04:  [5], 0.03:  [4], 0.02: [11], 0.01:  [1]\n",
        "102 items, Summary: affect coral , neg . relationship environment stressor bleach thing\n",
        "\n",
        "91 132   0.58:  [2], 0.24:  [-], 0.09:  [3], 0.06:  [1], 0.02: [50]\n",
        "124 items, Summary: water surfac warm , eastward . east drag shift travel\n",
        "\n",
        "95 103   0.64:  [-], 0.17:  [3], 0.15:  [2], 0.03: [50], 0.01:  [1]\n",
        "102 items, Summary: pacif water eastern region colder surfac . western rise warm\n",
        "\n",
        "97 117   0.56:  [-], 0.15: [50], 0.08:  [3], 0.06:  [1], 0.05: [14], 0.03:  [7], 0.03: [11], 0.03: [13]\n",
        "118 items, Summary: also . , coral bleach temperatur graph anoth ? \"\n",
        "\n",
        "101 134   0.62:  [-], 0.20: [50], 0.09:  [1], 0.04:  [7], 0.03:  [3], 0.01:  [2], 0.01:  [5]\n",
        "131 items, Summary: 0000 year , . bleach coral graph report per one\n",
        "\n",
        "102 90    0.66:  [3], 0.09:  [-], 0.07: [50], 0.04: [11], 0.04:  [5], 0.04:  [7], 0.03:  [6], 0.02:  [1]\n",
        "67 items, Summary: temperatur chang , coral . would bleach alga high ocean\n",
        "\n",
        "105 68    0.79:  [-], 0.12: [50], 0.03:  [1], 0.01: [13], 0.01:  [3], 0.01:  [7], 0.01:  [6]\n",
        "70 items, Summary: differ time coral depend due . , process everi environ\n",
        "\n",
        "108 85    0.64:  [-], 0.18:  [1], 0.13:  [2], 0.04:  [3], 0.01: [13], 0.01: [50]\n",
        "82 items, Summary: 0 surfac / . direct entir complet revers globe west\n",
        "\n",
        "109 57    0.53:  [-], 0.44: [50], 0.02:  [5], 0.02:  [7]\n",
        "57 items, Summary: color white bleach coral plain . becom , lost turn\n",
        "\n",
        "110 86    0.62:  [3], 0.10: [50], 0.10:  [1], 0.07:  [-], 0.06:  [4], 0.05:  [2]\n",
        "63 items, Summary: water temperatur increas , coral decreas . swell east solubl\n",
        "\n",
        "113 90    0.68:  [-], 0.14:  [3], 0.10: [50], 0.07:  [1], 0.01:  [5]\n",
        "91 items, Summary: 00 - . f , \\ degre coral 0 %\n",
        "\n",
        "119 60    0.62:  [-], 0.12: [50], 0.10:  [7], 0.08: [5b], 0.07:  [5], 0.02:  [4]\n",
        "58 items, Summary: need coral . , surviv nutrient zooxanthalla stay energi bleach\n",
        "\n",
        "123 88    0.31:  [-], 0.23:  [1], 0.19: [50], 0.18:  [3], 0.06: [13], 0.03:  [2]\n",
        "72 items, Summary: chang caus coral temperatur , wind . water direct weather\n",
        "\n",
        "124 79    0.51:  [4], 0.24:  [-], 0.09:  [3], 0.09:  [5], 0.04: [14], 0.03: [50], 0.01: [13]\n",
        "63 items, Summary: water decreas co2 dioxid carbon . solubl , sugar )\n",
        "\n",
        "125 87    0.72: [50], 0.08:  [3], 0.06:  [1], 0.05:  [7], 0.03:  [-], 0.03:  [5], 0.01:  [4], 0.01:  [6]\n",
        "66 items, Summary: coral occur bleach . , way differ factor temperatur case\n",
        "\n",
        "126 92    0.45:  [7], 0.41: [50], 0.14:  [-]\n",
        "69 items, Summary: bleach result , death zooxanthella without . long zooxanthalla coral\n",
        "\n",
        "128 52    0.50:  [7], 0.48:  [-], 0.02:  [4]\n",
        "52 items, Summary: coral zooxanthella long without goe , . relationship zooxanthalla symbiot\n",
        "\n",
        "129 72    0.76: [50], 0.08:  [7], 0.04:  [3], 0.03:  [1], 0.03:  [6], 0.01: [11], 0.01:  [-], 0.01:  [5]\n",
        "56 items, Summary: coral happen bleach . , ? differ would goe eject\n",
        "\n",
        "131 97    0.39:  [-], 0.31:  [3], 0.11:  [1], 0.07: [13], 0.06: [50], 0.02: [11], 0.01:  [4], 0.01:  [2]\n",
        "81 items, Summary: water , temp . salt higher weaker high coral graph\n",
        "\n",
        "133 67    0.58: [50], 0.37:  [-], 0.03:  [7], 0.01: [5b]\n",
        "64 items, Summary: white coral bleach . \" plain becom turn , make\n",
        "\n",
        "137 67    0.73:  [-], 0.07: [11], 0.06: [12], 0.04: [13], 0.03: [50], 0.01: [14], 0.01:  [1], 0.01:  [3]\n",
        "62 items, Summary: , fish blast tourist drop anchor like walk peopl damag\n",
        "\n",
        "138 63    0.97:  [-], 0.02:  [3], 0.02: [50]\n",
        "66 items, Summary: drop anchor , tourist fish blast walk . peopl boat\n",
        "\n",
        "140 82    0.50:  [4], 0.24:  [-], 0.20:  [5], 0.04:  [3], 0.01: [14], 0.01: [50]\n",
        "65 items, Summary: carbon dioxid decreas disrupt creat co2 ( process coral photosynthesi\n",
        "\n",
        "142 84    0.86: [50], 0.11:  [-], 0.02:  [1], 0.01:  [3]\n",
        "81 items, Summary: coral bleach 0000 year report , . sever per lot\n",
        "\n",
        "148 118   0.53: [12], 0.33: [13], 0.11: [11], 0.03:  [-]\n",
        "69 items, Summary: water drop ocean fresh salin , storm salt . less\n",
        "\n",
        "152 88    0.98:  [-], 0.02:  [3]\n",
        "89 items, Summary: live coral . , outsid coloni temperatur cannot stay tend\n",
        "\n",
        "155 69    0.83:  [-], 0.09:  [5], 0.04:  [7], 0.03: [5b], 0.01: [50]\n",
        "77 items, Summary: alga live coral polyp made sugar within , . feed\n",
        "\n",
        "156 76    0.57:  [-], 0.17: [50], 0.11:  [3], 0.07: [11], 0.05:  [1], 0.03:  [7], 0.01: [5b]\n",
        "73 items, Summary: get . bleach higher diseas , temperatur starv ocean may\n",
        "\n",
        "159 94    0.27:  [-], 0.17: [13], 0.17: [12], 0.17:  [3], 0.16:  [2], 0.05: [11], 0.01:  [1]\n",
        "71 items, Summary: water ocean , salt shift eastward atlant amount . found\n",
        "\n",
        "160 98    0.61:  [1], 0.16:  [-], 0.12: [50], 0.10:  [3]\n",
        "82 items, Summary: trade wind weaker , . strength way show graph coral\n",
        "\n",
        "163 83    0.35:  [1], 0.34:  [-], 0.24:  [3], 0.06: [50], 0.01:  [7]\n",
        "61 items, Summary: temperatur wind , . high show speed less effect sinc\n",
        "\n",
        "177 33    0.70:  [-], 0.15:  [3], 0.09: [50], 0.06:  [1]\n",
        "58 items, Summary: differ mani . time thing reason due type factor discov\n",
        "\n",
        "182 185   0.58:  [-], 0.17:  [7], 0.09:  [5], 0.09: [50], 0.03:  [3], 0.02: [5b], 0.01: [11], 0.01: [13]\n",
        "171 items, Summary: alga coral , sugar . made feed creat eat give\n",
        "\n",
        "184 154   0.48:  [-], 0.36: [50], 0.08:  [7], 0.03:  [5], 0.02:  [4], 0.02:  [6], 0.01:  [1], 0.01: [13]\n",
        "144 items, Summary: color coral . , give bleach lost lose \" blue\n",
        "\n",
        "190 71    0.72:  [-], 0.24:  [3], 0.03:  [2], 0.01:  [1]\n",
        "70 items, Summary: 00 - f % degre , energi region coral \\\n",
        "\n",
        "194 52    0.81:  [-], 0.10:  [3], 0.08: [11], 0.02: [50]\n",
        "53 items, Summary: water shallow , clear tropic . found coral reef trade\n",
        "\n",
        "197 46    1.00:  [-]\n",
        "50 items, Summary: , shallow tropic water clear . alga call get live\n",
        "\n",
        "199 65    0.62:  [1], 0.32:  [-], 0.05:  [3], 0.02: [50]\n",
        "65 items, Summary: trade wind shift . , \" : vari / ?\n",
        "\n",
        "201 57    0.84: [50], 0.12:  [7], 0.04:  [-]\n",
        "52 items, Summary: white turn coral bleach , due . \" process plant\n",
        "\n",
        "202 73    0.48: [14], 0.21:  [-], 0.15:  [5], 0.05:  [4], 0.04:  [3], 0.03: [50], 0.03:  [7], 0.01: [13]\n",
        "61 items, Summary: threaten balanc disrupt healthi delic coral keep , . requir\n",
        "\n",
        "212 52    0.56:  [-], 0.31:  [5], 0.08:  [3], 0.04: [50], 0.02:  [7]\n",
        "50 items, Summary: photosynthesi coral \" , . state process energi document reef\n",
        "\n",
        "219 96    0.52:  [-], 0.16: [5b], 0.14:  [5], 0.13:  [7], 0.05:  [6], 0.01: [50]\n",
        "94 items, Summary: food make coral energi sun , . pass zooxanthella environ\n",
        "\n",
        "222 60    0.95: [50], 0.03:  [1], 0.02:  [3]\n",
        "57 items, Summary: rate bleach coral differ reason , way chang ? may\n",
        "\n",
        "229 128   0.44:  [-], 0.13: [11], 0.12:  [3], 0.09: [13], 0.07: [50], 0.06:  [1], 0.03:  [7], 0.02:  [4]\n",
        "112 items, Summary: chang , coral . environ weather like storm / thing\n",
        "\n",
        "230 101   0.49:  [3], 0.30:  [-], 0.11:  [1], 0.06: [50], 0.02: [13], 0.01:  [4], 0.01:  [5], 0.01: [5b]\n",
        "85 items, Summary: water temperatur , . 0 higher tradewind high lower coral\n",
        "\n",
        "234 110   0.30: [12], 0.23: [13], 0.22: [11], 0.21:  [-], 0.02:  [7], 0.01: [14], 0.01:  [1], 0.01:  [3]\n",
        "78 items, Summary: water salt , fresh salin drop amount . rain storm\n",
        "\n",
        "238 87    0.69:  [-], 0.14: [50], 0.08:  [3], 0.06:  [1], 0.03:  [7]\n",
        "87 items, Summary: differ time vari due depend coral bleach environ temperatur .\n",
        "\n",
        "243 174   0.75: [50], 0.07:  [-], 0.06:  [3], 0.05:  [1], 0.03:  [7], 0.02:  [6], 0.01:  [5], 0.01: [13]\n",
        "144 items, Summary: coral bleach . , \" / go rate water lot\n",
        "\n",
        "245 61    0.44:  [-], 0.34:  [2], 0.13:  [1], 0.08:  [3]\n",
        "55 items, Summary: caus water surfac colder warm eastward east upwel major ,\n",
        "\n",
        "248 135   0.61:  [5], 0.16:  [-], 0.16:  [4], 0.04: [14], 0.03: [50], 0.01:  [3]\n",
        "105 items, Summary: photosynthesi process disrupt , . creat sunlight sugar coral decreas\n",
        "\n",
        "254 63    0.38: [50], 0.32:  [-], 0.06:  [7], 0.05: [13], 0.05:  [5], 0.03:  [4], 0.03:  [2], 0.03: [5b]\n",
        "57 items, Summary: caus coral , bleach . becom effect differ white zooxanthalla\n",
        "\n",
        "255 523   0.71:  [-], 0.06:  [1], 0.06: [11], 0.04:  [3], 0.03:  [5], 0.02: [13], 0.02:  [2], 0.01: [12]\n",
        "513 items, Summary: , . 0 \\ ? \" f o super nosupersub\n",
        "\n",
        "260 36    0.78:  [-], 0.08:  [3], 0.06: [50], 0.06:  [1], 0.03: [13]\n",
        "50 items, Summary: differ time . mani , vari reason temperatur due occur\n",
        "\n",
        "262 66    0.27:  [-], 0.23: [50], 0.23:  [7], 0.09:  [3], 0.08:  [5], 0.03: [5b], 0.03: [12], 0.03:  [4]\n",
        "56 items, Summary: die . , bleach could also start eventu make much\n",
        "\n",
        "264 110   0.75: [50], 0.14:  [-], 0.05:  [7], 0.03:  [1], 0.02:  [3], 0.01: [13], 0.01: [14], 0.01:  [5]\n",
        "99 items, Summary: bleach coral . , make seriou could vulner \" event\n",
        "\n",
        "268 73    0.66: [50], 0.23:  [-], 0.08:  [7], 0.03:  [1]\n",
        "68 items, Summary: color lose coral bleach phenomenon . , becom begin event\n",
        "\n",
        "269 70    0.93: [50], 0.04:  [1], 0.03:  [-]\n",
        "67 items, Summary: bleach coral differ vari time rate . , amount due\n",
        "\n",
        "280 80    0.65:  [-], 0.16:  [1], 0.11: [50], 0.06:  [3], 0.01:  [2]\n",
        "93 items, Summary: differ rate vari time due coral depend water environ mani\n",
        "\n",
        "294 75    0.60:  [-], 0.16: [50], 0.13:  [3], 0.04:  [1], 0.03:  [7], 0.01:  [2], 0.01:  [4], 0.01: [5b]\n",
        "78 items, Summary: reef coral . bleach get damag lot impact , temperatur\n",
        "\n",
        "201 below\n",
        "99 above\n",
        "[('-', 51), ('1', 6), ('12', 2), ('13', 2), ('14', 3), ('2', 2), ('3', 7), ('4', 2), ('5', 2), ('50', 19), ('6', 1), ('7', 2)]\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ntopics = [5, 10, 20, 30, 40, 50, 60, 80, 100, 200, 300]\n",
      "dlsa_vecs = dict()\n",
      "for ntopic in ntopics:\n",
      "    lsa = LsiModel(corpus = corpus, id2word = id2wd, num_topics = ntopic)\n",
      "    lsa_vectors = extract_vectors(corpus, lsa, ntopic)\n",
      "    dlsa_vecs[ntopic] = lsa_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,527 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,527 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,528 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,615 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,617 : INFO : 1st phase: constructing (1688, 105) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,695 : INFO : orthonormalizing (1688, 105) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,780 : INFO : 2nd phase: running dense svd on (105, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,999 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:37,999 : INFO : keeping 5 factors (discarding 77.546% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,000 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,001 : INFO : topic #0(22.427): -0.321*\"differ\" + -0.313*\"vari\" + -0.293*\"rate\" + -0.289*\"vari differ\" + -0.281*\"bleach\" + -0.277*\"rate vari\" + -0.252*\"rate vari differ\" + -0.244*\"time\" + -0.237*\"differ time\" + -0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,001 : INFO : topic #1(16.885): -0.464*\"coral\" + -0.337*\"coral bleach\" + -0.329*\"bleach\" + -0.214*\"temperatur\" + -0.203*\"water\" + 0.165*\"vari differ\" + 0.162*\"vari differ time\" + 0.158*\"differ time\" + 0.146*\"differ\" + -0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,002 : INFO : topic #2(14.964): -0.411*\"temperatur\" + -0.373*\"water\" + -0.285*\"00\" + 0.271*\"coral bleach\" + -0.259*\"water temperatur\" + 0.252*\"bleach\" + -0.234*\"wind\" + 0.223*\"coral\" + -0.161*\"trade\" + -0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,002 : INFO : topic #3(13.426): 0.560*\"00\" + -0.448*\"wind\" + -0.354*\"trade\" + -0.338*\"trade wind\" + 0.224*\"00 00\" + 0.133*\"temperatur 00\" + -0.124*\"chang\" + -0.100*\"shift\" + -0.099*\"shift trade\" + -0.092*\"shift trade wind\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,003 : INFO : topic #4(12.466): -0.577*\"00\" + 0.343*\"water\" + -0.289*\"wind\" + -0.274*\"trade\" + -0.267*\"trade wind\" + -0.244*\"00 00\" + 0.196*\"temperatur\" + 0.185*\"water temperatur\" + 0.114*\"increas\" + -0.085*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,385 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,385 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,386 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,460 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,461 : INFO : 1st phase: constructing (1688, 110) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,546 : INFO : orthonormalizing (1688, 110) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,628 : INFO : 2nd phase: running dense svd on (110, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,882 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,883 : INFO : keeping 10 factors (discarding 68.818% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,885 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,886 : INFO : topic #0(22.427): -0.321*\"differ\" + -0.313*\"vari\" + -0.293*\"rate\" + -0.289*\"vari differ\" + -0.281*\"bleach\" + -0.277*\"rate vari\" + -0.252*\"rate vari differ\" + -0.244*\"time\" + -0.237*\"differ time\" + -0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,886 : INFO : topic #1(16.885): -0.464*\"coral\" + -0.337*\"coral bleach\" + -0.329*\"bleach\" + -0.214*\"temperatur\" + -0.203*\"water\" + 0.165*\"vari differ\" + 0.162*\"vari differ time\" + 0.158*\"differ time\" + 0.146*\"differ\" + -0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,887 : INFO : topic #2(14.964): 0.411*\"temperatur\" + 0.373*\"water\" + 0.284*\"00\" + -0.271*\"coral bleach\" + 0.259*\"water temperatur\" + -0.252*\"bleach\" + 0.234*\"wind\" + -0.223*\"coral\" + 0.161*\"trade\" + 0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,887 : INFO : topic #3(13.426): 0.561*\"00\" + -0.447*\"wind\" + -0.355*\"trade\" + -0.338*\"trade wind\" + 0.224*\"00 00\" + 0.132*\"temperatur 00\" + -0.124*\"chang\" + -0.100*\"shift\" + -0.099*\"shift trade\" + -0.092*\"shift trade wind\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:38,888 : INFO : topic #4(12.466): 0.576*\"00\" + -0.344*\"water\" + 0.289*\"wind\" + 0.274*\"trade\" + 0.267*\"trade wind\" + 0.242*\"00 00\" + -0.197*\"temperatur\" + -0.185*\"water temperatur\" + -0.112*\"increas\" + 0.085*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,301 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,302 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,302 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,376 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,377 : INFO : 1st phase: constructing (1688, 120) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,457 : INFO : orthonormalizing (1688, 120) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,540 : INFO : 2nd phase: running dense svd on (120, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,797 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,797 : INFO : keeping 20 factors (discarding 57.393% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,801 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,802 : INFO : topic #0(22.427): 0.321*\"differ\" + 0.313*\"vari\" + 0.293*\"rate\" + 0.289*\"vari differ\" + 0.281*\"bleach\" + 0.277*\"rate vari\" + 0.252*\"rate vari differ\" + 0.244*\"time\" + 0.237*\"differ time\" + 0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,802 : INFO : topic #1(16.885): 0.464*\"coral\" + 0.337*\"coral bleach\" + 0.329*\"bleach\" + 0.214*\"temperatur\" + 0.203*\"water\" + -0.165*\"vari differ\" + -0.162*\"vari differ time\" + -0.158*\"differ time\" + -0.146*\"differ\" + 0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,803 : INFO : topic #2(14.964): 0.411*\"temperatur\" + 0.373*\"water\" + 0.284*\"00\" + -0.271*\"coral bleach\" + 0.259*\"water temperatur\" + -0.252*\"bleach\" + 0.234*\"wind\" + -0.224*\"coral\" + 0.161*\"trade\" + 0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,803 : INFO : topic #3(13.426): 0.561*\"00\" + -0.447*\"wind\" + -0.354*\"trade\" + -0.338*\"trade wind\" + 0.224*\"00 00\" + 0.132*\"temperatur 00\" + -0.123*\"chang\" + -0.100*\"shift\" + -0.098*\"shift trade\" + 0.092*\"temperatur\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:39,804 : INFO : topic #4(12.467): -0.576*\"00\" + 0.342*\"water\" + -0.289*\"wind\" + -0.274*\"trade\" + -0.267*\"trade wind\" + -0.245*\"00 00\" + 0.198*\"temperatur\" + 0.184*\"water temperatur\" + 0.113*\"increas\" + -0.086*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,344 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,344 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,345 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,418 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,419 : INFO : 1st phase: constructing (1688, 130) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,507 : INFO : orthonormalizing (1688, 130) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,593 : INFO : 2nd phase: running dense svd on (130, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,893 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,894 : INFO : keeping 30 factors (discarding 49.630% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,896 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,897 : INFO : topic #0(22.427): -0.321*\"differ\" + -0.313*\"vari\" + -0.293*\"rate\" + -0.289*\"vari differ\" + -0.281*\"bleach\" + -0.277*\"rate vari\" + -0.252*\"rate vari differ\" + -0.244*\"time\" + -0.237*\"differ time\" + -0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,897 : INFO : topic #1(16.885): 0.464*\"coral\" + 0.337*\"coral bleach\" + 0.329*\"bleach\" + 0.214*\"temperatur\" + 0.202*\"water\" + -0.165*\"vari differ\" + -0.162*\"vari differ time\" + -0.158*\"differ time\" + -0.146*\"differ\" + 0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,898 : INFO : topic #2(14.964): -0.411*\"temperatur\" + -0.373*\"water\" + -0.284*\"00\" + 0.271*\"coral bleach\" + -0.259*\"water temperatur\" + 0.252*\"bleach\" + -0.234*\"wind\" + 0.223*\"coral\" + -0.161*\"trade\" + -0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,898 : INFO : topic #3(13.426): 0.561*\"00\" + -0.447*\"wind\" + -0.354*\"trade\" + -0.338*\"trade wind\" + 0.224*\"00 00\" + 0.132*\"temperatur 00\" + -0.123*\"chang\" + -0.100*\"shift\" + -0.098*\"shift trade\" + -0.092*\"shift trade wind\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:40,899 : INFO : topic #4(12.467): -0.576*\"00\" + 0.343*\"water\" + -0.288*\"wind\" + -0.274*\"trade\" + -0.268*\"trade wind\" + -0.244*\"00 00\" + 0.197*\"temperatur\" + 0.185*\"water temperatur\" + 0.112*\"increas\" + -0.085*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,487 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,488 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,489 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,560 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,560 : INFO : 1st phase: constructing (1688, 140) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,652 : INFO : orthonormalizing (1688, 140) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:41,747 : INFO : 2nd phase: running dense svd on (140, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,037 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,037 : INFO : keeping 40 factors (discarding 43.674% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,039 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,039 : INFO : topic #0(22.427): -0.321*\"differ\" + -0.313*\"vari\" + -0.293*\"rate\" + -0.289*\"vari differ\" + -0.281*\"bleach\" + -0.277*\"rate vari\" + -0.252*\"rate vari differ\" + -0.244*\"time\" + -0.237*\"differ time\" + -0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,040 : INFO : topic #1(16.885): 0.464*\"coral\" + 0.337*\"coral bleach\" + 0.329*\"bleach\" + 0.214*\"temperatur\" + 0.203*\"water\" + -0.165*\"vari differ\" + -0.162*\"vari differ time\" + -0.158*\"differ time\" + -0.146*\"differ\" + 0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,040 : INFO : topic #2(14.965): -0.411*\"temperatur\" + -0.373*\"water\" + -0.284*\"00\" + 0.271*\"coral bleach\" + -0.259*\"water temperatur\" + 0.252*\"bleach\" + -0.235*\"wind\" + 0.223*\"coral\" + -0.161*\"trade\" + -0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,041 : INFO : topic #3(13.426): -0.561*\"00\" + 0.448*\"wind\" + 0.354*\"trade\" + 0.338*\"trade wind\" + -0.224*\"00 00\" + -0.132*\"temperatur 00\" + 0.123*\"chang\" + 0.100*\"shift\" + 0.098*\"shift trade\" + 0.092*\"shift trade wind\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,041 : INFO : topic #4(12.467): -0.577*\"00\" + 0.343*\"water\" + -0.287*\"wind\" + -0.275*\"trade\" + -0.268*\"trade wind\" + -0.243*\"00 00\" + 0.197*\"temperatur\" + 0.184*\"water temperatur\" + 0.113*\"increas\" + -0.085*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,717 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,718 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,718 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,796 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,797 : INFO : 1st phase: constructing (1688, 150) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:42,900 : INFO : orthonormalizing (1688, 150) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,019 : INFO : 2nd phase: running dense svd on (150, 14293) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,493 : INFO : computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,494 : INFO : keeping 50 factors (discarding 39.080% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,496 : INFO : processed documents up to #14293\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,497 : INFO : topic #0(22.427): -0.321*\"differ\" + -0.313*\"vari\" + -0.293*\"rate\" + -0.289*\"vari differ\" + -0.281*\"bleach\" + -0.277*\"rate vari\" + -0.252*\"rate vari differ\" + -0.244*\"time\" + -0.237*\"differ time\" + -0.228*\"bleach rate\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,497 : INFO : topic #1(16.885): -0.464*\"coral\" + -0.337*\"coral bleach\" + -0.329*\"bleach\" + -0.214*\"temperatur\" + -0.202*\"water\" + 0.165*\"vari differ\" + 0.162*\"vari differ time\" + 0.158*\"differ time\" + 0.146*\"differ\" + -0.145*\"00\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,498 : INFO : topic #2(14.965): 0.411*\"temperatur\" + 0.373*\"water\" + 0.284*\"00\" + -0.271*\"coral bleach\" + 0.259*\"water temperatur\" + -0.252*\"bleach\" + 0.234*\"wind\" + -0.223*\"coral\" + 0.161*\"trade\" + 0.147*\"chang\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,499 : INFO : topic #3(13.427): 0.561*\"00\" + -0.447*\"wind\" + -0.355*\"trade\" + -0.338*\"trade wind\" + 0.224*\"00 00\" + 0.132*\"temperatur 00\" + -0.124*\"chang\" + -0.100*\"shift\" + -0.098*\"shift trade\" + -0.092*\"shift trade wind\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:43,499 : INFO : topic #4(12.467): 0.576*\"00\" + -0.343*\"water\" + 0.289*\"wind\" + 0.274*\"trade\" + 0.268*\"trade wind\" + 0.244*\"00 00\" + -0.197*\"temperatur\" + -0.185*\"water temperatur\" + -0.112*\"increas\" + 0.085*\"shift trade\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,258 : INFO : using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,258 : INFO : updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,259 : INFO : preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,331 : INFO : using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,331 : INFO : 1st phase: constructing (1688, 160) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,434 : INFO : orthonormalizing (1688, 160) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-08-17 22:04:44,564 : INFO : 2nd phase: running dense svd on (160, 14293) matrix\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-180-f76d62754eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdlsa_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mntopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mntopics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2wd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlsa_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdlsa_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mntopic\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, corpus, chunksize, decay)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                         \u001b[0;31m# serial version, there is only one \"worker\" (myself) => process the job directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                         \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, m, k, docs, use_svdlibc, power_iters, extra_dims)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_svdlibc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 u, s = stochastic_svd(docs, k, chunksize=sys.maxint, num_terms=m,\n\u001b[0;32m--> 123\u001b[0;31m                     power_iters=self.power_iters, extra_dims=self.extra_dims)\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36mstochastic_svd\u001b[0;34m(corpus, rank, num_terms, chunksize, extra_dims, power_iters, dtype, eps)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2nd phase: running dense svd on %s matrix\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nclusters = [20, 40, 60, 80, 100, 150, 200, 250, 300, 500]\n",
      "\n",
      "results = []\n",
      "print \"LSA\", \"Clusters\", \"Entropy\"\n",
      "for ncl in nclusters:\n",
      "    for ntopic in ntopics:\n",
      "        lsa_vectors = dlsa_vecs[ntopic]\n",
      "        kmclusterer = KMeans(ncl)\n",
      "        cls = kmclusterer.fit_predict(lsa_vectors)\n",
      "        clusters, cluster_tags = group_clusters(cls)\n",
      "        ent = compute_clusters_entropy(cluster_tags, regular_tags)\n",
      "        results.append((ntopic,ncl,ent))\n",
      "        print str(ntopic).rjust(3,'0'), str(ncl).rjust(3,'0'), ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LSA Clusters Entropy\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.549824146287\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.54191426927\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.515402562929\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.545853224737\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.516199680557\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.480398541622\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.517315651405\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.599115183887\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.533041048118\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.552901221716\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 020 0.531174091557\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.512485003568\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.488024605946\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.477255368955\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.470429782908\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.462322418527\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.488218437583\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.470743193882\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.4701222969\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.503149090378\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.484996910926\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 040 0.470111563092\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.479835721472\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.452472008407\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.434216244353\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.449685808535\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.450935439092\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.441916191893\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.453620162257\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.452412500698\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.459779107467\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.448214556022\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 060 0.459979474134\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.464223244875\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.432501762504\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.42813304667\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.422614895471\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.441640613463\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.433716473247\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.434606529698\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.437662773605\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.427105724703\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.429787411376\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 080 0.443326141622\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.450548552915\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.41313264122\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.406975369925\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.410697480233\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.421745548596\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.426217385958\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.425219319076\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.426022039009\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.423394460592\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.401790567948\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 0.414937085226\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.435247151358\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.388859081144\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.383300564802\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.383814930854\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.394102872624\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.393259092565\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.395246036245\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.397226475564\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.400296345572\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.384782974379\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 0.391708747436\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.411862569489\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.37303089104\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.365546518829\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.364571055571\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.371352877189\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.370498186087\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.376763068243\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.378611881994\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.37887005676\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.384037590646\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200 0.377371449248\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.394329403492\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.361140088227\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.350463203852\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.352771461785\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.352785544448\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.354161136943\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.367228011928\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.364672554506\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.371233689411\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.373716753117\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 250 0.36868485233\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.389170969377\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.342582282537\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.338613218588\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.337213768741\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.343179805867\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.343746921109\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.347384139255\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.348813704233\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.35370469925\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.358608809189\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300 0.352797271033\n",
        "005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.350939786042\n",
        "010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.315431308338\n",
        "020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.30971461745\n",
        "030"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.303761172385\n",
        "040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.305316584956\n",
        "050"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.306821185166\n",
        "060"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.304124865461\n",
        "080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.304995884524\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.312722466488\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.320043475724\n",
        "300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 0.322344360659\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Entropy (lower is better)\n",
      "\n",
      "Num Topics = 5\n",
      "0.386577300119: LSA + Kmeans (300 clusters)\n",
      "0.561695225002: LDA + Kmeans (300 clusters)\n",
      "\n",
      "Num Topics = 10\n",
      "0.333818956715: LSA + Kmeans (300 clusters)\n",
      "0.44859705162 : LDA + Kmeans (300 clusters)\n",
      "\n",
      "Num Topics - 20\n",
      "0.34081459328: LSA + Kmeans (300 clusters)\n",
      "0.465505884751 LDA + Kmeans (300 clusters)\n",
      "\n",
      "Num topics = 30\n",
      "0.336215634651: LSA + Kmeans (300 clusters)\n",
      "0.439404895592: LDA + Kmeans (300 clusters)\n",
      "\n",
      "Num Topics = 100\n",
      "0.536677318116: LSA + spectral (20 clusters)\n",
      "0.622021222133: LDA + spectral (20 clusters)\n",
      "0.552504471088: LSA + KMeans (20 clusters)\n",
      "0.613743516771: LDA + KMeans (20 clusters)\n",
      "0.420213076346: LSA + Kmeans (100 clusters)\n",
      "0.458344942441: LDA + Kmeans (100 clusters)\n",
      "0.355248883079: LSA + Kmeans (300 clusters)\n",
      "0.399847990494: LDA + Kmeans (300 clusters)\n",
      "\n",
      "Num Topics = 300\n",
      "0.350088704901: LSA + Kmeans (300 clusters)\n",
      "0.400934343385: LDA + Kmeans (300 clusters)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Kmeans on LSA:\n",
      "\n",
      "LSA Clusters Entropy\n",
      "005 020 0.549824146287\n",
      "010 020 0.54191426927\n",
      "020 020 0.515402562929\n",
      "030 020 0.545853224737\n",
      "040 020 0.516199680557\n",
      "050 020 0.480398541622\n",
      "060 020 0.517315651405\n",
      "080 020 0.599115183887\n",
      "100 020 0.533041048118\n",
      "200 020 0.552901221716\n",
      "300 020 0.531174091557\n",
      "005 040 0.512485003568\n",
      "010 040 0.488024605946\n",
      "020 040 0.477255368955\n",
      "030 040 0.470429782908\n",
      "040 040 0.462322418527\n",
      "050 040 0.488218437583\n",
      "060 040 0.470743193882\n",
      "080 040 0.4701222969\n",
      "100 040 0.503149090378\n",
      "200 040 0.484996910926\n",
      "300 040 0.470111563092\n",
      "005 060 0.479835721472\n",
      "010 060 0.452472008407\n",
      "020 060 0.434216244353\n",
      "030 060 0.449685808535\n",
      "040 060 0.450935439092\n",
      "050 060 0.441916191893\n",
      "060 060 0.453620162257\n",
      "080 060 0.452412500698\n",
      "100 060 0.459779107467\n",
      "200 060 0.448214556022\n",
      "300 060 0.459979474134\n",
      "005 080 0.464223244875\n",
      "010 080 0.432501762504\n",
      "020 080 0.42813304667\n",
      "030 080 0.422614895471\n",
      "040 080 0.441640613463\n",
      "050 080 0.433716473247\n",
      "060 080 0.434606529698\n",
      "080 080 0.437662773605\n",
      "100 080 0.427105724703\n",
      "200 080 0.429787411376\n",
      "300 080 0.443326141622\n",
      "005 100 0.450548552915\n",
      "010 100 0.41313264122\n",
      "020 100 0.406975369925\n",
      "030 100 0.410697480233\n",
      "040 100 0.421745548596\n",
      "050 100 0.426217385958\n",
      "060 100 0.425219319076\n",
      "080 100 0.426022039009\n",
      "100 100 0.423394460592\n",
      "200 100 0.401790567948\n",
      "300 100 0.414937085226\n",
      "005 150 0.435247151358\n",
      "010 150 0.388859081144\n",
      "020 150 0.383300564802\n",
      "030 150 0.383814930854\n",
      "040 150 0.394102872624\n",
      "050 150 0.393259092565\n",
      "060 150 0.395246036245\n",
      "080 150 0.397226475564\n",
      "100 150 0.400296345572\n",
      "200 150 0.384782974379\n",
      "300 150 0.391708747436\n",
      "005 200 0.411862569489\n",
      "010 200 0.37303089104\n",
      "020 200 0.365546518829\n",
      "030 200 0.364571055571\n",
      "040 200 0.371352877189\n",
      "050 200 0.370498186087\n",
      "060 200 0.376763068243\n",
      "080 200 0.378611881994\n",
      "100 200 0.37887005676\n",
      "200 200 0.384037590646\n",
      "300 200 0.377371449248\n",
      "005 250 0.394329403492\n",
      "010 250 0.361140088227\n",
      "020 250 0.350463203852\n",
      "030 250 0.352771461785\n",
      "040 250 0.352785544448\n",
      "050 250 0.354161136943\n",
      "060 250 0.367228011928\n",
      "080 250 0.364672554506\n",
      "100 250 0.371233689411\n",
      "200 250 0.373716753117\n",
      "300 250 0.36868485233\n",
      "005 300 0.389170969377\n",
      "010 300 0.342582282537\n",
      "020 300 0.338613218588\n",
      "030 300 0.337213768741\n",
      "040 300 0.343179805867\n",
      "050 300 0.343746921109\n",
      "060 300 0.347384139255\n",
      "080 300 0.348813704233\n",
      "100 300 0.35370469925\n",
      "200 300 0.358608809189\n",
      "300 300 0.352797271033\n",
      "005 500 0.350939786042\n",
      "010 500 0.315431308338\n",
      "020 500 0.30971461745\n",
      "030 500 0.303761172385\n",
      "040 500 0.305316584956\n",
      "050 500 0.306821185166\n",
      "060 500 0.304124865461\n",
      "080 500 0.304995884524\n",
      "100 500 0.312722466488\n",
      "200 500 0.320043475724\n",
      "300 500 0.322344360659"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import similarities\n",
      "\n",
      "def precompute_sim_matrix(model, corpus, num_features):\n",
      "    sim_mat = similarities.MatrixSimilarity(corpus=model[corpus], num_features=num_features)\n",
      "    return sim_mat[model[corpus]]\n",
      "\n",
      "sims = precompute_sim_matrix(lsa, corpus, NUM_LSA_TOPICS)\n",
      "\n",
      "# Eats up RAM until machine dies\n",
      "ap_clusterer = AffinityPropagation(copy=False, affinity='precomputed', verbose=True, convergence_iter=2)\n",
      "#cl_labels = ap_clusterer.fit_predict(sims)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO\n",
      "\n",
      "1. Use a parser instead to extract S - V - O groups, and Adj - N groupings\n",
      "2. Cluster to identify concepts:\n",
      "    - LSA\n",
      "    - Wordnet semantic similarity (http://nltk.googlecode.com/svn/trunk/doc/howto/wordnet.html)\n",
      "    - Training word2vec vectors\n",
      "    - Pre-trained neural word embeddings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}