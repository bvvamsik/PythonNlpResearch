{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Settings\n",
    "from model_store import ModelStore\n",
    "from window_based_tagger_config import get_config\n",
    "from processessays import process_essays, build_spelling_corrector\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "from BrattEssay import Essay, load_bratt_essays\n",
    "\n",
    "from featureextractortransformer import FeatureExtractorTransformer\n",
    "from sent_feats_for_stacking import *\n",
    "from load_data import load_process_essays_without_annotations\n",
    "\n",
    "from featureextractionfunctions import *\n",
    "from wordtagginghelper import *\n",
    "\n",
    "from traceback import format_exc\n",
    "\n",
    "import logging\n",
    "\n",
    "def onlyascii(s):\n",
    "    out = \"\"\n",
    "    for char in s:\n",
    "        if ord(char) > 127:\n",
    "            out += \"\"\n",
    "        else:\n",
    "            out += char\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def friendly_tag(tag):\n",
    "    return tag.replace(\"Causer:\", \"\").replace(\"Result:\", \"\")\n",
    "\n",
    "def cr_sort_key(cr):\n",
    "    cr = cr.replace(\"5b\", \"5.5\")\n",
    "    # _'s last\n",
    "    if cr[0] == \"_\":\n",
    "        return (99999999, cr, cr, cr)\n",
    "    # Casual second to last, ordered by the order of the cause then the effect\n",
    "    if \"->\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        a,b = cr.split(\"->\")\n",
    "        if a.isdigit():\n",
    "            a = float(a)\n",
    "        if b.isdigit():\n",
    "            b = float(b)\n",
    "        return (9000, a,b, cr)\n",
    "    # order causer's before results\n",
    "    elif \"Result:\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        return (-1, float(cr),-1,cr)\n",
    "    elif \"Causer:\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        return (-2, float(cr),-1,cr)\n",
    "    else:\n",
    "        #place regular tags first, numbers ahead of words\n",
    "        if cr[0].isdigit():\n",
    "            return (-10, float(cr),-1,cr)\n",
    "        else:\n",
    "            return (-10, 9999.9   ,-1,cr.lower())\n",
    "    return (float(cr.split(\"->\")[0]), cr) if cr.split(\"->\")[0][0].isdigit() else (99999, cr)\n",
    "\n",
    "class Annotator(object):\n",
    "\n",
    "    def __init__(self, models_folder, temp_folder, essays_folder):\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        if not models_folder.endswith(\"/\"):\n",
    "            models_folder += \"/\"\n",
    "        if not temp_folder.endswith(\"/\"):\n",
    "            temp_folder += \"/\"\n",
    "        if not essays_folder.endswith(\"/\"):\n",
    "            essays_folder += \"/\"\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.temp_folder = temp_folder\n",
    "        cfg = get_config(temp_folder)\n",
    "        self.config = cfg\n",
    "        self.essays_folder = essays_folder\n",
    "\n",
    "        # Create spell checker\n",
    "        # Need annotations here purely to load the tags\n",
    "        tagged_essays = load_bratt_essays(essays_folder, include_vague=cfg[\"include_vague\"], include_normal=cfg[\"include_normal\"], load_annotations=True)\n",
    "        self.__set_tags_(tagged_essays)\n",
    "        self.wd_sent_freq = defaultdict(int)\n",
    "        self.spelling_corrector = build_spelling_corrector(tagged_essays, self.config[\"lower_case\"], self.wd_sent_freq)\n",
    "\n",
    "        offset = (self.config[\"window_size\"] - 1) / 2\n",
    "\n",
    "        unigram_window_stemmed = fact_extract_positional_word_features_stemmed(offset)\n",
    "        biigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "\n",
    "        extractors = [unigram_window_stemmed, biigram_window_stemmed]\n",
    "\n",
    "        # most params below exist ONLY for the purposes of the hashing to and from disk\n",
    "        self.feature_extractor = FeatureExtractorTransformer(extractors)\n",
    "\n",
    "        # load models\n",
    "        self.logger.info(\"Loading pickled models\")\n",
    "        store = ModelStore(models_folder=models_folder)\n",
    "\n",
    "        self.feature_transformer =  store.get_transformer()\n",
    "        self.logger.info(\"Loaded Transformer\")\n",
    "        self.tag_2_wd_classifier = store.get_tag_2_wd_classifier()\n",
    "        self.logger.info(\"Loaded word tagging model\")\n",
    "        self.tag_2_sent_classifier = store.get_tag_2_sent_classifier()\n",
    "        self.logger.info(\"Loaded sentence classifier\")\n",
    "\n",
    "    def __set_tags_(self, tagged_essays):\n",
    "\n",
    "        MIN_TAG_FREQ = 5\n",
    "\n",
    "        tag_freq = defaultdict(int)\n",
    "        for essay in tagged_essays:\n",
    "            for sentence in essay.tagged_sentences:\n",
    "                un_tags = set()\n",
    "                for word, tags in sentence:\n",
    "                    for tag in tags:\n",
    "                        if \"5b\" in tag:\n",
    "                            continue\n",
    "                        if      (tag[-1].isdigit() or tag in {\"Causer\", \"explicit\", \"Result\"} \\\n",
    "                                    or tag.startswith(\"Causer\") or tag.startswith(\"Result\") \\\n",
    "                                    or tag.startswith(\"explicit\") or \"->\" in tag) \\\n",
    "                                and not (\"Anaphor\" in tag or \"rhetorical\" in tag or \"other\" in tag):\n",
    "                            # if not (\"Anaphor\" in tag or \"rhetorical\" in tag or \"other\" in tag):\n",
    "                            un_tags.add(tag)\n",
    "                for tag in un_tags:\n",
    "                    tag_freq[tag] += 1\n",
    "\n",
    "        all_tags = list(tag_freq.keys())\n",
    "        freq_tags = list(set((tag for tag, freq in tag_freq.items() if freq >= MIN_TAG_FREQ)))\n",
    "        non_causal = [t for t in freq_tags if \"->\" not in t]\n",
    "        only_causal = [t for t in freq_tags if \"->\" in t]\n",
    "\n",
    "        CAUSE_TAGS = [\"Causer\", \"Result\", \"explicit\"]\n",
    "        CAUSAL_REL_TAGS = [CAUSAL_REL, CAUSE_RESULT, RESULT_REL]  # + [\"explicit\"]\n",
    "\n",
    "        \"\"\" works best with all the pair-wise causal relation codes \"\"\"\n",
    "        # Include all tags for the output\n",
    "        self.wd_test_tags = list(set(all_tags + CAUSE_TAGS))\n",
    "\n",
    "        # tags from tagging model used to train the stacked model\n",
    "        self.sent_input_feat_tags = list(set(freq_tags + CAUSE_TAGS))\n",
    "        # find interactions between these predicted tags from the word tagger to feed to the sentence tagger\n",
    "        self.sent_input_interaction_tags = list(set(non_causal + CAUSE_TAGS))\n",
    "        # tags to train (as output) for the sentence based classifier\n",
    "        self.sent_output_train_test_tags = list(set(all_tags + CAUSE_TAGS + CAUSAL_REL_TAGS))\n",
    "\n",
    "    def annotate(self, essay_text):\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(essay_text.strip())\n",
    "            contents = \"\\n\".join(sentences)\n",
    "\n",
    "            fname = self.temp_folder + \"essay.txt\"\n",
    "            with open(fname, 'w\"') as f:\n",
    "                f.write(contents)\n",
    "\n",
    "            essay = Essay(fname, include_vague=self.config[\"include_vague\"],\n",
    "                          include_normal=self.config[\"include_normal\"], load_annotations=False)\n",
    "\n",
    "            processed_essays = process_essays(essays=[essay],\n",
    "                                              spelling_corrector=self.spelling_corrector,\n",
    "                                              wd_sent_freq=self.wd_sent_freq,\n",
    "                                              remove_infrequent=self.config[\"remove_infrequent\"],\n",
    "                                              spelling_correct=self.config[\"spelling_correct\"],\n",
    "                                              replace_nums=self.config[\"replace_nums\"],\n",
    "                                              stem=self.config[\"stem\"],\n",
    "                                              remove_stop_words=self.config[\"remove_stop_words\"],\n",
    "                                              remove_punctuation=self.config[\"remove_punctuation\"],\n",
    "                                              lower_case=self.config[\"lower_case\"])\n",
    "\n",
    "            self.logger.info(\"Essay loaded successfully\")\n",
    "            essays_TD = self.feature_extractor.transform(processed_essays)\n",
    "\n",
    "            wd_feats, _ = flatten_to_wordlevel_feat_tags(essays_TD)\n",
    "            xs = self.feature_transformer.transform(wd_feats)\n",
    "            wd_predictions_by_code = test_classifier_per_code(xs, self.tag_2_wd_classifier, self.wd_test_tags)\n",
    "\n",
    "            dummy_wd_td_ys_bytag = defaultdict(lambda: np.asarray([0.0] * xs.shape[0]))\n",
    "            sent_xs, sent_ys_bycode = get_sent_feature_for_stacking_from_tagging_model(self.sent_input_feat_tags,\n",
    "                                                                                             self.sent_input_interaction_tags,\n",
    "                                                                                             essays_TD, xs,\n",
    "                                                                                             dummy_wd_td_ys_bytag,\n",
    "                                                                                             self.tag_2_wd_classifier,\n",
    "                                                                                             sparse=True,\n",
    "                                                                                             look_back=0)\n",
    "\n",
    "            \"\"\" Test Stack Classifier \"\"\"\n",
    "            sent_predictions_by_code = test_classifier_per_code(sent_xs, self.tag_2_sent_classifier, self.sent_output_train_test_tags)\n",
    "\n",
    "            \"\"\" Generate Return Values \"\"\"\n",
    "            essay_tags = self.__get_essay_tags_(sent_predictions_by_code)\n",
    "\n",
    "            essay_type = None\n",
    "            if \"coral\" in self.essays_folder.lower():\n",
    "                essay_type = \"CB\"\n",
    "            elif \"skin\" in self.essays_folder.lower():\n",
    "                essay_type = \"SC\"\n",
    "            else:\n",
    "                raise Exception(\"Unknown essay type\")\n",
    "\n",
    "            raw_essay_tags = \",\".join(sorted(essay_tags, key=cr_sort_key))\n",
    "            return {\"tagged_words\":      self.__get_tagged_words_(essay, essays_TD[0], wd_predictions_by_code),\n",
    "                    \"tagged_sentences\" : self.__get_tagged_sentences_(essay, sent_predictions_by_code),\n",
    "                    \"essay_tags\":        self.__format_essay_tags_(essay_tags),\n",
    "                    \"essay_category\":    self.essay_category(raw_essay_tags, essay_type),\n",
    "                    \"raw_essay_tags\":    raw_essay_tags\n",
    "            }\n",
    "        except Exception as x:\n",
    "            self.logger.exception(\"An exception occured while annotating essay\")\n",
    "            return {\"error\": format_exc()}\n",
    "        pass\n",
    "\n",
    "    def essay_category(self, s, essay_type):\n",
    "\n",
    "        essay_type = essay_type.strip().upper()\n",
    "\n",
    "        if not s or s == \"\" or s == \"nan\":\n",
    "            return 1\n",
    "        splt = s.strip().split(\",\")\n",
    "        splt = filter(lambda s: len(s.strip()) > 0, splt)\n",
    "        regular = [t.strip() for t in splt if t[0].isdigit()]\n",
    "        any_causal = [t.strip() for t in splt if \"->\" in t and ((\"Causer\" in t and \"Result\" in t) or \"C->R\" in t)]\n",
    "        causal = [t.strip() for t in splt if \"->\" in t and \"Causer\" in t and \"Result\" in t]\n",
    "        if len(regular) == 0 and len(any_causal) == 0:\n",
    "            return 1\n",
    "        if len(any_causal) == 0:  # i.e. by this point regular must have some\n",
    "            return 2  # no causal\n",
    "        # if only one causal then must be 3\n",
    "        elif len(any_causal) == 1 or len(causal) == 1:\n",
    "            return 3\n",
    "        # Map to Num->Num, e.g. Causer:3->Results:50 becomes 3->5\n",
    "        # Also map 6 to 16 and 7 to 17 to enforce the relative size relationship\n",
    "\n",
    "        def map_cb(code):\n",
    "            return code.replace(\"6\", \"16\").replace(\"7\", \"17\")\n",
    "\n",
    "        def map_sc(code):\n",
    "            return code.replace(\"4\", \"14\").replace(\"5\", \"15\").replace(\"6\", \"16\").replace(\"150\", \"50\")\n",
    "\n",
    "        is_cb = False\n",
    "        is_sc = False\n",
    "        if essay_type == \"CB\":\n",
    "            is_cb = True\n",
    "            crels = sorted(map(lambda t: map_cb(t.replace(\"Causer:\", \"\").replace(\"Result:\", \"\")).strip(), causal),\n",
    "                           key=cr_sort_key)\n",
    "        elif essay_type == \"SC\":\n",
    "            is_sc = True\n",
    "            crels = sorted(map(lambda t: map_sc(t.replace(\"Causer:\", \"\").replace(\"Result:\", \"\")).strip(), \\\n",
    "                               causal),\n",
    "                           key=cr_sort_key)\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized filename\")\n",
    "\n",
    "        un_results = set()\n",
    "        # For each unique pairwise combination\n",
    "        for a in crels:\n",
    "            for b in crels:\n",
    "                if cr_sort_key(b) >= cr_sort_key(a):  # don't compare each pair twice (a,b) == (b,a)\n",
    "                    break\n",
    "                # b is always the smaller of the two\n",
    "                bc, br = b.split(\"->\")\n",
    "                ac, ar = a.split(\"->\")\n",
    "                # if result from a is causer for b\n",
    "                if br.strip() == ac.strip():\n",
    "                    un_results.add((b, a))\n",
    "\n",
    "        if len(un_results) >= 1:\n",
    "            #CB and 6->7->50 ONLY\n",
    "            if len(un_results) == 1 and is_cb and (\"16->17\", \"17->50\") in un_results:\n",
    "                return 4\n",
    "            if len(un_results) <= 2 and is_sc:\n",
    "                #4->5->6->50\n",
    "                codes = set(\"14,15,16,50\".split(\",\"))\n",
    "                un_results_cp = set(un_results)\n",
    "                for a, b in un_results:\n",
    "                    alhs, arhs = a.split(\"->\")\n",
    "                    blhs, brhs = b.split(\"->\")\n",
    "                    if alhs in codes and arhs in codes and blhs in codes and brhs in codes:\n",
    "                        un_results_cp.remove((a, b))\n",
    "                if len(un_results_cp) == 0:\n",
    "                    return 4\n",
    "            return 5\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def __is_tag_to_return_(self, tag):\n",
    "        return tag[0].isdigit() or (\"->\" in tag and \"Causer\" in tag)\n",
    "\n",
    "    def __get_regular_tags_(self, pred_tags):\n",
    "        r_tags = sorted(filter(lambda t: t[0].isdigit() and \"->\" not in t, pred_tags),\n",
    "                        key=lambda s: (int(s), s) if s.isdigit() else ((-1, s)))\n",
    "        str_r_tags = \",\".join(r_tags)\n",
    "        return str_r_tags\n",
    "\n",
    "    def __get_causal_tags_(self, pred_tags):\n",
    "        c_tags = sorted(filter(lambda t: \"->\" in t, pred_tags), key=cr_sort_key)\n",
    "        str_c_tags = \",\".join(c_tags)\n",
    "        return str_c_tags\n",
    "\n",
    "    def __get_tagged_sentences_(self, essay, sent_predictions_by_code):\n",
    "        tagged_sents = []\n",
    "        for i, sent in enumerate(essay.tagged_sentences):\n",
    "            wds, _ = zip(*sent)\n",
    "            str_sent = \" \".join(wds)\n",
    "            pred_tags = set()\n",
    "            for tag, array in sent_predictions_by_code.items():\n",
    "                if self.__is_tag_to_return_(tag):\n",
    "                    if np.max(array[i]) == 1:\n",
    "                        pred_tags.add(friendly_tag(tag))\n",
    "\n",
    "            str_r_tags = self.__get_regular_tags_(pred_tags)\n",
    "            str_c_tags = self.__get_causal_tags_(pred_tags)\n",
    "\n",
    "            tagged_sents.append((str_sent, str_r_tags, str_c_tags ))\n",
    "        return tagged_sents\n",
    "\n",
    "    def __get_essay_tags_(self, sent_predictions_by_code):\n",
    "        tags = set()\n",
    "\n",
    "        for tag, array in sent_predictions_by_code.items():\n",
    "            if np.max(array) == 1:\n",
    "                tags.add(tag)\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def __format_essay_tags_(self, tags):\n",
    "\n",
    "        tags = map(lambda s: friendly_tag(s), filter(lambda t: self.__is_tag_to_return_(t), tags))\n",
    "\n",
    "        str_r_tags = self.__get_regular_tags_(tags)\n",
    "        str_c_tags = self.__get_causal_tags_(tags)\n",
    "\n",
    "        if not str_r_tags:\n",
    "            return str_c_tags\n",
    "        elif not str_c_tags:\n",
    "            return str_r_tags\n",
    "        else:\n",
    "            return str_r_tags + \",\" + str_c_tags\n",
    "\n",
    "    def __fuzzy_match_(self, original, feat_wd):\n",
    "        original = original.lower().strip()\n",
    "        feat_wd = feat_wd.lower().strip()\n",
    "        if original == feat_wd:\n",
    "            return True\n",
    "        if original[:3] == feat_wd[:3]:\n",
    "            return True\n",
    "        a = set(original)\n",
    "        b = set(feat_wd)\n",
    "        jaccard = float(len(a.intersection(b))) / float(len(a.union(b)))\n",
    "        return jaccard >= 0.5\n",
    "\n",
    "    def __align_wd_tags_(self, orig, feats):\n",
    "        \"\"\"\n",
    "        Once processed, there may be a different number of words than in the original sentence\n",
    "        Try and recover the tags for the original words by aligning the two using simple heuristics\n",
    "        \"\"\"\n",
    "        if len(orig) < len(feats):\n",
    "            raise Exception(\"align_wd_tags() : Original sentence is longer!\")\n",
    "\n",
    "        o_wds, _ = zip(*orig)\n",
    "        feat_wds, new_tags = zip(*feats)\n",
    "\n",
    "        if len(orig) == len(feats):\n",
    "            return zip(o_wds, new_tags)\n",
    "\n",
    "        #here orig is longer than feats\n",
    "        diff = len(orig) - len(feats)\n",
    "        tagged_wds = []\n",
    "        feat_offset = 0\n",
    "        while len(tagged_wds) < len(o_wds):\n",
    "            i = len(tagged_wds)\n",
    "            orig_wd = o_wds[i]\n",
    "            print i, orig_wd\n",
    "\n",
    "            if i >= len(feats):\n",
    "                tagged_wds.append((orig_wd, new_tags[-1]))\n",
    "                continue\n",
    "            else:\n",
    "                new_tag_ix = i - feat_offset\n",
    "                feat_wd = feats[new_tag_ix][0]\n",
    "                if feat_wd == \"INFREQUENT\" or feat_wd.isdigit():\n",
    "                    tagged_wds.append((orig_wd, new_tags[new_tag_ix]))\n",
    "                    continue\n",
    "\n",
    "                new_tagged_wds = []\n",
    "                found = False\n",
    "                for j in range(i, i + diff + 1):\n",
    "                    new_tagged_wds.append((o_wds[j], new_tags[new_tag_ix]))\n",
    "                    next_orig_wd = o_wds[j]\n",
    "                    if self.__fuzzy_match_(next_orig_wd, feat_wd):\n",
    "                        found = True\n",
    "                        tagged_wds.extend(new_tagged_wds)\n",
    "                        feat_offset += len(new_tagged_wds) - 1\n",
    "                        break\n",
    "                if not found:\n",
    "                    raise Exception(\"No matching word found for index:%i and processed word:%s\" % (i, feat_wd))\n",
    "        return tagged_wds\n",
    "\n",
    "    def __get_tagged_words_(self, original_essay, essay_TD, wd_predictions_by_code):\n",
    "        tagged_sents = []\n",
    "        # should be a one to one correspondance between words in essays_TD[0] and predictions\n",
    "        i = 0\n",
    "        for sent_ix, sent in enumerate(essay_TD.sentences):\n",
    "            tmp_tagged_wds = []\n",
    "            for wix, (feat) in enumerate(sent):\n",
    "                word = feat.word\n",
    "                tags = set()\n",
    "                for tag in wd_predictions_by_code.keys():\n",
    "                    if wd_predictions_by_code[tag][i] > 0:\n",
    "                        tags.add(tag)\n",
    "                i += 1\n",
    "                tmp_tagged_wds.append((word, tags))\n",
    "\n",
    "            # Now allign the predicted tags with the original words\n",
    "            wds, aligned_tags = zip(*self.__align_wd_tags_(original_essay.tagged_sentences[sent_ix], tmp_tagged_wds))\n",
    "            fr_aligned_tags = map(lambda tags: set(map(friendly_tag, tags)), aligned_tags)\n",
    "            tagged_words = zip(wds, fr_aligned_tags)\n",
    "            tagged_sents.append(map(lambda (wd, tags): (wd, self.__get_regular_tags_(tags), self.__get_causal_tags_(tags)), tagged_words))\n",
    "        return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "1154 files found\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05571.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05904.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_BGJD_1_CB_ES-05733.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_ERSK_7_CB_ES-05798.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_KYLS_5_CB_ES-05671.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_LRJE_5_CB_ES-05128.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05612.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05617.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05632.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05640.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SWSP_4_CB_ES-05459.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05484.ann file as .txt file is no essay. //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05485.ann file as .txt file is no essay.  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_2_CB_ES-05548.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFMV_3_CB_ES-05845.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05715.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05721.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06128.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06132.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05025.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05030.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_2_CB_ES-06140.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06149.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06153.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTKP_7-8_CB_ES-06174.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415post_TWNB_2_CB_ES-04948.ann file as .txt file is no essay.'\n",
      "1128 essays processed\n",
      "Loading models from /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/API/Models/CB/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/API\"\n",
    "\n",
    "settings = Settings.Settings()\n",
    "folder = settings.data_directory + \"CoralBleaching/BrattData/EBA1415_Merged/\"\n",
    "\n",
    "annotator = Annotator(models_folder= cwd +\"/Models/CB/\", temp_folder=cwd+\"/temp/\", essays_folder=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_text = \"\"\"\n",
    "Corals are living animals in the ocean.\n",
    "Corals live in one place and dont really move alot.\n",
    "Some corals have white on them and that is called \"coral bleaching.\"\n",
    "Coral Bleaching means that the coral is unhealthy and is trusting into a white color.\n",
    "Normal water tempatures that the coral live in are 70-80 degrees.\n",
    "But some of the waters are too cool like 3 to 10 degrees F.\n",
    "Corals are also affected by storms because corals rely on the amounts of salt in the waters.\n",
    "So when it storms the water tempatures and levels of salt will be all mest up and bad for the coral.\n",
    "The storms have to be very extreme to make corals sick or unhealthy.\n",
    "In the water if the tempature increases the amounts of dioxide will drop and willmake the coral unhealthy.\n",
    "The water tempatures coral usally build their reefs in are 70-85 degrees F.\n",
    "So those are the tempature range to keep them healthy.\n",
    "Corals and zooanthellae algae have a relatioship together.\n",
    "Most zooanthellae can not live without outside the corals bodies.\n",
    "It is because there isnt enough nutrience to have the ocean do photosynthesis.\n",
    "The zooanthellae rely on the coral to stay healthy, but the coral can get physical damage.\n",
    "Coral bleaching is a physical damage to the corals.\n",
    "Coral bleaching is also an example how the envionmental stressors can affect the relationships between the coral and the algae. //\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_annotations = d_annotations = annotator.annotate(essay_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Corals are living animals in the ocean .\"  | \n",
      "\"Corals live in one place and dont really move alot .\"  | \n",
      "\"Some corals have white on them and that is called \" coral bleaching . \"\" 50 | \n",
      "\"Coral Bleaching means that the coral is unhealthy and is trusting into a white color .\" 50 | \n",
      "\"Normal water tempatures that the coral live in are 70 - 80 degrees .\"  | \n",
      "\"But some of the waters are too cool like 3 to 10 degrees F .\" 3 | \n",
      "\"Corals are also affected by storms because corals rely on the amounts of salt in the waters .\" 11,13 | \n",
      "\"So when it storms the water tempatures and levels of salt will be all mest up and bad for the coral .\" 3,11,13,50 | 11->3,11->13,11->50\n",
      "\"The storms have to be very extreme to make corals sick or unhealthy .\" 11 | 11->14\n",
      "\"In the water if the tempature increases the amounts of dioxide will drop and willmake the coral unhealthy .\" 3,4 | 3->4\n",
      "\"The water tempatures coral usally build their reefs in are 70 - 85 degrees F .\"  | \n",
      "\"So those are the tempature range to keep them healthy .\"  | \n",
      "\"Corals and zooanthellae algae have a relatioship together .\"  | \n",
      "\"Most zooanthellae can not live without outside the corals bodies .\"  | \n",
      "\"It is because there isnt enough nutrience to have the ocean do photosynthesis .\"  | \n",
      "\"The zooanthellae rely on the coral to stay healthy , but the coral can get physical damage .\"  | \n",
      "\"Coral bleaching is a physical damage to the corals .\" 50 | \n",
      "\"Coral bleaching is also an example how the envionmental stressors can affect the relationships between the coral and the algae .\" 50 | \n"
     ]
    }
   ],
   "source": [
    "for sent, r_tags, c_tags in d_annotations[\"tagged_sentences\"]:\n",
    "    print \"\\\"\" + sent+ \"\\\"\", r_tags, \"|\", c_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Corals', '', '')\n",
      "('are', '', '')\n",
      "('living', '', '')\n",
      "('animals', '', '')\n",
      "('in', '', '')\n",
      "('the', '', '')\n",
      "('ocean', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Corals', '', '')\n",
      "('live', '', '')\n",
      "('in', '', '')\n",
      "('one', '', '')\n",
      "('place', '', '')\n",
      "('and', '', '')\n",
      "('dont', '', '')\n",
      "('really', '', '')\n",
      "('move', '', '')\n",
      "('alot', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Some', '50', '')\n",
      "('corals', '50', '')\n",
      "('have', '50', '')\n",
      "('white', '50', '')\n",
      "('on', '', '')\n",
      "('them', '', '')\n",
      "('and', '', '')\n",
      "('that', '', '')\n",
      "('is', '50', '')\n",
      "('called', '', '')\n",
      "('\"', '', '')\n",
      "('coral', '50', '')\n",
      "('bleaching', '50', '')\n",
      "('.', '', '')\n",
      "('\"', '', '')\n",
      "\n",
      "('Coral', '50', '')\n",
      "('Bleaching', '50', '')\n",
      "('means', '', '')\n",
      "('that', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('is', '', '')\n",
      "('unhealthy', '', '')\n",
      "('and', '', '')\n",
      "('is', '', '')\n",
      "('trusting', '50', '')\n",
      "('into', '50', '')\n",
      "('a', '50', '')\n",
      "('white', '50', '')\n",
      "('color', '50', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Normal', '', '')\n",
      "('water', '', '')\n",
      "('tempatures', '', '')\n",
      "('that', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('live', '', '')\n",
      "('in', '', '')\n",
      "('are', '', '')\n",
      "('70', '', '')\n",
      "('-', '', '')\n",
      "('80', '', '')\n",
      "('degrees', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('But', '', '')\n",
      "('some', '', '')\n",
      "('of', '', '')\n",
      "('the', '', '')\n",
      "('waters', '3', '')\n",
      "('are', '3', '')\n",
      "('too', '3', '')\n",
      "('cool', '', '')\n",
      "('like', '', '')\n",
      "('3', '3', '')\n",
      "('to', '3', '')\n",
      "('10', '3', '')\n",
      "('degrees', '3', '')\n",
      "('F', '3', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Corals', '', '')\n",
      "('are', '', '')\n",
      "('also', '', '')\n",
      "('affected', '', '')\n",
      "('by', '', '')\n",
      "('storms', '11', '')\n",
      "('because', '', '')\n",
      "('corals', '', '')\n",
      "('rely', '', '')\n",
      "('on', '', '')\n",
      "('the', '', '')\n",
      "('amounts', '', '')\n",
      "('of', '13', '')\n",
      "('salt', '13', '')\n",
      "('in', '13', '')\n",
      "('the', '13', '')\n",
      "('waters', '13', '')\n",
      "('.', '', '')\n",
      "\n",
      "('So', '', '')\n",
      "('when', '', '')\n",
      "('it', '', '')\n",
      "('storms', '11', '')\n",
      "('the', '', '11->13')\n",
      "('water', '3', '11->13')\n",
      "('tempatures', '3', '11->13')\n",
      "('and', '', '')\n",
      "('levels', '13', '')\n",
      "('of', '13', '11->13')\n",
      "('salt', '13', '11->13')\n",
      "('will', '13', '11->13')\n",
      "('be', '', '')\n",
      "('all', '', '')\n",
      "('mest', '', '')\n",
      "('up', '', '')\n",
      "('and', '', '')\n",
      "('bad', '', '')\n",
      "('for', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('The', '', '')\n",
      "('storms', '11', '')\n",
      "('have', '', '')\n",
      "('to', '', '')\n",
      "('be', '', '')\n",
      "('very', '', '')\n",
      "('extreme', '', '')\n",
      "('to', '', '')\n",
      "('make', '', '')\n",
      "('corals', '', '')\n",
      "('sick', '', '')\n",
      "('or', '', '')\n",
      "('unhealthy', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('In', '', '')\n",
      "('the', '', '')\n",
      "('water', '', '')\n",
      "('if', '3', '')\n",
      "('the', '3', '3->4')\n",
      "('tempature', '3', '3->4')\n",
      "('increases', '3', '3->4')\n",
      "('the', '', '3->4')\n",
      "('amounts', '4', '3->4')\n",
      "('of', '4', '')\n",
      "('dioxide', '', '')\n",
      "('will', '', '')\n",
      "('drop', '', '')\n",
      "('and', '', '')\n",
      "('willmake', '', '')\n",
      "('the', '', '')\n",
      "('coral', '14', '')\n",
      "('unhealthy', '14', '')\n",
      "('.', '', '')\n",
      "\n",
      "('The', '', '')\n",
      "('water', '3', '')\n",
      "('tempatures', '', '')\n",
      "('coral', '', '')\n",
      "('usally', '', '')\n",
      "('build', '', '')\n",
      "('their', '', '')\n",
      "('reefs', '', '')\n",
      "('in', '', '')\n",
      "('are', '', '')\n",
      "('70', '', '')\n",
      "('-', '', '')\n",
      "('85', '', '')\n",
      "('degrees', '', '')\n",
      "('F', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('So', '', '')\n",
      "('those', '', '')\n",
      "('are', '', '')\n",
      "('the', '', '')\n",
      "('tempature', '', '')\n",
      "('range', '', '')\n",
      "('to', '', '')\n",
      "('keep', '', '')\n",
      "('them', '', '')\n",
      "('healthy', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Corals', '', '')\n",
      "('and', '', '')\n",
      "('zooanthellae', '', '')\n",
      "('algae', '', '')\n",
      "('have', '', '')\n",
      "('a', '', '')\n",
      "('relatioship', '', '')\n",
      "('together', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Most', '', '')\n",
      "('zooanthellae', '', '')\n",
      "('can', '', '')\n",
      "('not', '', '')\n",
      "('live', '', '')\n",
      "('without', '', '')\n",
      "('outside', '', '')\n",
      "('the', '', '')\n",
      "('corals', '', '')\n",
      "('bodies', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('It', '', '')\n",
      "('is', '', '')\n",
      "('because', '', '')\n",
      "('there', '', '')\n",
      "('isnt', '', '')\n",
      "('enough', '', '')\n",
      "('nutrience', '', '')\n",
      "('to', '', '')\n",
      "('have', '', '')\n",
      "('the', '', '')\n",
      "('ocean', '', '')\n",
      "('do', '', '')\n",
      "('photosynthesis', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('The', '', '')\n",
      "('zooanthellae', '', '')\n",
      "('rely', '', '')\n",
      "('on', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('to', '', '')\n",
      "('stay', '', '')\n",
      "('healthy', '', '')\n",
      "(',', '', '')\n",
      "('but', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('can', '', '')\n",
      "('get', '', '')\n",
      "('physical', '', '')\n",
      "('damage', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Coral', '50', '')\n",
      "('bleaching', '50', '')\n",
      "('is', '', '')\n",
      "('a', '', '')\n",
      "('physical', '', '')\n",
      "('damage', '', '')\n",
      "('to', '', '')\n",
      "('the', '', '')\n",
      "('corals', '', '')\n",
      "('.', '', '')\n",
      "\n",
      "('Coral', '50', '')\n",
      "('bleaching', '50', '')\n",
      "('is', '', '')\n",
      "('also', '', '')\n",
      "('an', '', '')\n",
      "('example', '', '')\n",
      "('how', '', '14->50')\n",
      "('the', '', '14->50')\n",
      "('envionmental', '6', '14->50')\n",
      "('stressors', '', '')\n",
      "('can', '', '')\n",
      "('affect', '', '')\n",
      "('the', '', '')\n",
      "('relationships', '', '')\n",
      "('between', '', '')\n",
      "('the', '', '')\n",
      "('coral', '', '')\n",
      "('and', '', '')\n",
      "('the', '', '')\n",
      "('algae', '', '')\n",
      "('.', '', '')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in d_annotations[\"tagged_words\"]:\n",
    "    for wd, r_tags, c_tags in sent:\n",
    "        print str((wd, r_tags, c_tags))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,4,11,13,50,3->4,11->3,11->13,11->14,11->50'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_annotations[\"essay_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '4',\n",
       " '11',\n",
       " '13',\n",
       " '50',\n",
       " 'Causer',\n",
       " 'explicit',\n",
       " 'Result',\n",
       " 'Causer:3',\n",
       " 'Causer:11',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:13',\n",
       " 'Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:50',\n",
       " '_C->R',\n",
       " '_CRel',\n",
       " '_RRel']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_annotations[\"raw_essay_tags\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '4',\n",
       " '11',\n",
       " '13',\n",
       " '50',\n",
       " 'Causer',\n",
       " 'explicit',\n",
       " 'Result',\n",
       " 'Causer:3',\n",
       " 'Causer:11',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:13',\n",
       " 'Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:50',\n",
       " '_C->R',\n",
       " '_CRel',\n",
       " '_RRel']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cr_sort_key(cr):\n",
    "    cr = cr.replace(\"5b\", \"5.5\")\n",
    "    # _'s last\n",
    "    if cr[0] == \"_\":\n",
    "        return (99999999, cr, cr, cr)\n",
    "    # Casual second to last, ordered by the order of the cause then the effect\n",
    "    if \"->\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        a,b = cr.split(\"->\")\n",
    "        if a.isdigit():\n",
    "            a = float(a)\n",
    "        if b.isdigit():\n",
    "            b = float(b)\n",
    "        return (9000, a,b, cr)\n",
    "    # order causer's before results\n",
    "    elif \"Result:\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        return (-1, float(cr),-1,cr)\n",
    "    elif \"Causer:\" in cr:\n",
    "        cr = friendly_tag(cr)\n",
    "        return (-2, float(cr),-1,cr)\n",
    "    else:\n",
    "        #place regular tags first, numbers ahead of words\n",
    "        if cr[0].isdigit():\n",
    "            return (-10, float(cr),-1,cr)\n",
    "        else:\n",
    "            return (-10, 9999.9   ,-1,cr.lower())\n",
    "    return (float(cr.split(\"->\")[0]), cr) if cr.split(\"->\")[0][0].isdigit() else (99999, cr)\n",
    "\n",
    "#codes = d_annotations[\"raw_essay_tags\"]\n",
    "codes = \"3,4,11,13,50,Causer,Causer:11,Causer:3,Result,Result:13,Result:3,Result:4,Result:50,_CRel,_RRel,explicit,Causer:3->Result:4,Causer:11->Result:3,Causer:11->Result:13,Causer:11->Result:14,Causer:11->Result:50,_C->R\"\n",
    "sorted(codes.split(\",\"), key=cr_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '4',\n",
       " '11',\n",
       " '13',\n",
       " '50',\n",
       " 'Causer',\n",
       " 'explicit',\n",
       " 'Result',\n",
       " 'Causer:3',\n",
       " 'Causer:11',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:13',\n",
       " 'Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:50',\n",
       " '_C->R',\n",
       " '_CRel',\n",
       " '_RRel']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes = '11,13,3,4,50,Causer,Causer:11,Causer:11->Result:13,Causer:11->Result:14,Causer:11->Result:3,Causer:11->Result:50,Causer:3,Causer:3->Result:4,Result,Result:13,Result:3,Result:4,Result:50,_C->R,_CRel,_RRel,explicit'\n",
    "sorted(codes.split(\",\"), key=cr_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
