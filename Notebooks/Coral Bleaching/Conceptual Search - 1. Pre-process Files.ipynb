{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"/Users/simon.hughes/Google Drive/PhD/Data/CoralBleaching/PhraseExtractionAnalysis\"\n",
    "\n",
    "DOCS_FOLDER   = \"%s/Docs\" % ROOT_FOLDER\n",
    "OUTPUT_FOLDER = \"%s/ProcessedDocs\" % ROOT_FOLDER\n",
    "EMPTY_OUTPUT_FOLDER = True\n",
    "\n",
    "FILE_MASK = \".*\\.txt\"\n",
    "PARSE_HTML = False\n",
    "FILE_SIZE_LIMIT_CHARS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Predictions Output File for Processing Corrected Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "def find_files(folder, regex, remove_empty = False):\n",
    "    \"\"\"\n",
    "    Find all files matching the [regex] pattern in [folder]\n",
    "\n",
    "    folder  :   string\n",
    "                    folder to search (not recursive)\n",
    "    regex   :   string (NOT regex object)\n",
    "                    pattern to match\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder)\n",
    "    matches = [os.path.abspath(os.path.join(folder, f))\n",
    "               for f in files\n",
    "               if re.search(regex, f, re.IGNORECASE)]\n",
    "\n",
    "    if remove_empty:\n",
    "        matches = [f for f in matches if os.path.getsize(f) > 0]\n",
    "    matches.sort()\n",
    "    return matches\n",
    "\n",
    "def delete_files(folder, regex):\n",
    "    \"\"\" Deletes files in [folder] that match [regex] \n",
    "        e.g. delete_files(\"C:/Dice Data/DelTest\", \".*\\.txt\", 30)\n",
    "\n",
    "        folder      :   string\n",
    "                            folder to search\n",
    "        regex       :   string\n",
    "                            file pattern to match\n",
    "    \"\"\"\n",
    "    matches = find_files(folder, regex)\n",
    "    for full_path in matches:\n",
    "        os.remove(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "fname = \"/Users/simon.hughes/Google Drive/PhD/Data/CoralBleaching/Results/predictions_causal_and_codes.txt\"\n",
    "data = pd.read_csv(fname, sep=\"|\")\n",
    "data = data[[\"Essay\", \"Sent Number\", \"Processed Sentence\"]]\n",
    "data[\"s_Sent_Num\"] = data[\"Sent Number\"].apply(lambda n : str(n).rjust(3,'0'))\n",
    "data[\"Num_Sent\"] = data[\"s_Sent_Num\"] + \"|\" + data[\"Processed Sentence\"]\n",
    "data = data[[\"Essay\", \"Num_Sent\"]]\n",
    "data.head()\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "grouped = group_by(data, [\"Essay\"], [(\"Num_Sent\", lambda strings: \"||\".join(sorted(strings)))])\n",
    "grouped.head()\n",
    "\n",
    "#completely empty docs folder\n",
    "delete_files(DOCS_FOLDER,\".*\")\n",
    "\n",
    "for i in range(len(grouped)):\n",
    "    row = grouped.iloc[i]\n",
    "    essay_name = row[\"Essay\"][:-4] + \".txt\"\n",
    "    with open(DOCS_FOLDER + \"/\" + essay_name, \"w+\") as f:\n",
    "        contents = row[\"Num_Sent\"].split(\"||\")\n",
    "        for item in contents:\n",
    "            num, sent = item.split(\"|\")\n",
    "            f.write(sent.strip().lower() + \"\\n\")\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Processed Output and Dump to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shared\n",
    "import re\n",
    "re_collapse_spaces = re.compile(\"\\s+\")\n",
    "\n",
    "def collapse_spaces(s):\n",
    "    return re_collapse_spaces.sub(\" \", s).strip()\n",
    "\n",
    "re1 = re.compile(\"[;:\\'\\\"\\*/\\),\\(\\|\\s]+\")\n",
    "def clean_str(s):\n",
    "    s = str(s).replace(\"'s\",\" \")\n",
    "    #doesn't work in regex\n",
    "    s = s.replace(\"-\", \" \").replace(\"\\\\\",\" \")\n",
    "    s = re1.sub(\" \",s).strip()\n",
    "    return collapse_spaces(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "REPL = \".\\n\"\n",
    "\n",
    "def strip_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "\n",
    "# Make common html tags line breaks\n",
    "def pre_process_text(txt):\n",
    "    txt = txt.replace(\"</li><li>\", REPL).replace(\"<li>\", REPL).replace(\"</li>\", REPL)\n",
    "    txt = txt.replace(\"<br>\", REPL)\n",
    "    txt = txt.replace(\"<br/>\", REPL)\n",
    "    txt = txt.replace(\"<br />\", REPL)\n",
    "    txt = txt.replace(\"<p>\",  REPL)\n",
    "    txt = txt.replace(\"<p/>\",  REPL)\n",
    "    txt = txt.replace(\"<p />\",  REPL)\n",
    "    txt = txt.replace(\"</p>\", REPL)\n",
    "    txt = txt.replace(\". .\",  REPL)\n",
    "    txt = txt.replace(\"&nbsp;\", \" \")\n",
    "    while \"..\" in txt:\n",
    "        txt = txt.replace(\"..\", \". \")\n",
    "    while \"  \" in txt:    \n",
    "        txt = txt.replace(\"  \", \" \")\n",
    "    return txt\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', strip_non_ascii(element)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_text(html):\n",
    "    bs = BeautifulSoup(html)\n",
    "    texts = bs.findAll(text=True)\n",
    "    visible_texts = filter(visible, texts)\n",
    "    return REPL.join(visible_texts)\n",
    "\n",
    "def parse_html(html):\n",
    "    txt = get_text(pre_process_text(html))\n",
    "    return txt\n",
    "\n",
    "def split_into_sentences(txt):\n",
    "    txt = strip_non_ascii(txt)\n",
    "    #sents = map(clean_str,sent_tokenize(txt))\n",
    "    #This has already been done\n",
    "    sents = map(clean_str,txt.split(\"\\n\"))\n",
    "    return filter(lambda s: len(s.strip()) > 5, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "Loading and processing documents took 0.599376916885 seconds\n"
     ]
    }
   ],
   "source": [
    "import ntpath\n",
    "\n",
    "ntpath.basename(\"a/b/c\")\n",
    "def get_file_name(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if EMPTY_OUTPUT_FOLDER:\n",
    "    if OUTPUT_FOLDER == DOCS_FOLDER:\n",
    "        print(\"ERROR - Can't empty output folder if the same as the input folder\")\n",
    "    else:\n",
    "        delete_files(OUTPUT_FOLDER,\".*\")\n",
    "    \n",
    "files = find_files(DOCS_FOLDER, FILE_MASK, True)\n",
    "for i, fpath in enumerate(files):\n",
    "    with open(fpath) as f:\n",
    "        contents = f.read()\n",
    "        if len(contents) < FILE_SIZE_LIMIT_CHARS:\n",
    "            continue\n",
    "        if PARSE_HTML:\n",
    "            contents = parse_html(contents)\n",
    "            if len(contents) < FILE_SIZE_LIMIT_CHARS:\n",
    "                continue\n",
    "\n",
    "        sents = split_into_sentences(contents)\n",
    "        doc = \"\\n\".join(sents)\n",
    "        \n",
    "        file_name = get_file_name(fpath)        \n",
    "        fout_name = OUTPUT_FOLDER + \"/\" + file_name.split(\".\")[0] + \"_proc.txt\"\n",
    "        with open(fout_name, \"w+\") as fout:\n",
    "            fout.write(doc)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "end = time.time()\n",
    "print(\"Loading and processing documents took %s seconds\" % str(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
