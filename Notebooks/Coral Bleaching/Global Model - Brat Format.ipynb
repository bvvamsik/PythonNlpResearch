{
 "metadata": {
  "name": "",
  "signature": "sha256:9d34dcf042ac696ca62cd7865a53f6a9a075198874ddadc7e76b02d77c46fa15"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from Decorators import timeit, memoize\n",
      "from BrattEssay import load_bratt_essays\n",
      "from processessays import process_sentences, process_essays\n",
      "from wordtagginghelper import flatten_to_wordlevel_feat_tags, get_wordlevel_ys_by_code\n",
      "\n",
      "from featureextractor import FeatureExtractor\n",
      "from featuretransformer import FeatureTransformer\n",
      "from featureextractionfunctions import *\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from wordtagginghelper import *\n",
      "from IterableFP import flatten\n",
      "\n",
      "# Classifiers\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "# END Classifiers\n",
      "\n",
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "logger = logging.getLogger()\n",
      "\n",
      "b4 = set(dir())\n",
      "\n",
      "MIN_SENTENCE_FREQ   = 0        # i.e. df. Note this is calculated BEFORE creating windows\n",
      "REMOVE_INFREQUENT   = False    # if false, infrequent words are replaced with \"INFREQUENT\"\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = False\n",
      "REPLACE_NUMS        = True     # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 3\n",
      "REMOVE_STOP_WORDS   = False\n",
      "REMOVE_PUNCTUATION  = True\n",
      "LOWER_CASE          = False\n",
      "WINDOW_SIZE = 7\n",
      "\n",
      "PCT_VALIDATION      = 0.2\n",
      "MIN_FEAT_FREQ       = 15        # 15 best so far, and faster also\n",
      "\n",
      "after = set(dir())\n",
      "diff = after - b4\n",
      "# construct unique key using settings for pickling\n",
      "pickle_key = \"_\".join(map(lambda k: k + \"_\" + str(eval(k)), sorted(filter(lambda a: a != \"b4\", diff))))\n",
      "print pickle_key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LOWER_CASE_False_MIN_FEAT_FREQ_15_MIN_SENTENCE_FREQ_0_MIN_SENTENCE_LENGTH_3_PCT_VALIDATION_0.2_REMOVE_INFREQUENT_False_REMOVE_PUNCTUATION_True_REMOVE_STOP_WORDS_False_REPLACE_NUMS_True_SPELLING_CORRECT_True_STEM_False_WINDOW_SIZE_7\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "settings = Settings.Settings()\n",
      "filename = settings.data_directory + \"CoralBleaching/BrattData/pickled_\" + pickle_key\n",
      "if os.path.exists(filename):\n",
      "    logger.info(\"Unpickling essays\")\n",
      "    tagged_essays = pickle.load(open(filename))\n",
      "else:\n",
      "    # load from disk\n",
      "    logger.info(\"Loading Essays\")\n",
      "    # parse bratt essays\n",
      "    essays = load_bratt_essays()\n",
      "\n",
      "    logger.info(\"Processing Essays\")\n",
      "    #TODO pickle this so save time - create a hash key per unique parameter set\n",
      "\n",
      "    tagged_essays = process_essays(essays,\n",
      "                                   min_df=MIN_SENTENCE_FREQ,\n",
      "                                   remove_infrequent=REMOVE_INFREQUENT,\n",
      "                                   spelling_correct=SPELLING_CORRECT,\n",
      "                                   replace_nums=REPLACE_NUMS,\n",
      "                                   stem=STEM,\n",
      "                                   remove_stop_words=REMOVE_STOP_WORDS,\n",
      "                                   remove_punctuation=REMOVE_PUNCTUATION,\n",
      "                                   lower_case=False)\n",
      "    pickle.dump(tagged_essays, open(filename,\"w+\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-12-07 15:35:27,306 : INFO : Unpickling essays\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = (WINDOW_SIZE-1) / 2\n",
      "unigram_window = fact_extract_positional_word_features(offset)\n",
      "biigram_window = fact_extract_ngram_features(offset, 2)\n",
      "\n",
      "#TODO - add POS TAGS (positional)\n",
      "#TODO - add dep parse feats\n",
      "#TODO - memoize features above for speed\n",
      "extractors = [unigram_window, biigram_window]\n",
      "\n",
      "feature_extractor = FeatureExtractor(extractors)\n",
      "essay_feats = feature_extractor.transform(tagged_essays)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Data Partitioning and Training \"\"\"\n",
      "TD, VD = train_test_split(essay_feats, test_size=PCT_VALIDATION)\n",
      "td_feats, td_tags = flatten_to_wordlevel_feat_tags(TD)\n",
      "vd_feats, vd_tags = flatten_to_wordlevel_feat_tags(VD)\n",
      "\n",
      "feature_transformer = FeatureTransformer(min_feature_freq=MIN_FEAT_FREQ)\n",
      "\n",
      "td_X = feature_transformer.fit_transform(td_feats)\n",
      "vd_X = feature_transformer.transform(vd_feats)\n",
      "td_ys_bycode = get_wordlevel_ys_by_code(td_tags)\n",
      "vd_ys_bycode = get_wordlevel_ys_by_code(vd_tags)\n",
      "\n",
      "#TODO\n",
      "fn_create_cls = lambda : LogisticRegression()\n",
      "all_tags = set(flatten(td_tags))\n",
      "\n",
      "# use more tags for training for sentence level classifier\n",
      "train_tags = [c for c in all_tags if c != \"it\"]\n",
      "test_tags = [c for c in all_tags if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "\"\"\" TRAIN \"\"\"\n",
      "tag2Classifier = train_wordlevel_classifier(td_X, td_ys_bycode, fn_create_cls, train_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TEST \"\"\"\n",
      "td_metricsByTag, td_wt_mean_prfa, td_mean_prfa = test_word_level_classifiers(td_X, td_ys_bycode, tag2Classifier, test_tags)\n",
      "vd_metricsByTag, vd_wt_mean_prfa, vd_mean_prfa = test_word_level_classifiers(vd_X, vd_ys_bycode, tag2Classifier, test_tags)\n",
      "\n",
      "print fn_create_cls()\n",
      "# print results for each code\n",
      "print_metrics_for_codes(td_metricsByTag,vd_metricsByTag)\n",
      "\n",
      "# print macro measures\n",
      "print \"\\nTraining   Performance\"\n",
      "print \"Weighted:\" + str(td_wt_mean_prfa)\n",
      "print \"Mean    :\" + str(td_mean_prfa)\n",
      "print \"\\nValidation Performance\"\n",
      "print \"Weighted:\" + str(vd_wt_mean_prfa)\n",
      "print \"Mean    :\" + str(vd_mean_prfa)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n",
        "Training:       Validation Performance\n",
        "            Training             Validation          \n",
        "TAG:        1                   \n",
        "recall:     0.698985343856       0.549783549784      \n",
        "precision:  0.884450784593       0.769696969697      \n",
        "f1:         0.780856423174       0.641414141414      \n",
        "accuracy:   0.988974082758       0.982733463035      \n",
        "sentences:  887                  231                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        11                  \n",
        "recall:     0.319327731092       0.3                 \n",
        "precision:  1.0                  0.857142857143      \n",
        "f1:         0.484076433121       0.444444444444      \n",
        "accuracy:   0.997433622711       0.998176070039      \n",
        "sentences:  119                  20                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        12                  \n",
        "recall:     0.516949152542       0.380952380952      \n",
        "precision:  0.953125             0.888888888889      \n",
        "f1:         0.67032967033        0.533333333333      \n",
        "accuracy:   0.998098979786       0.99829766537       \n",
        "sentences:  118                  21                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        13                  \n",
        "recall:     0.623693379791       0.325               \n",
        "precision:  0.927461139896       0.619047619048      \n",
        "f1:         0.745833333333       0.426229508197      \n",
        "accuracy:   0.996134592231       0.995744163424      \n",
        "sentences:  287                  40                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        14                  \n",
        "recall:     0.734082397004       0.521739130435      \n",
        "precision:  0.844827586207       0.545454545455      \n",
        "f1:         0.785571142285       0.533333333333      \n",
        "accuracy:   0.996609847285       0.997446498054      \n",
        "sentences:  267                  23                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        2                   \n",
        "recall:     0.531609195402       0.174757281553      \n",
        "precision:  0.872641509434       0.4                 \n",
        "f1:         0.660714285714       0.243243243243      \n",
        "accuracy:   0.993980102655       0.986381322957      \n",
        "sentences:  348                  103                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        3                   \n",
        "recall:     0.6317715959         0.589743589744      \n",
        "precision:  0.828214971209       0.636678200692      \n",
        "f1:         0.716777408638       0.61231281198       \n",
        "accuracy:   0.978391736899       0.971668287938      \n",
        "sentences:  1366                 312                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        4                   \n",
        "recall:     0.74358974359        0.739726027397      \n",
        "precision:  0.962085308057       0.771428571429      \n",
        "f1:         0.838842975207       0.755244755245      \n",
        "accuracy:   0.997528673722       0.995744163424      \n",
        "sentences:  273                  73                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        5                   \n",
        "recall:     0.448275862069       0.326203208556      \n",
        "precision:  0.85046728972        0.677777777778      \n",
        "f1:         0.587096774194       0.440433212996      \n",
        "accuracy:   0.987833470629       0.981152723735      \n",
        "sentences:  609                  187                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        50                  \n",
        "recall:     0.842154566745       0.794117647059      \n",
        "precision:  0.938413361169       0.876267748479      \n",
        "f1:         0.887682053814       0.833172613308      \n",
        "accuracy:   0.985583930042       0.978964007782      \n",
        "sentences:  2135                 544                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        6                   \n",
        "recall:     0.441860465116       0.333333333333      \n",
        "precision:  0.883720930233       0.928571428571      \n",
        "f1:         0.589147286822       0.490566037736      \n",
        "accuracy:   0.998320765477       0.99671692607       \n",
        "sentences:  86                   39                  \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        7                   \n",
        "recall:     0.460296096904       0.345945945946      \n",
        "precision:  0.916890080429       0.79012345679       \n",
        "f1:         0.612903225806       0.481203007519      \n",
        "accuracy:   0.986312654458       0.983219844358      \n",
        "sentences:  743                  185                 \n",
        "\n",
        "            Training             Validation          \n",
        "TAG:        explicit            \n",
        "recall:     0.2959697733         0.162995594714      \n",
        "precision:  0.796610169492       0.55223880597       \n",
        "f1:         0.431588613407       0.251700680272      \n",
        "accuracy:   0.980387808124       0.973249027237      \n",
        "sentences:  794                  227                 \n",
        "\n",
        "\n",
        "Training   Performance\n",
        "Weighted:Recall: 0.6264, Precision: 0.8860, F1: 0.7212, Accuracy: 0.9865, Codes:  8032\n",
        "Mean    :Recall: 0.5607, Precision: 0.8968, F1: 0.6763, Accuracy: 0.9912, Codes:    13\n",
        "\n",
        "Validation Performance\n",
        "Weighted:Recall: 0.5132, Precision: 0.7273, F1: 0.5834, Accuracy: 0.9805, Codes:  2005\n",
        "Mean    :Recall: 0.4265, Precision: 0.7164, F1: 0.5144, Accuracy: 0.9877, Codes:    13\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(td_metricsByTag[\"50\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "Rpfa.rpfa"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}