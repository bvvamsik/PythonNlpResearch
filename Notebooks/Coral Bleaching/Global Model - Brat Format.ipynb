{
 "metadata": {
  "name": "",
  "signature": "sha256:e9836367832c7148fe9480abbe92f6db3c9a0e924ee72d0a27670b1e42dd063e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "from Decorators import timeit, memoize, memoize_to_disk\n",
      "from BrattEssay import load_bratt_essays\n",
      "from processessays import process_sentences, process_essays\n",
      "from wordtagginghelper import flatten_to_wordlevel_feat_tags, get_wordlevel_ys_by_code\n",
      "from sent_feats_for_stacking import get_sent_feature_for_stacking, CAUSAL_REL, CAUSE_RESULT, RESULT_REL\n",
      "\n",
      "from featureextractortransformer import FeatureExtractorTransformer\n",
      "from featurevectorizer import FeatureVectorizer\n",
      "from featureextractionfunctions import *\n",
      "from CrossValidation import cross_validation\n",
      "from wordtagginghelper import *\n",
      "from IterableFP import flatten\n",
      "from DictionaryHelper import tally_items\n",
      "\n",
      "# Classifiers\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "from Rpfa import mean_rpfa, weighted_mean_rpfa\n",
      "from metric_processing import *\n",
      "# END Classifiers\n",
      "\n",
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "logger = logging.getLogger()\n",
      "\n",
      "# Settings for essay pre-processing\n",
      "MIN_SENTENCE_FREQ   = 2        # i.e. df. Note this is calculated BEFORE creating windows\n",
      "REMOVE_INFREQUENT   = False    # if false, infrequent words are replaced with \"INFREQUENT\"\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = False    # note this tends to improve matters, but is needed to be on for pos tagging and dep parsing\n",
      "                               # makes tagging model better but causal model worse\n",
      "REPLACE_NUMS        = True     # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 3\n",
      "REMOVE_STOP_WORDS   = False\n",
      "REMOVE_PUNCTUATION  = True\n",
      "LOWER_CASE          = False\n",
      "# construct unique key using settings for pickling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "settings = Settings.Settings()\n",
      "essay_filename_prefix = settings.data_directory + \"CoralBleaching/BrattData/Pickled/essays_pickled_\"\n",
      "processed_essay_filename_prefix = settings.data_directory + \"CoralBleaching/BrattData/Pickled/essays_proc_pickled_\"\n",
      "features_filename_prefix = settings.data_directory + \"CoralBleaching/BrattData/Pickled/feats_pickled_\"\n",
      "\n",
      "logger.info(\"Loading Essays\")\n",
      "mem_load_brat_essays = memoize_to_disk(filename_prefix=essay_filename_prefix)(load_bratt_essays)\n",
      "essays = mem_load_brat_essays()\n",
      "\n",
      "logger.info(\"Processing Essays\")\n",
      "mem_process_essays = memoize_to_disk(filename_prefix=processed_essay_filename_prefix)(process_essays)\n",
      "tagged_essays = mem_process_essays(min_df=MIN_SENTENCE_FREQ, remove_infrequent=REMOVE_INFREQUENT,\n",
      "                                       spelling_correct=SPELLING_CORRECT,\n",
      "                                       replace_nums=REPLACE_NUMS, stem=STEM, remove_stop_words=REMOVE_STOP_WORDS,\n",
      "                                       remove_punctuation=REMOVE_PUNCTUATION, lower_case=LOWER_CASE)\n",
      "\n",
      "# most params below exist ONLY for the purposes of the hashing to and from disk\n",
      "@memoize_to_disk(filename_prefix=features_filename_prefix)\n",
      "def extract_features(min_df,\n",
      "                     rem_infreq, spell_correct,\n",
      "                     replace_nos, stem, rem_stop_wds,\n",
      "                     rem_punc, l_case,\n",
      "                     win_size, pos_win_size, min_feat_freq,\n",
      "                     extractors):\n",
      "    feature_extractor = FeatureExtractorTransformer(extractors)\n",
      "    return feature_extractor.transform(tagged_essays)\n",
      "\n",
      "# FEATURE SETTINGS\n",
      "WINDOW_SIZE         = 7\n",
      "POS_WINDOW_SIZE     = 1\n",
      "MIN_FEAT_FREQ       = 5        # 5 best so far\n",
      "CV_FOLDS            = 2\n",
      "# END FEATURE SETTINGS\n",
      "\n",
      "# not hashed as don't affect persistence of feature processing\n",
      "SPARSE_WD_FEATS     = True\n",
      "SPARSE_SENT_FEATS   = True\n",
      "MIN_TAG_FREQ        = 5\n",
      "LOOK_BACK           = 0     # how many sentences to look back when predicting tags\n",
      "# end not hashed\n",
      "\n",
      "offset = (WINDOW_SIZE-1) / 2\n",
      "unigram_window = fact_extract_positional_word_features(offset)\n",
      "biigram_window = fact_extract_ngram_features(offset, 2)\n",
      "pos_tag_window = fact_extract_positional_POS_features((POS_WINDOW_SIZE-1/2))\n",
      "#TODO - add POS TAGS (positional)\n",
      "#TODO - add dep parse feats\n",
      "#extractors = [unigram_window, biigram_window, pos_tag_window]\n",
      "extractors = [unigram_window, biigram_window]\n",
      "\n",
      "essay_feats = extract_features(min_df=MIN_SENTENCE_FREQ, rem_infreq=REMOVE_INFREQUENT,\n",
      "                               spell_correct=SPELLING_CORRECT,\n",
      "                               replace_nos=REPLACE_NUMS, stem=STEM, rem_stop_wds=REMOVE_STOP_WORDS,\n",
      "                               rem_punc=REMOVE_PUNCTUATION, l_case=LOWER_CASE,\n",
      "                               win_size=WINDOW_SIZE, pos_win_size=POS_WINDOW_SIZE,\n",
      "                               min_feat_freq=MIN_FEAT_FREQ, extractors=extractors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-12-22 22:23:52,738 : INFO : Loading Essays\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-12-22 22:23:53,681 : INFO : Processing Essays\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_, lst_all_tags = flatten_to_wordlevel_feat_tags(essay_feats)\n",
      "\"\"\" Get all tags above the frequency above \"\"\"\n",
      "\"\"\" NOTE WHEN OUTPUTING RESULTS WE NEED TO USE ALL TAGS, NOT HIGHER FREQ TAGS \"\"\"\n",
      "flt_lst_tags = flatten(lst_all_tags)\n",
      "tally_tags = tally_items(flt_lst_tags, freq_threshold=MIN_TAG_FREQ)\n",
      "all_tags_above_threshold = set(tally_tags.keys())\n",
      "if \"it\" in all_tags_above_threshold:\n",
      "    all_tags_above_threshold.remove(\"it\")\n",
      "\n",
      "# use more tags for training for sentence level classifier\n",
      "\"\"\" TAGS \"\"\"\n",
      "regular_tags = [t for t in all_tags_above_threshold if t[0].isdigit()]\n",
      "cause_tags = [\"Causer\", \"Result\", \"explicit\"]\n",
      "causal_rel_tags = [CAUSAL_REL, CAUSE_RESULT, RESULT_REL]\n",
      "\n",
      "#wd_train_tags = list(all_tags_above_threshold)\n",
      "wd_train_tags = regular_tags + cause_tags\n",
      "#wd_test_tags  = [tag for tag in all_tags if tag.isdigit() or tag == \"explicit\"]\n",
      "wd_test_tags  = wd_train_tags\n",
      "\n",
      "# tags from tagging model used to train the stacked model\n",
      "sent_input_feat_tags = wd_train_tags\n",
      "# find interactions between these predicted tags from the word tagger to feed to the sentence tagger\n",
      "#sent_input_interaction_tags = [tag for tag in all_tags_above_threshold if tag.isdigit() or tag in set((\"Causer\", \"Result\", \"explicit\")) ]\n",
      "sent_input_interaction_tags = wd_train_tags\n",
      "# tags to train (as output) for the sentence based classifier\n",
      "sent_output_train_test_tags = regular_tags + causal_rel_tags\n",
      "\n",
      "assert \"Causer\" in sent_input_feat_tags   , \"To extract causal relations, we need Causer tags\"\n",
      "assert \"Result\" in sent_input_feat_tags   , \"To extract causal relations, we need Result tags\"\n",
      "assert \"explicit\" in sent_input_feat_tags , \"To extract causal relations, we need explicit tags\"\n",
      "# tags to evaluate against\n",
      "\n",
      "folds = cross_validation(essay_feats, CV_FOLDS)\n",
      "\"\"\"Word level metrics \"\"\"\n",
      "wd_td_wt_mean_prfa, wd_vd_wt_mean_prfa, wd_td_mean_prfa, wd_vd_mean_prfa = [], [], [], []\n",
      "wd_td_all_metricsByTag, wd_vd_all_metricsByTag = defaultdict(list), defaultdict(list)\n",
      "\n",
      "\"\"\"Sentence level metrics \"\"\"\n",
      "sent_td_wt_mean_prfa, sent_vd_wt_mean_prfa, sent_td_mean_prfa, sent_vd_mean_prfa = [], [], [], []\n",
      "sent_td_all_metricsByTag , sent_vd_all_metricsByTag = defaultdict(list), defaultdict(list)\n",
      "\n",
      "# Linear SVC seems to do better\n",
      "#fn_create_cls = lambda: LogisticRegression()\n",
      "fn_create_wd_cls    = lambda : LinearSVC(C=1.0)\n",
      "fn_create_sent_cls  = lambda : LinearSVC(C=1.0)\n",
      "#fn_create_sent_cls  = lambda : GradientBoostingClassifier() #F1 = 0.5312 on numeric + 5b + casual codes for sentences\n",
      "\n",
      "if type(fn_create_sent_cls()) == GradientBoostingClassifier:\n",
      "    SPARSE_SENT_FEATS = False\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO Parallelize\n",
      "for i,(essays_TD, essays_VD) in enumerate(folds):\n",
      "\n",
      "    # TD and VD are lists of Essay objects. The sentences are lists\n",
      "    # of featureextractortransformer.Word objects\n",
      "    print \"\\nFold %s\" % i\n",
      "    print \"Training Tagging Model\"\n",
      "    \"\"\" Data Partitioning and Training \"\"\"\n",
      "    td_feats, td_tags = flatten_to_wordlevel_feat_tags(essays_TD)\n",
      "    vd_feats, vd_tags = flatten_to_wordlevel_feat_tags(essays_VD)\n",
      "\n",
      "    feature_transformer = FeatureVectorizer(min_feature_freq=MIN_FEAT_FREQ, sparse=SPARSE_WD_FEATS)\n",
      "    td_X = feature_transformer.fit_transform(td_feats)\n",
      "    vd_X = feature_transformer.transform(vd_feats)\n",
      "    td_ys_bytag = get_wordlevel_ys_by_code(td_tags, wd_train_tags)\n",
      "    vd_ys_bytag = get_wordlevel_ys_by_code(vd_tags, wd_train_tags)\n",
      "\n",
      "    \"\"\" TRAIN Tagger \"\"\"\n",
      "    tag2word_classifier = train_classifier_per_code(td_X, td_ys_bytag, fn_create_wd_cls, wd_train_tags)\n",
      "\n",
      "    \"\"\" TEST Tagger \"\"\"\n",
      "    td_metricsByTag, td_wt_mean_prfa, td_mean_prfa = test_classifier_per_code(td_X, td_ys_bytag, tag2word_classifier, wd_test_tags)\n",
      "    vd_metricsByTag, vd_wt_mean_prfa, vd_mean_prfa = test_classifier_per_code(vd_X, vd_ys_bytag, tag2word_classifier, wd_test_tags)\n",
      "\n",
      "    wd_td_wt_mean_prfa.append(td_wt_mean_prfa), wd_td_mean_prfa.append(td_mean_prfa)\n",
      "    wd_vd_wt_mean_prfa.append(vd_wt_mean_prfa), wd_vd_mean_prfa.append(vd_mean_prfa)\n",
      "    merge_metrics(td_metricsByTag, wd_td_all_metricsByTag)\n",
      "    merge_metrics(vd_metricsByTag, wd_vd_all_metricsByTag)\n",
      "\n",
      "    print \"Training Sentence Model\"\n",
      "    \"\"\" SENTENCE LEVEL PREDICTIONS FROM STACKING \"\"\"\n",
      "    sent_td_xs, sent_td_ys_bycode = get_sent_feature_for_stacking(sent_input_feat_tags, sent_input_interaction_tags, essays_TD, td_X, td_ys_bytag, tag2word_classifier, SPARSE_SENT_FEATS, LOOK_BACK)\n",
      "    sent_vd_xs, sent_vd_ys_bycode = get_sent_feature_for_stacking(sent_input_feat_tags, sent_input_interaction_tags, essays_VD, vd_X, vd_ys_bytag, tag2word_classifier, SPARSE_SENT_FEATS, LOOK_BACK)\n",
      "\n",
      "    \"\"\" Train Stacked Classifier \"\"\"\n",
      "    tag2sent_classifier = train_classifier_per_code(sent_td_xs, sent_td_ys_bycode , fn_create_sent_cls, sent_output_train_test_tags)\n",
      "\n",
      "    \"\"\" Test Stack Classifier \"\"\"\n",
      "    s_td_metricsByTag, s_td_wt_mean_prfa, s_td_mean_prfa = test_classifier_per_code(sent_td_xs, sent_td_ys_bycode , tag2sent_classifier, sent_output_train_test_tags )\n",
      "    s_vd_metricsByTag, s_vd_wt_mean_prfa, s_vd_mean_prfa = test_classifier_per_code(sent_vd_xs, sent_vd_ys_bycode , tag2sent_classifier, sent_output_train_test_tags )\n",
      "\n",
      "    sent_td_wt_mean_prfa.append(s_td_wt_mean_prfa), sent_td_mean_prfa.append(s_td_mean_prfa)\n",
      "    sent_vd_wt_mean_prfa.append(s_vd_wt_mean_prfa), sent_vd_mean_prfa.append(s_vd_mean_prfa)\n",
      "    merge_metrics(s_td_metricsByTag, sent_td_all_metricsByTag)\n",
      "    merge_metrics(s_vd_metricsByTag, sent_vd_all_metricsByTag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fold 0\n",
        "Training Tagging Model\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for : 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for : 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for : 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training Sentence Model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for : 11\n",
        "Training for : 12\n",
        "Training for : 13\n",
        "Training for : 14\n",
        "Training for : 2\n",
        "Training for : 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for : 5\n",
        "Training for : 50\n",
        "Training for : 5b\n",
        "Training for : 6\n",
        "Training for : 7\n",
        "Training for : _C->R\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " _CRel\n",
        "Training for : _RRel\n",
        "\n",
        "Fold 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training Tagging Model\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-0c554bbedb5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfeature_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_feature_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMIN_FEAT_FREQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPARSE_WD_FEATS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtd_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvd_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvd_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtd_ys_bytag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wordlevel_ys_by_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_train_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print results for each code\n",
      "wd_mean_td_metrics = agg_metrics(wd_td_all_metricsByTag, cv_mean_rpfa)\n",
      "wd_mean_vd_metrics = agg_metrics(wd_vd_all_metricsByTag, cv_mean_rpfa_total_codes)\n",
      "\n",
      "sent_mean_td_metrics = agg_metrics(sent_td_all_metricsByTag, cv_mean_rpfa)\n",
      "sent_mean_vd_metrics = agg_metrics(sent_vd_all_metricsByTag, cv_mean_rpfa_total_codes)\n",
      "\n",
      "print \"TAGGING\"\n",
      "print_metrics_for_codes(wd_mean_td_metrics, wd_mean_vd_metrics)\n",
      "\n",
      "print \"\\n\\nSENTENCE\"\n",
      "print_metrics_for_codes(sent_mean_td_metrics, sent_mean_vd_metrics)\n",
      "\n",
      "print fn_create_wd_cls()\n",
      "# print macro measures\n",
      "print \"\\nTAGGING\"\n",
      "print \"\\nTraining   Performance\"\n",
      "print \"Weighted:\" + str(cv_mean_rpfa(wd_td_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(cv_mean_rpfa(wd_td_mean_prfa))\n",
      "\n",
      "print \"\\nValidation Performance\"\n",
      "print \"Weighted:\" + str(cv_mean_rpfa_total_codes(wd_vd_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(cv_mean_rpfa_total_codes(wd_vd_mean_prfa))\n",
      "\n",
      "print \"\\n\\n\"\n",
      "print fn_create_sent_cls()\n",
      "# print macro measures\n",
      "print \"\\nSENTENCE\"\n",
      "print \"\\nTraining   Performance\"\n",
      "print \"Weighted:\" + str(cv_mean_rpfa(sent_td_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(cv_mean_rpfa(sent_td_mean_prfa))\n",
      "\n",
      "print \"\\nValidation Performance\"\n",
      "print \"Weighted:\" + str(cv_mean_rpfa_total_codes(sent_vd_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(cv_mean_rpfa_total_codes(sent_vd_mean_prfa))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}