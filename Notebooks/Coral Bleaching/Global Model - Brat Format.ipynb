{
 "metadata": {
  "name": "",
  "signature": "sha256:5d6db0f660d278c950946bd7efc26a193300286c6293a900e4cca6720df6028c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from Decorators import timeit, memoize\n",
      "from BrattEssay import load_bratt_essays\n",
      "from processessays import process_sentences, process_essays\n",
      "from helperfunctions import get_word_feat_tags, get_ys_by_code\n",
      "\n",
      "from featureextractor import FeatureExtractor\n",
      "from featuretransformer import FeatureTransformer\n",
      "from featureextractionfunctions import *\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from helperfunctions import *\n",
      "\n",
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "logger = logging.getLogger()\n",
      "\n",
      "b4 = set(dir())\n",
      "\n",
      "MIN_SENTENCE_FREQ   = 0        # i.e. df. Note this is calculated BEFORE creating windows\n",
      "REMOVE_INFREQUENT   = False    # if false, infrequent words are replaced with \"INFREQUENT\"\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = False\n",
      "REPLACE_NUMS        = True     # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 3\n",
      "REMOVE_STOP_WORDS   = False\n",
      "REMOVE_PUNCTUATION  = True\n",
      "LOWER_CASE          = False\n",
      "WINDOW_SIZE = 7\n",
      "\n",
      "PCT_VALIDATION      = 0.2\n",
      "MIN_FEAT_FREQ       = 15        # 15 best so far, and faster also\n",
      "\n",
      "after = set(dir())\n",
      "diff = after - b4\n",
      "# construct unique key using settings for pickling\n",
      "pickle_key = \"_\".join(map(lambda k: k + \"_\" + str(eval(k)), sorted(filter(lambda a: a != \"b4\", diff))))\n",
      "print pickle_key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LOWER_CASE_False_MIN_FEAT_FREQ_15_MIN_SENTENCE_FREQ_0_MIN_SENTENCE_LENGTH_3_PCT_VALIDATION_0.2_REMOVE_INFREQUENT_False_REMOVE_PUNCTUATION_True_REMOVE_STOP_WORDS_False_REPLACE_NUMS_True_SPELLING_CORRECT_True_STEM_False_WINDOW_SIZE_7\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "settings = Settings.Settings()\n",
      "filename = settings.data_directory + \"CoralBleaching/BrattData/pickled_\" + pickle_key\n",
      "if os.path.exists(filename):\n",
      "    logger.info(\"Unpickling essays\")\n",
      "    tagged_essays = pickle.load(open(filename))\n",
      "else:\n",
      "    # load from disk\n",
      "    logger.info(\"Loading Essays\")\n",
      "    # parse bratt essays\n",
      "    essays = load_bratt_essays()\n",
      "\n",
      "    logger.info(\"Processing Essays\")\n",
      "    #TODO pickle this so save time - create a hash key per unique parameter set\n",
      "\n",
      "    tagged_essays = process_essays(essays,\n",
      "                                   min_df=MIN_SENTENCE_FREQ,\n",
      "                                   remove_infrequent=REMOVE_INFREQUENT,\n",
      "                                   spelling_correct=SPELLING_CORRECT,\n",
      "                                   replace_nums=REPLACE_NUMS,\n",
      "                                   stem=STEM,\n",
      "                                   remove_stop_words=REMOVE_STOP_WORDS,\n",
      "                                   remove_punctuation=REMOVE_PUNCTUATION,\n",
      "                                   lower_case=False)\n",
      "    pickle.dump(tagged_essays, open(filename,\"w+\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-12-07 12:22:02,041 : INFO : Unpickling essays\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = (WINDOW_SIZE-1) / 2\n",
      "unigram_window = fact_extract_positional_word_features(offset)\n",
      "biigram_window = fact_extract_ngram_features(offset, 2)\n",
      "\n",
      "#TODO - add POS TAGS (positional)\n",
      "#TODO - add dep parse feats\n",
      "#TODO - memoize features above for speed\n",
      "extractors = [unigram_window, biigram_window]\n",
      "\n",
      "feature_extractor = FeatureExtractor(extractors)\n",
      "essay_feats = feature_extractor.transform(tagged_essays)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Data Partitioning and Training \"\"\"\n",
      "TD, VD = train_test_split(essay_feats, test_size=PCT_VALIDATION)\n",
      "td_feats, td_tags = get_word_feat_tags(TD)\n",
      "\n",
      "feature_transformer = FeatureTransformer(min_feature_freq=MIN_FEAT_FREQ)\n",
      "td_X = feature_transformer.fit_transform(td_feats)\n",
      "td_ys_bycode = get_ys_by_code(td_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}