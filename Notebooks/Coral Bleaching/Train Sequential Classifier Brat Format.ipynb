{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Sequential Based Classier on the Coral Bleaching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports \"\"\"\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from Metrics import rpf1a\n",
    "from Rpfa import rpfa, weighted_mean_rpfa\n",
    "from BrattEssay import load_bratt_essays\n",
    "from WindowSplitter import split_into_windows\n",
    "\n",
    "from IterableFP import flatten\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\" TODO \n",
    "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Settings \"\"\"\n",
    "\"\"\" Start Script \"\"\"\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "MIN_SENTENCE_FREQ = 1\n",
    "PCT_VALIDATION = 0.25\n",
    "\n",
    "STEM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Essays\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154 files found\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05571.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05904.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_BGJD_1_CB_ES-05733.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_ERSK_7_CB_ES-05798.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_KYLS_5_CB_ES-05671.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_LRJE_5_CB_ES-05128.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05612.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05617.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05632.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05640.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SWSP_4_CB_ES-05459.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05484.ann file as .txt file is no essay. //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05485.ann file as .txt file is no essay.  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_2_CB_ES-05548.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFMV_3_CB_ES-05845.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05715.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05721.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06128.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06132.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05025.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05030.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_2_CB_ES-06140.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06149.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06153.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTKP_7-8_CB_ES-06174.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415post_TWNB_2_CB_ES-04948.ann file as .txt file is no essay.'\n",
      "1128 essays processed\n",
      "Running Spelling Correction..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load Essays \"\"\"\n",
    "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/\")\n",
    "\n",
    "all_codes = set()\n",
    "all_words = []\n",
    "\n",
    "for essay in essays:\n",
    "    for sentence in essay.tagged_sentences:\n",
    "        for w, tags in sentence:\n",
    "            all_words.append(w)\n",
    "            all_codes.update(tags)\n",
    "                \n",
    "# Correct miss-spellings\n",
    "from SpellingCorrector import SpellingCorrector\n",
    "\n",
    "print \"Running Spelling Correction..\"\n",
    "corrector = SpellingCorrector(all_words)\n",
    "corrections = defaultdict(int)\n",
    "code_freq = defaultdict(int)\n",
    "for essay in essays:\n",
    "    for i, sentence in enumerate(essay.tagged_sentences):\n",
    "        for j, (w, tags) in enumerate(sentence):\n",
    "            for t in tags:\n",
    "                code_freq[t] += 1\n",
    "            # common error is ..n't and ..nt\n",
    "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
    "                cw = w[:-3] + \"nt\"\n",
    "            elif w.endswith(\"'s\"):\n",
    "                cw = w[:-2]\n",
    "            else:\n",
    "                cw = corrector.correct(w)\n",
    "            if cw != w:\n",
    "                corrections[(w,cw)] += 1\n",
    "                sentence[j] = (cw, tags)            \n",
    "            \n",
    "wd_sent_freq = defaultdict(int)\n",
    "for essay in essays:\n",
    "    for sentence in essay.tagged_sentences:\n",
    "        wds, tag_list = zip(*sentence)\n",
    "        unique_wds = set(wds)\n",
    "        for w in unique_wds: \n",
    "            wd_sent_freq[w] += 1\n",
    "            \n",
    "numeric_codes = set([c for c in all_codes if not c.isalpha()])\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Spelling Corrections\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DictionaryHelper import *\n",
    "cor_srtd = sort_by_value(corrections, reverse = True)\n",
    "print MIN_SENTENCE_FREQ\n",
    "cor_srtd[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Corpus in CRF Format (list of list of tuples(word,tag))\n",
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, more than 1 numeric code\n",
      "\tset(['50', '7'])\n",
      "\tChose: 7\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_by_code = defaultdict(list)\n",
    "\n",
    "#store raw sentence\n",
    "ix2sentence = {}\n",
    "ix2tags = {}\n",
    "\n",
    "#TODO Try IOBE encoding\n",
    "out_tag = 'O'\n",
    "ix = -1\n",
    "#Keep untagged sentences for testing\n",
    "sentences = []\n",
    "tags4sentences = []\n",
    "CODE_ALL_NUMERIC = \"All_Numeric_Codes\"\n",
    "for essay in essays:\n",
    "    for i, sentence in enumerate(essay.tagged_sentences):\n",
    "        ix+= 1\n",
    "        \n",
    "        sent = [(w,t) for (w,t) in sentence if wd_sent_freq[w] >= MIN_SENTENCE_FREQ]\n",
    "        if not sent:\n",
    "            print \"No words above minimum frequency\", zip(*sentence)[0]\n",
    "            continue\n",
    "        words, tags = zip(*sent)\n",
    "        \n",
    "        ix2sentence[ix] = words\n",
    "        sentences.append(words)\n",
    "        \n",
    "        utags = set(flatten(tags))\n",
    "        tags4sentences.append(utags)\n",
    "        \n",
    "        for code in utags:\n",
    "            tagged = []\n",
    "            for w, tags in sentence:\n",
    "                tagged.append((w, code if code in tags else out_tag))\n",
    "            tagged_sentences_by_code[code].append(tagged)\n",
    "            \n",
    "        tagged_with_numeric = []\n",
    "        for w, tags in sentence:\n",
    "            intersect = tags & numeric_codes\n",
    "            if intersect:\n",
    "                code = list(intersect)[0]\n",
    "                if len(intersect) != 1:\n",
    "                    # Choose the least frequent\n",
    "                    code = sorted(intersect, key = lambda c: code_freq[c])[0]\n",
    "                    print \"Error, more than 1 numeric code\\n\\t\", intersect\n",
    "                    print \"\\tChose:\", code\n",
    "                tagged_with_numeric.append((w, code))\n",
    "            else:\n",
    "                tagged_with_numeric.append((w, out_tag))\n",
    "        tagged_sentences_by_code[CODE_ALL_NUMERIC].append(tagged_with_numeric)\n",
    "            \n",
    "        no_tags = [(w,out_tag) for w in words]\n",
    "        missing_tags = all_codes - utags\n",
    "        for code in missing_tags:\n",
    "            tagged_sentences_by_code[code].append(no_tags)\n",
    "\n",
    "#To numpy so we can filter using indexes\n",
    "for code in list(all_codes) + [CODE_ALL_NUMERIC]:\n",
    "    sents = tagged_sentences_by_code[code]\n",
    "    tagged_sentences_by_code[code] = np.asarray(sents)\n",
    "\n",
    "sentences = np.asarray(sentences)\n",
    "tags4sentences = np.asarray(tags4sentences)\n",
    "\n",
    "def tags_to_binary_matches(tags_lst, code):\n",
    "    return np.asarray([1 if code in tags else 0 for tags in tags_lst])\n",
    "\n",
    "binary_lbls_for_code = {}\n",
    "for code in all_codes:\n",
    "    binary_lbls_for_code[code] = tags_to_binary_matches(tags4sentences, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "11                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "12                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "13                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "14                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "2                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "3                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "4                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "5                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "50                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "5b                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "6                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "7                    2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "Anaphor              2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "explicit             2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "it                   2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "other                2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n",
      "rhetorical           2144 <type 'numpy.ndarray'> <type 'list'> <type 'tuple'> <type 'str'> <type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Verify all codes same length, types are the same (np.array of lists of tuples of (word,tag) pairs)\n",
    "for code in sorted(all_codes):\n",
    "    print code.ljust(20), len(tagged_sentences_by_code[code]), \\\n",
    "      type(tagged_sentences_by_code[code]), \\\n",
    "            type(tagged_sentences_by_code[code][0]), \\\n",
    "                type(tagged_sentences_by_code[code][0][0]), \\\n",
    "                    type(tagged_sentences_by_code[code][0][0][0]),\\\n",
    "                        type(tagged_sentences_by_code[code][0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "        Extract features for numbers\n",
    "        Extract features for years\n",
    "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
    "\"\"\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "WINDOW_OFFSET = int(WINDOW_SIZE / 2)\n",
    "PAD_START = [\"START\" for i in range(WINDOW_OFFSET)]\n",
    "PAD_END = [\"END\" for i in range(WINDOW_OFFSET)]\n",
    "POS_TAG = 'x'\n",
    "\n",
    "def extract_features(words, index):\n",
    "    \"\"\" Takes a list of string and the index for the word in the list\n",
    "    \"\"\"\n",
    "    \n",
    "    wds = PAD_START + list(words[::]) + PAD_END\n",
    "    ix_offset = index + WINDOW_OFFSET\n",
    "    start = ix_offset - WINDOW_OFFSET\n",
    "    end   = start + WINDOW_SIZE\n",
    "    \n",
    "    feats = {}\n",
    "    for i in range(start, end):\n",
    "        word = wds[i].strip()\n",
    "        if STEM:\n",
    "            word = stemmer.stem(word)\n",
    "        feats[\"WD\" + str(i - ix_offset) + \":\" + word] = POS_TAG\n",
    "    \n",
    "    #word_features = extract_word_features(words, feature_val=1)\n",
    "    #features.update(word_features)    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Feature Extractor\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the        ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:the   ', 'WD1:cat   ', 'WD2:sat   ', 'WD3:on    ']\n",
      "cat        ['WD-3:START', 'WD-2:START', 'WD-1:the  ', 'WD0:cat   ', 'WD1:sat   ', 'WD2:on    ', 'WD3:the   ']\n",
      "sat        ['WD-3:START', 'WD-2:the  ', 'WD-1:cat  ', 'WD0:sat   ', 'WD1:on    ', 'WD2:the   ', 'WD3:mat   ']\n",
      "on         ['WD-3:the  ', 'WD-2:cat  ', 'WD-1:sat  ', 'WD0:on    ', 'WD1:the   ', 'WD2:mat   ', 'WD3:END   ']\n",
      "the        ['WD-3:cat  ', 'WD-2:sat  ', 'WD-1:on   ', 'WD0:the   ', 'WD1:mat   ', 'WD2:END   ', 'WD3:END   ']\n",
      "mat        ['WD-3:sat  ', 'WD-2:on   ', 'WD-1:the  ', 'WD0:mat   ', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n",
      "coral      ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:coral ', 'WD1:bleach', 'WD2:END   ', 'WD3:END   ']\n",
      "bleaching  ['WD-3:START', 'WD-2:START', 'WD-1:coral', 'WD0:bleach', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n",
      "president  ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:presid', 'WD1:obama ', 'WD2:approach', 'WD3:the   ']\n",
      "obama      ['WD-3:START', 'WD-2:START', 'WD-1:presid', 'WD0:obama ', 'WD1:approach', 'WD2:the   ', 'WD3:senate,']\n",
      "approached ['WD-3:START', 'WD-2:presid', 'WD-1:obama', 'WD0:approach', 'WD1:the   ', 'WD2:senate,', 'WD3:...   ']\n",
      "the        ['WD-3:presid', 'WD-2:obama', 'WD-1:approach', 'WD0:the   ', 'WD1:senate,', 'WD2:...   ', 'WD3:END   ']\n",
      "senate,    ['WD-3:obama', 'WD-2:approach', 'WD-1:the  ', 'WD0:senate,', 'WD1:...   ', 'WD2:END   ', 'WD3:END   ']\n",
      "...        ['WD-3:approach', 'WD-2:the  ', 'WD-1:senate,', 'WD0:...   ', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comparator(kvp):\n",
    "    k,v = kvp\n",
    "    num = (k[2:]).split(\":\")[0]\n",
    "    return int(num)\n",
    "\n",
    "def test_feature_extractor_on_sentence(extractor, sent):\n",
    "    sent = sent.split(\" \")\n",
    "    for i in range(len(sent)):\n",
    "        print sent[i].ljust(10),\n",
    "        s = sorted(extractor(sent, i).items(), key = comparator)\n",
    "        print map(lambda item: str(item).ljust(10),zip(*s)[0])\n",
    "\n",
    "sent1 = \"the cat sat on the mat\"\n",
    "sent2 = \"coral bleaching\"\n",
    "sent3 = \"president obama approached the senate, ...\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent1)\n",
    "print \"\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent2)\n",
    "print \"\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent3)\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train 1608 #validation 536 #combined 2144 #total 2144\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import MalletCRF\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "n = len(sentences)\n",
    "ixs = range(n)\n",
    "num_train = int((1 - PCT_VALIDATION) * n)\n",
    "ix_train = ixs[:num_train]\n",
    "ix_valid = ixs[num_train:]\n",
    "print \"#train\", len(ix_train), \"#validation\", len(ix_valid), \"#combined\", len(ix_train + ix_valid), \"#total\", len(ixs)\n",
    "\n",
    "def train(codes):\n",
    "    code2tagger = {}\n",
    "    for code in sorted(codes):\n",
    "\n",
    "        print \"Training on\", code\n",
    "        data = tagged_sentences_by_code[code]\n",
    "        #Training Data\n",
    "        train = data[ix_train]\n",
    "        #tagger = HiddenMarkovModelTagger.train(train, verbose=True)\n",
    "        tagger = MalletCRF.train(feature_detector= extract_features, corpus=train, \n",
    "                      filename=None, weight_groups=None, gaussian_variance=1, default_label='O',\n",
    "                      transduction_type='VITERBI', max_iterations=500,\n",
    "                      add_start_state=True, add_end_state=True, trace=1)\n",
    "        code2tagger[code] = tagger\n",
    "    return code2tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total sent: 536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Metrics import rpf1a\n",
    "from Rpfa import rpfa, weighted_mean_rpfa\n",
    "\n",
    "def test_for_code(code, ixs, code2tagger):\n",
    "    #Validation Data\n",
    "    valid_sentences = sentences[ixs]\n",
    "    act_ys = binary_lbls_for_code[code][ixs]\n",
    "    \n",
    "    tagger = code2tagger[code]\n",
    "    \n",
    "    #pred_tags = [tagger.tag(s) for s in valid_sentences]\n",
    "    pred_tags = tagger.batch_tag(valid_sentences)\n",
    "    \n",
    "    tags_list = map(lambda tag_sent: set(zip(*tag_sent)[1]) , pred_tags)\n",
    "    pred_ys = tags_to_binary_matches(tags_list, code)\n",
    "    \n",
    "    num_codes = len([y for y in act_ys if y == 1])\n",
    "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
    "    print \"code:      \", code\n",
    "    print \"recall:    \", r\n",
    "    print \"precision: \", p\n",
    "    print \"f1:        \", f1\n",
    "    print \"accuracy:  \", a\n",
    "    print \"sentences: \", num_codes\n",
    "    print \"\"\n",
    "    return rpfa(r,p,f1,a,num_codes)\n",
    "\n",
    "print \"\"\n",
    "print \"total sent:\", len(ix_valid)\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data Performance\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1\n",
      "Training on 11\n",
      "Training on 12\n",
      "Training on 13\n",
      "Training on 14\n",
      "Training on 2\n",
      "Training on 3\n",
      "Training on 4\n",
      "Training on 5\n",
      "Training on 50\n",
      "Training on 6\n",
      "Training on 7\n",
      "Training on explicit\n",
      "code:       1\n",
      "recall:     0.914634146341\n",
      "precision:  0.920245398773\n",
      "f1:         0.917431192661\n",
      "accuracy:   0.983208955224\n",
      "sentences:  164\n",
      "\n",
      "code:       11\n",
      "recall:     0.452380952381\n",
      "precision:  0.904761904762\n",
      "f1:         0.603174603175\n",
      "accuracy:   0.984452736318\n",
      "sentences:  42\n",
      "\n",
      "code:       12\n",
      "recall:     0.944444444444\n",
      "precision:  0.944444444444\n",
      "f1:         0.944444444444\n",
      "accuracy:   0.998756218905\n",
      "sentences:  18\n",
      "\n",
      "code:       13\n",
      "recall:     0.62962962963\n",
      "precision:  0.871794871795\n",
      "f1:         0.731182795699\n",
      "accuracy:   0.984452736318\n",
      "sentences:  54\n",
      "\n",
      "code:       14\n",
      "recall:     1.0\n",
      "precision:  0.473684210526\n",
      "f1:         0.642857142857\n",
      "accuracy:   0.993781094527\n",
      "sentences:  9\n",
      "\n",
      "code:       2\n",
      "recall:     0.678571428571\n",
      "precision:  0.59375\n",
      "f1:         0.633333333333\n",
      "accuracy:   0.98631840796\n",
      "sentences:  28\n",
      "\n",
      "code:       3\n",
      "recall:     0.864734299517\n",
      "precision:  0.860576923077\n",
      "f1:         0.86265060241\n",
      "accuracy:   0.964552238806\n",
      "sentences:  207\n",
      "\n",
      "code:       4\n",
      "recall:     0.852941176471\n",
      "precision:  0.644444444444\n",
      "f1:         0.73417721519\n",
      "accuracy:   0.986940298507\n",
      "sentences:  34\n",
      "\n",
      "code:       5\n",
      "recall:     0.707692307692\n",
      "precision:  0.676470588235\n",
      "f1:         0.691729323308\n",
      "accuracy:   0.974502487562\n",
      "sentences:  65\n",
      "\n",
      "code:       50\n",
      "recall:     0.906095551895\n",
      "precision:  0.945017182131\n",
      "f1:         0.925147182506\n",
      "accuracy:   0.944651741294\n",
      "sentences:  607\n",
      "\n",
      "code:       6\n",
      "recall:     0.782608695652\n",
      "precision:  1.0\n",
      "f1:         0.878048780488\n",
      "accuracy:   0.996890547264\n",
      "sentences:  23\n",
      "\n",
      "code:       7\n",
      "recall:     0.605504587156\n",
      "precision:  0.857142857143\n",
      "f1:         0.709677419355\n",
      "accuracy:   0.966417910448\n",
      "sentences:  109\n",
      "\n",
      "code:       explicit\n",
      "recall:     0.078488372093\n",
      "precision:  0.771428571429\n",
      "f1:         0.142480211082\n",
      "accuracy:   0.797885572139\n",
      "sentences:  344\n",
      "\n",
      "Training Data: \n",
      "Recall: 0.6825, Precision: 0.8646, F1: 0.7115, Accuracy: 0.9290, Codes:  1704\n"
     ]
    }
   ],
   "source": [
    "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
    "\n",
    "code2tagger = train(reg_codes)\n",
    "\n",
    "td_metrics = []\n",
    "for c in sorted(reg_codes):\n",
    "    td_metrics.append(test_for_code(c, ix_train, code2tagger))\n",
    "\n",
    "print \"\"\n",
    "td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
    "print \"Training Data: \"\n",
    "print td_wt_mn_prfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Data Performance\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code:       1\n",
      "recall:     0.879518072289\n",
      "precision:  0.503448275862\n",
      "f1:         0.640350877193\n",
      "accuracy:   0.847014925373\n",
      "sentences:  83\n",
      "\n",
      "code:       11\n",
      "recall:     0.153846153846\n",
      "precision:  0.0363636363636\n",
      "f1:         0.0588235294118\n",
      "accuracy:   0.880597014925\n",
      "sentences:  13\n",
      "\n",
      "code:       12\n",
      "recall:     0.818181818182\n",
      "precision:  0.145161290323\n",
      "f1:         0.246575342466\n",
      "accuracy:   0.897388059701\n",
      "sentences:  11\n",
      "\n",
      "code:       13\n",
      "recall:     0.411764705882\n",
      "precision:  0.111111111111\n",
      "f1:         0.175\n",
      "accuracy:   0.876865671642\n",
      "sentences:  17\n",
      "\n",
      "code:       14\n",
      "recall:     0.909090909091\n",
      "precision:  0.243902439024\n",
      "f1:         0.384615384615\n",
      "accuracy:   0.940298507463\n",
      "sentences:  11\n",
      "\n",
      "code:       2\n",
      "recall:     0.631578947368\n",
      "precision:  0.328767123288\n",
      "f1:         0.432432432432\n",
      "accuracy:   0.882462686567\n",
      "sentences:  38\n",
      "\n",
      "code:       3\n",
      "recall:     0.773913043478\n",
      "precision:  0.640287769784\n",
      "f1:         0.700787401575\n",
      "accuracy:   0.858208955224\n",
      "sentences:  115\n",
      "\n",
      "code:       4\n",
      "recall:     0.9\n",
      "precision:  0.253521126761\n",
      "f1:         0.395604395604\n",
      "accuracy:   0.897388059701\n",
      "sentences:  20\n",
      "\n",
      "code:       5\n",
      "recall:     0.727272727273\n",
      "precision:  0.390243902439\n",
      "f1:         0.507936507937\n",
      "accuracy:   0.884328358209\n",
      "sentences:  44\n",
      "\n",
      "code:       50\n",
      "recall:     0.954166666667\n",
      "precision:  0.845018450185\n",
      "f1:         0.896281800391\n",
      "accuracy:   0.901119402985\n",
      "sentences:  240\n",
      "\n",
      "code:       6\n",
      "recall:     0.6\n",
      "precision:  0.0508474576271\n",
      "f1:         0.09375\n",
      "accuracy:   0.891791044776\n",
      "sentences:  5\n",
      "\n",
      "code:       7\n",
      "recall:     0.7\n",
      "precision:  0.33734939759\n",
      "f1:         0.455284552846\n",
      "accuracy:   0.875\n",
      "sentences:  40\n",
      "\n",
      "code:       explicit\n",
      "recall:     0.167701863354\n",
      "precision:  0.369863013699\n",
      "f1:         0.230769230769\n",
      "accuracy:   0.664179104478\n",
      "sentences:  161\n",
      "\n",
      "Validation Data:\n",
      "Recall: 0.6905, Precision: 0.5425, F1: 0.5790, Accuracy: 0.8379, Codes:   798\n"
     ]
    }
   ],
   "source": [
    "vd_metrics = []\n",
    "#for c in sorted(all_codes):\n",
    "for c in sorted(reg_codes):\n",
    "#for c in sorted(numeric_codes):\n",
    "    vd_metrics.append(test_for_code(c, ix_valid, code2tagger))\n",
    "\n",
    "vd_wt_mn_prfa = weighted_mean_rpfa(vd_metrics)\n",
    "print \"Validation Data:\"\n",
    "print vd_wt_mn_prfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRF - reg codes** - MIN_SENT_FREQ = 1, STEM = False, PCT_VALIDATION = 0.25  \n",
    "Validation Data:\n",
    "Recall: 0.6892, Precision: 0.7949, **F1: 0.7212**, Accuracy: 0.9061, Codes:   798\n",
    "\n",
    "**HMM - reg codes ** - MIN_SENT_FREQ = 1, STEM = True, PCT_VALIDATION = 0.25  \n",
    "Validation Data:\n",
    "Recall: 0.6905, Precision: 0.5425, **F1: 0.5790**, Accuracy: 0.8379, Codes:   798\n",
    "\n",
    "**CRF - numeric codes ** - MIN_SENT_FREQ = 1, STEM = False, PCT_VALIDATION = 0.25  \n",
    "Validation Data:\n",
    "Recall: 0.7731, Precision: 0.8286, **F1: 0.7875**, Accuracy: 0.9507, Codes:   648"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "Try training it on all codes rather than one at a time, ignoring where multiple (so ignore word codes, restrict to codes with a number in them somewhere).  \n",
    "Try IOBE encoding  \n",
    "Try wordnet synonyms, and word2vec learned synonyms (boost recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Tagger Trained on Numeric Concept Codes (None-Overlapping)\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_for_numeric_codes(code, ixs, tagger):\n",
    "    #Validation Data\n",
    "    valid_sentences = sentences[ixs]\n",
    "    act_ys = binary_lbls_for_code[code][ixs]\n",
    "    \n",
    "    pred_tags = tagger.batch_tag(valid_sentences)\n",
    "    tags_list = map(lambda tag_sent: set(zip(*tag_sent)[1]) , pred_tags)\n",
    "    pred_ys = tags_to_binary_matches(tags_list, code)\n",
    "    \n",
    "    num_codes = len([y for y in act_ys if y == 1])\n",
    "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
    "    print \"code:      \", code\n",
    "    print \"recall:    \", r\n",
    "    print \"precision: \", p\n",
    "    print \"f1:        \", f1\n",
    "    print \"accuracy:  \", a\n",
    "    print \"sentences: \", num_codes\n",
    "    print \"\"\n",
    "    return rpfa(r,p,f1,a,num_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on All_Numeric_Codes\n",
      "[MalletCRF] Training a new CRF: /var/folders/97/_9xfnqln35vb6b1ng9czmr2d4x97nt/T/modelx2SzvL.crf\n",
      "[MalletCRF] Calling mallet to train CRF...\n",
      "[MalletCRF]   Number of weights = 14028\n",
      "[MalletCRF]   CRF about to train with 500 iterations\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=0\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=1\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=2\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=3\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=4\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=5\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=6\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=7\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=8\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=9\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=10\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=11\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=12\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=13\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=14\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=15\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=16\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=17\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=18\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=19\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=20\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=21\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=22\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=23\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=24\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=25\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=26\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=27\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=28\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=29\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=30\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=31\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=32\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=33\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=34\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=35\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=36\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=37\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=38\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=39\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=40\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=41\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=42\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=43\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=44\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=45\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=46\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=47\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=48\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=49\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=50\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=51\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=52\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=53\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=54\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=55\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=56\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=57\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=58\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=59\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=60\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=61\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=62\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=63\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=64\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=65\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=66\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=67\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=68\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=69\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=70\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=71\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=72\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=73\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=74\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=75\n",
      "[MalletCRF]   CRF finished one iteration of maximizer, i=76\n",
      "[MalletCRF]   CRF training has converged, i=76\n",
      "[MalletCRF] Training complete.\n",
      "[MalletCRF]   Model stored in: /var/folders/97/_9xfnqln35vb6b1ng9czmr2d4x97nt/T/modelx2SzvL.crf\n",
      "code:       1\n",
      "recall:     0.963414634146\n",
      "precision:  0.923976608187\n",
      "f1:         0.94328358209\n",
      "accuracy:   0.988184079602\n",
      "sentences:  164\n",
      "\n",
      "code:       11\n",
      "recall:     0.761904761905\n",
      "precision:  1.0\n",
      "f1:         0.864864864865\n",
      "accuracy:   0.993781094527\n",
      "sentences:  42\n",
      "\n",
      "code:       12\n",
      "recall:     0.944444444444\n",
      "precision:  1.0\n",
      "f1:         0.971428571429\n",
      "accuracy:   0.999378109453\n",
      "sentences:  18\n",
      "\n",
      "code:       13\n",
      "recall:     0.851851851852\n",
      "precision:  0.958333333333\n",
      "f1:         0.901960784314\n",
      "accuracy:   0.993781094527\n",
      "sentences:  54\n",
      "\n",
      "code:       14\n",
      "recall:     0.555555555556\n",
      "precision:  0.833333333333\n",
      "f1:         0.666666666667\n",
      "accuracy:   0.996890547264\n",
      "sentences:  9\n",
      "\n",
      "code:       2\n",
      "recall:     0.642857142857\n",
      "precision:  0.857142857143\n",
      "f1:         0.734693877551\n",
      "accuracy:   0.991915422886\n",
      "sentences:  28\n",
      "\n",
      "code:       3\n",
      "recall:     0.92270531401\n",
      "precision:  0.918269230769\n",
      "f1:         0.920481927711\n",
      "accuracy:   0.97947761194\n",
      "sentences:  207\n",
      "\n",
      "code:       4\n",
      "recall:     0.941176470588\n",
      "precision:  0.914285714286\n",
      "f1:         0.927536231884\n",
      "accuracy:   0.996890547264\n",
      "sentences:  34\n",
      "\n",
      "code:       5\n",
      "recall:     0.923076923077\n",
      "precision:  0.821917808219\n",
      "f1:         0.869565217391\n",
      "accuracy:   0.988805970149\n",
      "sentences:  65\n",
      "\n",
      "code:       50\n",
      "recall:     0.973640856672\n",
      "precision:  0.942583732057\n",
      "f1:         0.957860615883\n",
      "accuracy:   0.967661691542\n",
      "sentences:  607\n",
      "\n",
      "code:       5b\n",
      "recall:     0.428571428571\n",
      "precision:  1.0\n",
      "f1:         0.6\n",
      "accuracy:   0.997512437811\n",
      "sentences:  7\n",
      "\n",
      "code:       6\n",
      "recall:     0.826086956522\n",
      "precision:  1.0\n",
      "f1:         0.904761904762\n",
      "accuracy:   0.997512437811\n",
      "sentences:  23\n",
      "\n",
      "code:       7\n",
      "recall:     0.816513761468\n",
      "precision:  0.855769230769\n",
      "f1:         0.835680751174\n",
      "accuracy:   0.978233830846\n",
      "sentences:  109\n",
      "\n",
      "Training Data: \n",
      "Recall: 0.9225, Precision: 0.9252, F1: 0.9217, Accuracy: 0.9781, Codes:  1367\n"
     ]
    }
   ],
   "source": [
    "c2tagger = train([CODE_ALL_NUMERIC])\n",
    "tagger = c2tagger[CODE_ALL_NUMERIC]\n",
    "\n",
    "td_metrics = []\n",
    "for c in sorted(numeric_codes):\n",
    "    td_metrics.append(test_for_numeric_codes(c, ix_train, tagger))\n",
    "\n",
    "td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
    "print \"Training Data: \"\n",
    "print td_wt_mn_prfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code:       1\n",
      "recall:     0.903614457831\n",
      "precision:  0.688073394495\n",
      "f1:         0.78125\n",
      "accuracy:   0.921641791045\n",
      "sentences:  83\n",
      "\n",
      "code:       11\n",
      "recall:     0.615384615385\n",
      "precision:  1.0\n",
      "f1:         0.761904761905\n",
      "accuracy:   0.990671641791\n",
      "sentences:  13\n",
      "\n",
      "code:       12\n",
      "recall:     0.454545454545\n",
      "precision:  1.0\n",
      "f1:         0.625\n",
      "accuracy:   0.988805970149\n",
      "sentences:  11\n",
      "\n",
      "code:       13\n",
      "recall:     0.470588235294\n",
      "precision:  0.727272727273\n",
      "f1:         0.571428571429\n",
      "accuracy:   0.977611940299\n",
      "sentences:  17\n",
      "\n",
      "code:       14\n",
      "recall:     0.454545454545\n",
      "precision:  0.714285714286\n",
      "f1:         0.555555555556\n",
      "accuracy:   0.985074626866\n",
      "sentences:  11\n",
      "\n",
      "code:       2\n",
      "recall:     0.5\n",
      "precision:  0.95\n",
      "f1:         0.655172413793\n",
      "accuracy:   0.962686567164\n",
      "sentences:  38\n",
      "\n",
      "code:       3\n",
      "recall:     0.721739130435\n",
      "precision:  0.741071428571\n",
      "f1:         0.73127753304\n",
      "accuracy:   0.886194029851\n",
      "sentences:  115\n",
      "\n",
      "code:       4\n",
      "recall:     0.75\n",
      "precision:  1.0\n",
      "f1:         0.857142857143\n",
      "accuracy:   0.990671641791\n",
      "sentences:  20\n",
      "\n",
      "code:       5\n",
      "recall:     0.704545454545\n",
      "precision:  0.756097560976\n",
      "f1:         0.729411764706\n",
      "accuracy:   0.957089552239\n",
      "sentences:  44\n",
      "\n",
      "code:       50\n",
      "recall:     0.979166666667\n",
      "precision:  0.971074380165\n",
      "f1:         0.97510373444\n",
      "accuracy:   0.977611940299\n",
      "sentences:  240\n",
      "\n",
      "code:       5b\n",
      "recall:     0.0\n",
      "precision:  0.0\n",
      "f1:         0.0\n",
      "accuracy:   0.97947761194\n",
      "sentences:  11\n",
      "\n",
      "code:       6\n",
      "recall:     0.8\n",
      "precision:  0.666666666667\n",
      "f1:         0.727272727273\n",
      "accuracy:   0.994402985075\n",
      "sentences:  5\n",
      "\n",
      "code:       7\n",
      "recall:     0.7\n",
      "precision:  0.777777777778\n",
      "f1:         0.736842105263\n",
      "accuracy:   0.962686567164\n",
      "sentences:  40\n",
      "\n",
      "Validation Data: \n",
      "Recall: 0.7963, Precision: 0.8386, F1: 0.8068, Accuracy: 0.9522, Codes:   648\n"
     ]
    }
   ],
   "source": [
    "tagger = c2tagger[CODE_ALL_NUMERIC]\n",
    "\n",
    "vd_metrics = []\n",
    "for c in sorted(numeric_codes):\n",
    "    vd_metrics.append(test_for_numeric_codes(c, ix_valid, tagger))\n",
    "\n",
    "vd_wt_mn_prfa = weighted_mean_rpfa(vd_metrics)\n",
    "print \"Validation Data: \"\n",
    "print vd_wt_mn_prfa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CRF - All Numeric Codes, Stem = False, MIN_SENT_FREQ = 1, PCT_VALIDATION = 0.25, WINDOW_SIZE = 5:\n",
    "Validation Data: \n",
    "Recall: 0.7778, Precision: 0.8423, F1: 0.7943, Accuracy: 0.9519, Codes:   648\n",
    "\n",
    "CRF - All Numeric Codes, Stem = True, MIN_SENT_FREQ = 1, PCT_VALIDATION = 0.25, WINDOW_SIZE = 5:\n",
    "Validation Data: \n",
    "Recall: 0.7901, Precision: 0.8453, F1: 0.8016, Accuracy: 0.9516, Codes:   648\n",
    "\n",
    "CRF - All Numeric Codes, Stem = True, MIN_SENT_FREQ = 2, PCT_VALIDATION = 0.25, WINDOW_SIZE = 5:\n",
    "Validation Data: \n",
    "Recall: 0.7917, Precision: 0.8424, F1: 0.8008, Accuracy: 0.9512, Codes:   648\n",
    "\n",
    "CRF - All Numeric Codes, Stem = True, MIN_SENT_FREQ = 1, PCT_VALIDATION = 0.25, WINDOW_SIZE = 3:\n",
    "Validation Data: \n",
    "Recall: 0.7639, Precision: 0.8194, F1: 0.7742, Accuracy: 0.9493, Codes:   648\n",
    "\n",
    "CRF - All Numeric Codes, Stem = True, MIN_SENT_FREQ = 1, PCT_VALIDATION = 0.25, WINDOW_SIZE = 7:\n",
    "Validation Data: \n",
    "Recall: 0.7963, Precision: 0.8386, F1: 0.8068, Accuracy: 0.9522, Codes:   648"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
