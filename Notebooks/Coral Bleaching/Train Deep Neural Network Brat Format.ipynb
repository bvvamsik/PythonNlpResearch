{
 "metadata": {
  "name": "",
  "signature": "sha256:8e36b2ff70719e2276dcd7686b293a5a7235d454e4fbda2bac1a31b29351b73f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classifer on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser\n",
      "\n",
      "from DeepNeuralNetwork import MLP\n",
      "from MLPLayers import ConvolutionalLayer, Layer\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\"\n",
      "\n",
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 7\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 5\n",
      "PCT_VALIDATION  = 0.2\n",
      "MIN_FEAT_FREQ = 5     #15 best so far\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\"\n",
      "STEM = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            elif w == \"&\":\n",
      "                cw = \"and\"\n",
      "            else:\n",
      "                cw = corrector.correct(w)\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "                sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "297 files found\n",
        "297 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "cor_srtd = sort_by_value(corrections, reverse = True)\n",
      "cor_srtd[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "[((\"it's\", 'it'), 52),\n",
        " (('zox', 'zo'), 41),\n",
        " ((\"don't\", 'dont'), 31),\n",
        " ((\"that's\", 'that'), 29),\n",
        " (('algea', 'algae'), 26),\n",
        " ((\"world's\", 'world'), 20),\n",
        " (('&', 'and'), 17),\n",
        " ((\"can't\", 'cant'), 14),\n",
        " (('bleaches', 'bleached'), 13),\n",
        " (('cloral', 'coral'), 11),\n",
        " ((\"they're\", 'there'), 11),\n",
        " ((\"coral's\", 'coral'), 11),\n",
        " ((\"isn't\", 'isnt'), 11),\n",
        " (('tempeture', 'temperature'), 9),\n",
        " ((\"won't\", 'wont'), 9),\n",
        " (('alge', 'algae'), 9),\n",
        " (('tiems', 'times'), 8),\n",
        " ((\"doesn't\", 'doesnt'), 8),\n",
        " (('tempature', 'temperature'), 8),\n",
        " (('varys', 'vary'), 7)]"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Single char words \"\"\"\n",
      "wds = [(w,f) for w,f in wd_sent_freq.items() if len(w.strip()) == 1 and not w[0].isalpha()]\n",
      "print \"\\n\".join(map(str,sorted(wds, key = lambda (w,f): -f)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('.', 2551)\n",
        "(',', 675)\n",
        "('/', 118)\n",
        "('-', 104)\n",
        "('\"', 100)\n",
        "('?', 67)\n",
        "('(', 43)\n",
        "(')', 42)\n",
        "('3', 40)\n",
        "('%', 36)\n",
        "('\\xc2', 27)\n",
        "('\\xb0', 27)\n",
        "('\\x80', 17)\n",
        "('\\xe2', 17)\n",
        "('1', 17)\n",
        "('\\\\', 16)\n",
        "('5', 15)\n",
        "(';', 12)\n",
        "(':', 9)\n",
        "('\\x99', 8)\n",
        "('+', 7)\n",
        "('2', 7)\n",
        "('\\x93', 7)\n",
        "(\"'\", 6)\n",
        "('0', 4)\n",
        "('6', 4)\n",
        "('!', 4)\n",
        "('8', 2)\n",
        "('9', 2)\n",
        "('\\xa6', 2)\n",
        "('=', 2)\n",
        "('7', 1)\n",
        "('4', 1)\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "VALID_CHARS = {\".\", \"?\", \"!\", \"=\", \"/\", \":\", \";\", \"&\", \"+\",  \"-\", \"=\",  \"%\", \"'\", \",\", \"\\\\\", \"(\", \")\", \"\\\"\"}\n",
      "\"\"\" Remove bad chars (see above - e.g. '\\x93') \"\"\"\n",
      "removed = set()\n",
      "def valid_wd(wd):\n",
      "    wd = wd.strip()\n",
      "    if len(wd) != 1:\n",
      "        return True\n",
      "    if wd in removed:\n",
      "        return False\n",
      "    if wd.isalpha() or wd.isdigit() or wd in VALID_CHARS:\n",
      "        return True\n",
      "    removed.add(wd)\n",
      "    return False\n",
      "    \n",
      "def filterout_punctuation(sentence):\n",
      "    return filter(lambda (w, tags4word): valid_wd(w), sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "sentences = []\n",
      "tokenized_sentences = []\n",
      "\n",
      "i = 0\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        modified_sentence = filterout_punctuation(modified_sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size=WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        # tagged words\n",
      "        sentences.append(sentence)\n",
      "        # words only\n",
      "        tokenized_sentences.append(zip(*sentence)[0])\n",
      "        \n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        i += 1\n",
      "        \n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ix2windows[0][0]\n",
      "ix2sents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "[('<START>', set()),\n",
        " ('<START>', set()),\n",
        " ('<START>', set()),\n",
        " ('the', set()),\n",
        " ('coral', {'50'}),\n",
        " ('bleaching', {'50'}),\n",
        " ('is', set()),\n",
        " ('a', set()),\n",
        " ('different', set()),\n",
        " ('type', set()),\n",
        " ('they', set()),\n",
        " ('are', set()),\n",
        " ('bleached', set()),\n",
        " ('and', set()),\n",
        " ('coral', {'50'}),\n",
        " ('bleaching', {'50'}),\n",
        " ('.', set()),\n",
        " ('<END>', set()),\n",
        " ('<END>', set()),\n",
        " ('<END>', set())]"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Removed Characters\n",
      "------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(removed))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Map Words to One Hot Vectors\n",
      "----------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IdGenerator import IdGenerator as IdGen\n",
      "from Decorators import memoize\n",
      "\n",
      "wdgenerator = IdGen()\n",
      "\"\"\" Get all unique words remaining \"\"\"\n",
      "unique_words = set()\n",
      "unique_labels = set()\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "@memoize\n",
      "def stem(word):\n",
      "    return stemmer.stem(word)\n",
      "\n",
      "for i, sentence in ix2sents.items():\n",
      "    for wd, lbls in sentence:\n",
      "        if STEM:\n",
      "            wd = stem(wd)\n",
      "        unique_wds.add(wd)\n",
      "        for lbl in lbls:\n",
      "            unique_labels.add(lbl)\n",
      "\n",
      "\"\"\" map to vector form \"\"\"\n",
      "wd2vector = {}\n",
      "vec_len = len(unique_wds)\n",
      "for wd in unique_wds:\n",
      "    wid = wdgenerator.get_id(wd)\n",
      "    vector = np.zeros(vec_len)\n",
      "    vector[wid] = 1\n",
      "    wd2vector[wd] = vector\n",
      "    \n",
      "print \"Unique Words:\", len(unique_wds), \"Unique Labels:\", len(unique_labels)\n",
      "# min_freq (5), no stem, words = 627\n",
      "# min_freq (5), stem,    words = 515"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unique Words: 555 Unique Labels: 20\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lblgenerator = IdGen()\n",
      "def window2vector(window, code):\n",
      "    xs = []\n",
      "    for wd, lbls in window:\n",
      "        if STEM:\n",
      "            wd = stem(wd)\n",
      "        xs.append(wd2vector[wd])\n",
      "    \n",
      "    np_xs = np.hstack(xs)\n",
      "    lbls = window[MID_IX][1]\n",
      "    if code in lbls:\n",
      "        ys =  [1]\n",
      "    else:\n",
      "        ys =  [-1]\n",
      "    return ((np_xs * 2) -1, ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs, ys = [], []\n",
      "code = \"50\"\n",
      "for i, windows in ix2windows.items():\n",
      "    for window in windows:\n",
      "        x, y = window2vector(window, code)\n",
      "        xs.append(x)\n",
      "        ys.append(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "VECTOR_SIZE = 50\n",
      "TD = int(len(xs) * 0.65)\n",
      "\n",
      "hidden_in  = VECTOR_SIZE * WINDOW_SIZE \n",
      "#hidden_out = (hidden_in + len(ys[0])) / 2\n",
      "hidden_out = 10\n",
      "\n",
      "sup_layers = [\n",
      "        #Single Layer\n",
      "        #Layer(             len(xs[0]),  1,  activation_fn = \"tanh\", momentum=0.5),        \n",
      "\n",
      "        ConvolutionalLayer(len(xs[0]),  hidden_in,  activation_fn = \"tanh\", momentum=0.5, convolutions=WINDOW_SIZE),\n",
      "        Layer(             hidden_in,   len(ys[0]), activation_fn = \"tanh\", momentum=0.5),  \n",
      "        \n",
      "        #Layer(             hidden_out,  len(ys[0]), activation_fn = \"tanh\", momentum=0.5),  \n",
      "        #Layer(             10,              10, activation_fn = \"tanh\", momentum=0.5),\n",
      "        #Layer(             10,      len(ys[0]), activation_fn = \"tanh\", momentum=0.5),\n",
      "    ]\n",
      "\n",
      "# With low freq classes and convolutional layers, use a batch_size == 1 and NO momentum in that layer\n",
      "nn_sup = MLP(sup_layers, learning_rate=0.004, weight_decay=0.0, epochs=100, batch_size=1,\n",
      "             lr_increase_multiplier=1.1, lr_decrease_multiplier=0.4)\n",
      "\n",
      "nn_sup.fit(xs[:TD], ys[:TD], epochs=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MSE for epoch 0 is 0.12858772 \tMAE for epoch 0 is 0.14748952 \tlearning rate is 0.004\n",
        "MSE for epoch 1 is 0.08250978"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 1 is 0.08161955 \tlearning rate is 0.004\n",
        "MSE for epoch 2 is 0.07959152"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 2 is 0.07659472 \tlearning rate is 0.0044\n",
        "MSE for epoch 3 is 0.07671596"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 3 is 0.07577924 \tlearning rate is 0.00484\n",
        "MSE for epoch 4 is 0.07487904"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 4 is 0.07257349 \tlearning rate is 0.005324\n",
        "MSE for epoch 5 is 0.07328537"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 5 is 0.06704783 \tlearning rate is 0.0058564\n",
        "MSE for epoch 6 is 0.07384117"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 6 is 0.06605606 \tlearning rate is 0.00644204\n",
        "MSE for epoch 7 is 0.07528054"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 7 is 0.06394695 \tlearning rate is 0.007086244\n",
        "MSE for epoch 8 is 0.07655025"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 8 is 0.05984562 \tlearning rate is 0.0077948684\n",
        "MSE for epoch 9 is 0.08376975"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 9 is 0.06027743 \tlearning rate is 0.00857435524\n",
        "MSE for epoch 10 is 0.06805825"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 10 is 0.05090064 \tlearning rate is 0.003429742096\n",
        "MSE for epoch 11 is 0.06734922"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 11 is 0.05046286 \tlearning rate is 0.0037727163056\n",
        "MSE for epoch 12 is 0.06715476"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 12 is 0.05112468 \tlearning rate is 0.00414998793616\n",
        "MSE for epoch 13 is 0.06400103"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 13 is 0.04907818 \tlearning rate is 0.00165999517446\n",
        "MSE for epoch 14 is 0.06334136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 14 is 0.04981538 \tlearning rate is 0.00182599469191\n",
        "MSE for epoch 15 is 0.06224093"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 15 is 0.05061902 \tlearning rate is 0.000730397876764\n",
        "MSE for epoch 16 is 0.06180761"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 16 is 0.05002617 \tlearning rate is 0.000292159150706\n",
        "MSE for epoch 17 is 0.06167384"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 17 is 0.05067528 \tlearning rate is 0.000321375065776\n",
        "MSE for epoch 18 is 0.06141109"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 18 is 0.05169625 \tlearning rate is 0.00012855002631\n",
        "MSE for epoch 19 is 0.06134956"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 19 is 0.0518857 \tlearning rate is 5.14200105242e-05\n",
        "MSE for epoch 20 is 0.06130942"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 20 is 0.05180422 \tlearning rate is 2.05680042097e-05\n",
        "MSE for epoch 21 is 0.06127835"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 21 is 0.05164058 \tlearning rate is 2.26248046306e-05\n",
        "MSE for epoch 22 is 0.06125822"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 22 is 0.05150878 \tlearning rate is 2.48872850937e-05\n",
        "MSE for epoch 23 is 0.06124695"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 23 is 0.05139469 \tlearning rate is 2.73760136031e-05\n",
        "MSE for epoch 24 is 0.06122997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 24 is 0.05135148 \tlearning rate is 3.01136149634e-05\n",
        "MSE for epoch 25 is 0.06121247"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 25 is 0.05138558 \tlearning rate is 3.31249764597e-05\n",
        "MSE for epoch 26 is 0.06118368"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 26 is 0.05130778 \tlearning rate is 1.32499905839e-05\n",
        "MSE for epoch 27 is 0.06117653"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 27 is 0.05129014 \tlearning rate is 1.45749896423e-05\n",
        "MSE for epoch 28 is 0.06116877"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 28 is 0.05143422 \tlearning rate is 1.60324886065e-05\n",
        "MSE for epoch 29 is 0.06115473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 29 is 0.05152874 \tlearning rate is 6.4129954426e-06\n",
        "MSE for epoch 30 is 0.06114756"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 30 is 0.05156937 \tlearning rate is 2.56519817704e-06\n",
        "MSE for epoch 31 is 0.06114474"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 31 is 0.05154635 \tlearning rate is 1.02607927082e-06\n",
        "MSE for epoch 32 is 0.06114425"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 32 is 0.05156008 \tlearning rate is 1.1286871979e-06\n",
        "MSE for epoch 33 is 0.06114299"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 33 is 0.05155392 \tlearning rate is 4.51474879159e-07\n",
        "MSE for epoch 34 is 0.06114277"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 34 is 0.05155726 \tlearning rate is 4.96622367075e-07\n",
        "MSE for epoch 35 is 0.06114221"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 35 is 0.05155394 \tlearning rate is 1.9864894683e-07\n",
        "MSE for epoch 36 is 0.06114212"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 36 is 0.05155433 \tlearning rate is 2.18513841513e-07\n",
        "MSE for epoch 37 is 0.06114187"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 37 is 0.05155375 \tlearning rate is 8.74055366052e-08\n",
        "MSE for epoch 38 is 0.06114183"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 38 is 0.05155442 \tlearning rate is 9.61460902658e-08\n",
        "MSE for epoch 39 is 0.06114172"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 39 is 0.05155418 \tlearning rate is 3.84584361063e-08\n",
        "MSE for epoch 40 is 0.06114171"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 40 is 0.051554 \tlearning rate is 4.23042797169e-08\n",
        "MSE for epoch 41 is 0.06114169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 41 is 0.05155399 \tlearning rate is 4.65347076886e-08\n",
        "MSE for epoch 42 is 0.06114166"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 42 is 0.05155399 \tlearning rate is 5.11881784575e-08\n",
        "MSE for epoch 43 is 0.06114161"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 43 is 0.0515539 \tlearning rate is 2.0475271383e-08\n",
        "MSE for epoch 44 is 0.0611416"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 44 is 0.0515541 \tlearning rate is 2.25227985213e-08\n",
        "MSE for epoch 45 is 0.06114157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 45 is 0.05155386 \tlearning rate is 9.00911940852e-09\n",
        "MSE for epoch 46 is 0.06114157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 46 is 0.05155382 \tlearning rate is 9.91003134937e-09\n",
        "MSE for epoch 47 is 0.06114156"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 47 is 0.05155386 \tlearning rate is 1.09010344843e-08\n",
        "MSE for epoch 48 is 0.06114155"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 48 is 0.05155382 \tlearning rate is 4.36041379372e-09\n",
        "MSE for epoch 49 is 0.06114155"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 49 is 0.0515538 \tlearning rate is 4.7964551731e-09\n",
        "MSE for epoch 50 is 0.06114155"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 50 is 0.05155385 \tlearning rate is 5.2761006904e-09\n",
        "MSE for epoch 51 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 51 is 0.05155383 \tlearning rate is 2.11044027616e-09\n",
        "MSE for epoch 52 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 52 is 0.05155383 \tlearning rate is 2.32148430378e-09\n",
        "MSE for epoch 53 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 53 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 54 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 54 is 0.05155382 \tlearning rate is 1e-09\n",
        "MSE for epoch 55 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 55 is 0.05155385 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 56 is 0.06114154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 56 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 57 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 57 is 0.05155383 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 58 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 58 is 0.05155385 \tlearning rate is 1.21e-09\n",
        "MSE for epoch 59 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 59 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 60 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 60 is 0.05155384 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 61 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 61 is 0.05155385 \tlearning rate is 1.21e-09\n",
        "MSE for epoch 62 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 62 is 0.05155383 \tlearning rate is 1e-09\n",
        "MSE for epoch 63 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 63 is 0.05155384 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 64 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 64 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 65 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 65 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 66 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 66 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 67 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 67 is 0.05155383 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 68 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 68 is 0.05155384 \tlearning rate is 1.21e-09\n",
        "MSE for epoch 69 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 69 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 70 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 70 is 0.05155384 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 71 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 71 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 72 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 72 is 0.05155384 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 73 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 73 is 0.05155383 \tlearning rate is 1.21e-09\n",
        "MSE for epoch 74 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 74 is 0.05155385 \tlearning rate is 1.331e-09\n",
        "MSE for epoch 75 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 75 is 0.05155383 \tlearning rate is 1e-09\n",
        "MSE for epoch 76 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 76 is 0.05155385 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 77 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 77 is 0.05155384 \tlearning rate is 1e-09\n",
        "MSE for epoch 78 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 78 is 0.05155383 \tlearning rate is 1.1e-09\n",
        "MSE for epoch 79 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 79 is 0.05155383 \tlearning rate is 1.21e-09\n",
        "MSE for epoch 80 is 0.06114153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \tMAE for epoch 80 is 0.05155382 \tlearning rate is 1e-09\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-96-aa948c893af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m              lr_increase_multiplier=1.1, lr_decrease_multiplier=0.4)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mnn_sup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/simon.hughes/GitHub/NlpResearch/DeepLearning/MyCode/DeepNeuralNetwork.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, xs, ys, min_error, epochs, batch_size)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 mini_batch_errors, gradients = self.__compute_gradient__(mini_batch_in, mini_batch_out,\n\u001b[0m\u001b[1;32m    117\u001b[0m                                                                          self.layers, self.learning_rate)\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/GitHub/NlpResearch/DeepLearning/MyCode/DeepNeuralNetwork.py\u001b[0m in \u001b[0;36m__compute_gradient__\u001b[0;34m(self, input_vectors, outputs, layers, learning_rate, input_masks)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# TODO Sparsity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mwtdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiasdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwtdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbiasdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/simon.hughes/GitHub/NlpResearch/DeepLearning/MyCode/MLPLayers.pyc\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(self, delta, inputs)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mcon_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0min_rows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0min_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mwtg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon_inputs_T\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0min_rows\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0min_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cross Validation Score\n",
      "----------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Metrics import rpf1a\n",
      "from ResultsHelper import rfp\n",
      "\n",
      "def mae(ys, pr):\n",
      "    errors = []\n",
      "    for i,y in enumerate(ys):\n",
      "        errors.append(abs(y - pr[i]))\n",
      "    return np.mean(errors)\n",
      "\n",
      "def evaluate_nn(nn, xs, ys):\n",
      "    pred = nn.predict(xs)\n",
      "    m_ys = map(lambda arr: float(arr[0]), ys)\n",
      "    min_y = min(m_ys)\n",
      "    cls_pred = map(lambda arr: 1 if arr[0] >= 0.0 else min_y, pred)\n",
      "    print \"MAE:\", mae(m_ys, pred), \"MAE Constant:\", mae(m_ys, np.zeros((len(ys),1)) + min_y)\n",
      "    print \"\"\n",
      "    print rfp(m_ys, cls_pred)\n",
      "\n",
      "def print_prediction_distribution(nn, xs):\n",
      "    pred = nn.predict(xs)\n",
      "    d = defaultdict(int)\n",
      "    for p in pred:\n",
      "        d[round(p[0], 1)] += 1\n",
      "    print \"\\n\".join(map(str,sorted(d.items())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Supervised only: Training Accuracy\"\n",
      "evaluate_nn(nn_sup, xs[:TD], ys[:TD])\n",
      "\n",
      "print \"Supervised only: Validation Accuracy\"\n",
      "evaluate_nn(nn_sup, xs[TD:], ys[TD:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_prediction_distribution(nn_sup, xs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train a Multi Class Classifier\n",
      "------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lblgenerator = IdGen()\n",
      "# for multi class classification\n",
      "def window2multiclassvector(window):\n",
      "    xs = []\n",
      "    for wd, lbls in window:\n",
      "        if STEM:\n",
      "            wd = stem(wd)\n",
      "        xs.append(wd2vector[wd])\n",
      "    \n",
      "    np_xs = np.hstack(xs)\n",
      "    lbls = window[MID_IX][1]\n",
      "    y = np.zeros((len(unique_labels)))\n",
      "    for lbl in lbls:\n",
      "        lblid = lblgenerator.get_id(lbl)\n",
      "        y[lblid] = 1\n",
      "    return ((np_xs * 2) -1, (y * 2) -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs, mcys = [], []\n",
      "for i, windows in ix2windows.items():\n",
      "    for window in windows:\n",
      "        x, y = window2multiclassvector(window)\n",
      "        xs.append(x)\n",
      "        mcys.append(y)\n",
      "\n",
      "xs   = np.asarray(xs)\n",
      "mcys = np.asarray(mcys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "VECTOR_SIZE = 10\n",
      "TD = int(len(xs) * 0.80)\n",
      "\n",
      "hidden_in  = VECTOR_SIZE * WINDOW_SIZE \n",
      "#hidden_out = (hidden_in + len(ys[0])) / 2\n",
      "\n",
      "mclass_layers = [\n",
      "        #Relu is better\n",
      "        ConvolutionalLayer(len(xs[0]),  hidden_in,    activation_fn = \"tanh\", momentum=0.5, convolutions=WINDOW_SIZE),\n",
      "        Layer(             hidden_in,   len(mcys[0]), activation_fn = \"tanh\", momentum=0.5)  \n",
      "]\n",
      "\n",
      "# With low freq classes and convolutional layers, use a batch_size == 1 and NO momentum in that layer\n",
      "nn_mclass = MLP(mclass_layers, learning_rate=0.01, weight_decay=0.0, epochs=100, batch_size=1,\n",
      "                   lr_increase_multiplier=1.2, lr_decrease_multiplier=0.4)\n",
      "\n",
      "nn_mclass.fit(xs[:TD], mcys[:TD], epochs=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "argmaxes = set()\n",
      "p = nn_mclass.predict(xs)\n",
      "for row in p:\n",
      "    #print str(np.max(row)).rjust(20), np.max(row) > 0, np.argmax(row)\n",
      "    for i,val in enumerate(row):\n",
      "        if val >= -0.0:\n",
      "            argmaxes.add(i)\n",
      "argmaxes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train a Neural Network Language Model\n",
      "-------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def window2lmvectors(window):\n",
      "    xs = []\n",
      "    for wd, lbls in window[:-1]:\n",
      "        if STEM:\n",
      "            wd = stem(wd)\n",
      "        xs.append(wd2vector[wd])\n",
      "    np_xs = np.hstack(xs)\n",
      "    return (np_xs, wd2vector[window[-1][0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs2, ys2 = [], []\n",
      "for i, windows in ix2windows.items():\n",
      "    for window in windows:\n",
      "        x, y = window2lmvectors(window)\n",
      "        xs2.append(x)\n",
      "        ys2.append(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.layers[0].weights.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "VECTOR_SIZE = 50\n",
      "TD = int(len(xs2) * 0.65)\n",
      "\n",
      "hidden_in  = VECTOR_SIZE * (WINDOW_SIZE -1)\n",
      "#hidden_out = (hidden_in + len(ys[0])) / 2\n",
      "convolutions = len(xs2[0]) / len(wd2vector) #WINDOW_SIZE - 1\n",
      "\n",
      "lmlayers = [\n",
      "        ConvolutionalLayer(len(xs2[0]),  hidden_in,   activation_fn = \"tanh\",    momentum=0.0, convolutions=convolutions),\n",
      "        Layer(              hidden_in,   len(ys2[0]), activation_fn = \"softmax\", momentum=0.5),        \n",
      "    ]\n",
      "\n",
      "# With low freq classes and convolutional layers, use a batch_size == 1 and NO momentum in that layer\n",
      "lmnn = MLP(lmlayers, learning_rate=0.1, weight_decay=0.0, epochs=100, batch_size=5,\n",
      "             lr_increase_multiplier=1.2, lr_decrease_multiplier=0.6)\n",
      "\n",
      "lmnn.fit(xs2[:TD], ys2[:TD], epochs=150)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, windows in ix2windows.items()[-100:]:\n",
      "    for win in windows:\n",
      "        s = \"\"\n",
      "        for wd, lbl in win[:-1]:\n",
      "            s += wd + \" \"\n",
      "        x,y = window2lmvectors(win)\n",
      "        p = lmnn.predict([x])\n",
      "        ixpred = p.argmax()\n",
      "        wd = wdgenerator.get_key(ixpred)\n",
      "\n",
      "        print s.rjust(60), \":\", win[-1][0].ljust(10), \":\", wd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use Language Model to Initialize Supervised Convolutional Network\n",
      "-----------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wts  = lmlayers[0].weights.copy()\n",
      "bias = lmlayers[0].bias.copy()\n",
      "\n",
      "VECTOR_SIZE = 50\n",
      "TD = int(len(xs) * 0.65)\n",
      "\n",
      "hidden_in  = VECTOR_SIZE * WINDOW_SIZE \n",
      "#hidden_out = (hidden_in + len(ys[0])) / 2\n",
      "hidden_out = 10\n",
      "\n",
      "semisup_layers = [\n",
      "        ConvolutionalLayer(len(xs[0]),  hidden_in,  activation_fn = \"tanh\", momentum=0.0, convolutions=WINDOW_SIZE, \n",
      "                           weights = wts, bias=bias),\n",
      "        Layer(             hidden_in,   len(ys[0]), activation_fn = \"tanh\", momentum=0.5),  \n",
      "    ]\n",
      "\n",
      "# With low freq classes and convolutional layers, use a batch_size == 1 and NO momentum in that layer\n",
      "semisup_nn = MLP(semisup_layers, learning_rate=0.004, weight_decay=0.0, epochs=100, batch_size=1,\n",
      "             lr_increase_multiplier=1.2, lr_decrease_multiplier=0.6)\n",
      "\n",
      "semisup_nn.fit(xs[:TD], ys[:TD], epochs=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Semi-Supervised: Training Accuracy\"\n",
      "evaluate_nn(semisup_nn, xs[:TD], ys[:TD])\n",
      "\n",
      "print \"Semi-Supervised: Validation Accuracy\"\n",
      "evaluate_nn(semisup_nn, xs[TD:], ys[TD:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lmnn1 - first network - relu units\n",
      "\n",
      "VECTOR_SIZE = 50\n",
      "TD = int(len(xs) * 0.65)\n",
      "\n",
      "hidden_in  = VECTOR_SIZE * WINDOW_SIZE \n",
      "\n",
      "projection_layers = [\n",
      "        ConvolutionalLayer(len(xs[0]),  hidden_in,  activation_fn = \"tanh\", momentum=0.0, convolutions=WINDOW_SIZE, \n",
      "                           weights = semisup_layers[0].weights.copy(), bias=semisup_layers[0].bias.copy())\n",
      "]\n",
      "projection_nn = MLP(projection_layers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs_proj = projection_nn.predict(xs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "ys_flat = map(lambda a: a[0], ys)\n",
      "\n",
      "cls = LinearSVC(C=1.0)\n",
      "cls.fit(xs_proj[:TD], ys_flat[:TD])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tdpred = cls.predict(xs_proj[:TD])\n",
      "vdpred = cls.predict(xs_proj[TD:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"TD\"\n",
      "print rfp(ys_flat[:TD], tdpred)\n",
      "\n",
      "print \"VD\"\n",
      "print rfp(ys_flat[TD:], vdpred)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TD = int(len(xs) * 0.65)\n",
      "ys_flat = map(lambda a: a[0], ys)\n",
      "\n",
      "cls2 = LinearSVC(C=1.0)\n",
      "cls2.fit(xs[:TD], ys_flat[:TD])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ResultsHelper import rfp\n",
      "\n",
      "tdpred2 = cls2.predict(xs[:TD])\n",
      "vdpred2 = cls2.predict(xs[TD:])\n",
      "\n",
      "print \"TD\"\n",
      "print rfp(ys_flat[:TD], tdpred2)\n",
      "\n",
      "print \"VD\"\n",
      "print rfp(ys_flat[TD:], vdpred2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(ys_flat[TD:]), len(vdpred2), len(xs), len(xs[TD:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "len(ys_flat), len(xs), len(ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}