{
 "metadata": {
  "name": "",
  "signature": "sha256:8a2bb27cf4976da36620db527c446eec67aa49295dbf1e2729199ac33ea0b15a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "import pylibfm \n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\"\n",
      "\n",
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 7 #7 is best\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 2\n",
      "PCT_VALIDATION  = 0.2\n",
      "MIN_FEAT_FREQ = 5     #15 best so far\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\"\n",
      "STEM = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/jpype/_pykeywords.py:18: DeprecationWarning: the sets module is deprecated\n",
        "  import sets\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            elif w == \"&\":\n",
      "                cw = \"and\"\n",
      "            else:\n",
      "                cw = corrector.correct(w)\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "                sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "297 files found\n",
        "297 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "cor_srtd = sort_by_value(corrections, reverse = True)\n",
      "cor_srtd[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[((\"it's\", 'it'), 52),\n",
        " (('zox', 'zo'), 41),\n",
        " ((\"don't\", 'dont'), 31),\n",
        " ((\"that's\", 'that'), 29),\n",
        " (('algea', 'algae'), 26),\n",
        " ((\"world's\", 'world'), 20),\n",
        " (('&', 'and'), 17),\n",
        " ((\"can't\", 'cant'), 14),\n",
        " (('bleaches', 'bleached'), 13),\n",
        " (('cloral', 'coral'), 11),\n",
        " ((\"they're\", 'there'), 11),\n",
        " ((\"coral's\", 'coral'), 11),\n",
        " ((\"isn't\", 'isnt'), 11),\n",
        " (('tempeture', 'temperature'), 9),\n",
        " ((\"won't\", 'wont'), 9),\n",
        " (('alge', 'algae'), 9),\n",
        " (('tiems', 'times'), 8),\n",
        " ((\"doesn't\", 'doesnt'), 8),\n",
        " (('tempature', 'temperature'), 8),\n",
        " (('varys', 'vary'), 7)]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Single char words \"\"\"\n",
      "wds = [(w,f) for w,f in wd_sent_freq.items() if len(w.strip()) == 1 and not w[0].isalpha()]\n",
      "print \"\\n\".join(map(str,sorted(wds, key = lambda (w,f): -f)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('.', 2623)\n",
        "(',', 681)\n",
        "('-', 105)\n",
        "('\"', 105)\n",
        "('?', 90)\n",
        "('(', 43)\n",
        "(')', 42)\n",
        "('3', 40)\n",
        "('%', 36)\n",
        "('\\xc2', 28)\n",
        "('\\xb0', 28)\n",
        "('\\x80', 17)\n",
        "('\\xe2', 17)\n",
        "('1', 17)\n",
        "('\\\\', 16)\n",
        "('5', 15)\n",
        "(';', 12)\n",
        "(':', 9)\n",
        "('\\x99', 8)\n",
        "('+', 7)\n",
        "('2', 7)\n",
        "('!', 7)\n",
        "('\\x93', 7)\n",
        "(\"'\", 6)\n",
        "('0', 4)\n",
        "('6', 4)\n",
        "('8', 2)\n",
        "('9', 2)\n",
        "('\\xa6', 2)\n",
        "('=', 2)\n",
        "('7', 1)\n",
        "('4', 1)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "VALID_CHARS = {\".\", \"?\", \"!\", \"=\", \"/\", \":\", \";\", \"&\", \"+\",  \"-\", \"=\",  \"%\", \"'\", \",\", \"\\\\\", \"(\", \")\", \"\\\"\"}\n",
      "\"\"\" Remove bad chars (see above - e.g. '\\x93') \"\"\"\n",
      "removed = set()\n",
      "def valid_wd(wd):\n",
      "    wd = wd.strip()\n",
      "    if len(wd) != 1:\n",
      "        return True\n",
      "    if wd in removed:\n",
      "        return False\n",
      "    if wd.isalpha() or wd.isdigit() or wd in VALID_CHARS:\n",
      "        return True\n",
      "    removed.add(wd)\n",
      "    return False\n",
      "    \n",
      "def filterout_punctuation(sentence):\n",
      "    return filter(lambda (w, tags4word): valid_wd(w), sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "# maps sentence index to essay\n",
      "ix2essays = {}\n",
      "sentences = []\n",
      "tokenized_sentences = []\n",
      "\n",
      "i = 0\n",
      "for e_ix, essay in enumerate(essays):\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        modified_sentence = filterout_punctuation(modified_sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size= WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        # tagged words\n",
      "        sentences.append(sentence)\n",
      "        # words only\n",
      "        tokenized_sentences.append(zip(*sentence)[0])\n",
      "        \n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        ix2essays[i] = e_ix\n",
      "        i += 1\n",
      "        \n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentence Level Features\n",
      "-----------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from gensim import matutils\n",
      "\n",
      "def filter_words(wd):\n",
      "    return wd.isalnum()\n",
      "\n",
      "docs = map(lambda sen : \" \".join(filter(filter_words,sen)),tokenized_sentences)\n",
      "\n",
      "#Vectorize\n",
      "vectorizer = TfidfVectorizer(use_idf = False, ngram_range = (1, 1), min_df = 5, binary=True)\n",
      "sentence_vectors = vectorizer.fit_transform(docs)\n",
      "sentence_vectors = sentence_vectors.todense()\n",
      "sentence_vectors = map(lambda s: s.tolist()[0], sentence_vectors)\n",
      "ix2vector = dict(enumerate(sentence_vectors))\n",
      "print len(ix2vector[0]), \"features\"\n",
      "#ix2vector[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "662 features\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Removed Characters\n",
      "------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(removed))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract Features \"\"\"\n",
      "from WindowFeatures import extract_positional_word_features, extract_word_features\n",
      "from NgramGenerator import compute_ngrams\n",
      "\n",
      "def extract_skip_b4_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[:mid_ix]:\n",
      "        feats[\"_BEFORE: \" + wd + \"|\" + target] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_skip_after_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[mid_ix+1:]:\n",
      "        feats[\"_AFTER: \" + target + \"|\" + wd] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_positional_bigram_features(window, mid_ix, feature_val = 1):\n",
      "    bi_grams = compute_ngrams(window, max_len = 2, min_len = 2)\n",
      "    d = {}\n",
      "    for i, bi_gram in enumerate(bi_grams):\n",
      "        d[\"BI\" + \":\" + str(-mid_ix + i) + \" \" + bi_gram[0] + \" | \" + bi_gram[1]] = feature_val\n",
      "    return d\n",
      "\n",
      "def extract_positional_skip_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for i, wd in enumerate(window):\n",
      "        if i == mid_ix:\n",
      "            continue\n",
      "        a,b = wd,target\n",
      "        if i > mid_ix:\n",
      "            a,b = b,a\n",
      "        feats[\"SKIP:\" + str(-mid_ix + i) + \" \" + a + \" | \" + b] = feature_val\n",
      "    return feats\n",
      "\n",
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "idgen = IdGenerator()\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def extract_features(words):\n",
      "    \n",
      "    if STEM:\n",
      "        words = [stemmer.stem(w) for w in words]\n",
      "    #Extract features for words\n",
      "    \n",
      "    \"\"\" Try only middle word \"\"\"\n",
      "    features = {}\n",
      "    ###\n",
      "    pos_features = extract_positional_word_features(words, MID_IX, feature_val=1)    \n",
      "    word_features  = extract_word_features(words, feature_val=1)\n",
      "    \n",
      "    #DO NOT HELP\n",
      "    #b4_features    = extract_skip_b4_word_features(words, MID_IX, feature_val=1)\n",
      "    #after_features = extract_skip_after_word_features(words, MID_IX, feature_val=1)\n",
      "    #pos_skip_grams = extract_positional_skip_word_features(words, MID_IX,  feature_val = 1)\n",
      "    pos_bi_grams = extract_positional_bigram_features(words, MID_IX, feature_val = 1)\n",
      "\n",
      "    features.update(pos_features)\n",
      "    features.update(word_features)\n",
      "    #features.update(b4_features)\n",
      "    #features.update(after_features)\n",
      "    #features.update(pos_skip_grams)\n",
      "    features.update(pos_bi_grams)\n",
      "    return features.items()\n",
      "\n",
      "def extract_ys_by_code(tags, ysByCode):\n",
      "    for code in all_codes:\n",
      "        ysByCode[code].append(1 if code in tags else 0 )    \n",
      "\n",
      "ix2ys = {}\n",
      "ix2feats = {}\n",
      "feat_counts = defaultdict(int)\n",
      "def tally_features(feats):\n",
      "    for k,v in feats:\n",
      "        feat_counts[k] += 1\n",
      "\n",
      "for i, windows in ix2windows.items():\n",
      "    feats = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    \n",
      "    ix2feats[i] = feats\n",
      "    ix2ys[i] = ysByCode\n",
      "    for window in windows:\n",
      "        # Get the words minus tags\n",
      "        words, tags = zip(*window)                \n",
      "        feat = extract_features(words)\n",
      "        tally_features(feat)\n",
      "        feats.append(feat)\n",
      "        \n",
      "        #Tags for middle word (target)\n",
      "        tags4word = tags[MID_IX]\n",
      "        extract_ys_by_code(tags4word, ysByCode)\n",
      "    assert len(windows) == len(feats)\n",
      "    assert all(map(lambda (k,v): len(v) == len(feats), ysByCode.items()))\n",
      "        \n",
      "\"\"\" Convert sparse dictionary features to sparse arrays \"\"\"\n",
      "ix2xs = {}\n",
      "for i, feature_lists in ix2feats.items():\n",
      "    xs = []\n",
      "    ix2xs[i] = xs\n",
      "    for feats in feature_lists:\n",
      "        x = [(idgen.get_id(f),v) \n",
      "                 for f,v in feats \n",
      "                 if feat_counts[f] >= MIN_FEAT_FREQ or f.startswith(\"WD:0\" )]\n",
      "        xs.append(x)        \n",
      "\n",
      "num_features = idgen.max_id() + 1\n",
      "print \"Number of features:\", num_features\n",
      "\n",
      "\"\"\" Convert to dense numpy arrays \"\"\"\n",
      "for i in ix2xs.keys():\n",
      "    xs = ix2xs[i]\n",
      "    xs = np.array([matutils.sparse2full(x, num_features) for x in xs])        \n",
      "    ix2xs[i] = xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 14183\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "\n",
      "def count_above(ft_counts, threshold):\n",
      "    above = [ v for k,v in ft_counts.items() if v >= threshold]\n",
      "    return (sum(above), len(above))\n",
      "\n",
      "total_all, cnt_all = count_above(feat_counts, 0)\n",
      "total_above, cnt_above = count_above(feat_counts, MIN_FEAT_FREQ)\n",
      "\n",
      "print \"Counts\"\n",
      "print \"all:     \", cnt_all\n",
      "print \"above:   \", cnt_above\n",
      "print \"% above: \", str(100.0 * cnt_above / float(cnt_all))+ \"%\"\n",
      "\n",
      "print \"\\nTotal Frequency\"\n",
      "print \"all:     \", total_all\n",
      "print \"above:   \", total_above\n",
      "print \"% above: \", str(100.0 * total_above / float(total_all))+ \"%\"\n",
      "\n",
      "#srtd = sort_by_value(feat_counts, reverse = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counts\n",
        "all:      68809\n",
        "above:    13879\n",
        "% above:  20.1703265561%\n",
        "\n",
        "Total Frequency\n",
        "all:      846975\n",
        "above:    764055\n",
        "% above:  90.2098645178%\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_window(win):\n",
      "    def set2str(st):\n",
      "        return \"{\" + str(t)[5:-2] + \"}\"\n",
      "    \n",
      "    w, tg = zip(*win)\n",
      "    lens = [max(len(wd),len(set2str(t))) for wd,t in win]\n",
      "    \n",
      "    for i, wd in enumerate(w):\n",
      "        print wd.ljust(lens[i]) , \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "    for i, t in enumerate(tg):\n",
      "        print set2str(t).ljust(lens[i]), \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "def extract_features(window, feat_vals):\n",
      "    feats = [idgen.get_key(i) for i,val in enumerate(feat_vals) if val]\n",
      "    \n",
      "    wd_feats = []\n",
      "    for win in window:\n",
      "        wd, tgs = win\n",
      "        if STEM:\n",
      "            match = filter(lambda feat: \" \" + stemmer.stem(wd) + \" \" in \" \" + feat + \" \", feats)\n",
      "        else:\n",
      "            match = filter(lambda feat: \" \" + wd + \" \" in \" \" + feat + \" \", feats)\n",
      "        wd_feats.append((wd, match))\n",
      "    return wd_feats\n",
      "\n",
      "def print_features(wf):\n",
      "    w_f = wf\n",
      "    for w,ft in w_f:\n",
      "        print w.ljust(10), map(lambda s:s.ljust(10), sorted(ft, key=lambda s:(len(s),s)))\n",
      "    print \"\"\n",
      "\n",
      "#uncomment to verify code output\n",
      "\n",
      "sentence_no = 101\n",
      "print \"Tagged Windows\"\n",
      "for win in ix2windows[sentence_no][:5]:\n",
      "    print_window(win)\n",
      "print \"\"    \n",
      "\n",
      "print \"Features\"\n",
      "def prn_sent_features(sentence_num):\n",
      "    win = ix2windows[sentence_num]\n",
      "    for i in range(len(win)):\n",
      "        print \"[%s]\" % str(i)\n",
      "        wf = extract_features(win[i], ix2xs[sentence_num][i])\n",
      "        print_features(wf)\n",
      "\n",
      "prn_sent_features(sentence_no)\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tagged Windows\n",
        "<START> | <START> | <START> | so | they | need | to | \n",
        "{}      | {}      | {}      | {} | {}   | {}   | {} | \n",
        "<START> | <START> | so | they | need | to | have | \n",
        "{}      | {}      | {} | {}   | {}   | {} | {}   | \n",
        "<START> | so | they | need | to | have | light | \n",
        "{}      | {} | {}   | {}   | {} | {}   | {}    | \n",
        "so | they | need | to | have | light | to | \n",
        "{} | {}   | {}   | {} | {}   | {}    | {} | \n",
        "they | need | to | have | light | to | eat | \n",
        "{}   | {}   | {} | {}   | {}    | {} | {}  | \n",
        "\n",
        "Features\n",
        "[0]\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "so         [u'so        ', u'WD:0 so   ', u'BI:0 so | they', u'BI:-1 <START> | so']\n",
        "they       [u'they      ', u'WD:1 they ', u'BI:0 so | they', u'BI:1 they | need']\n",
        "need       [u'need      ', u'WD:2 need ', u'BI:2 need | to', u'BI:1 they | need']\n",
        "to         [u'to        ', u'WD:3 to   ', u'BI:2 need | to']\n",
        "\n",
        "[1]\n",
        "<START>    [u'<START>   ', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-2 <START> | so', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-2 <START> | so', u'BI:-3 <START> | <START>']\n",
        "so         [u'so        ', u'WD:-1 so  ', u'BI:-1 so | they', u'BI:-2 <START> | so']\n",
        "they       [u'they      ', u'WD:0 they ', u'BI:-1 so | they', u'BI:0 they | need']\n",
        "need       [u'need      ', u'WD:1 need ', u'BI:1 need | to', u'BI:0 they | need']\n",
        "to         [u'to        ', u'WD:2 to   ', u'BI:1 need | to', u'BI:2 to | have']\n",
        "have       [u'have      ', u'WD:3 have ', u'BI:2 to | have']\n",
        "\n",
        "[2]\n",
        "<START>    [u'<START>   ', u'WD:-3 <START>', u'BI:-3 <START> | so']\n",
        "so         [u'so        ', u'WD:-2 so  ', u'BI:-2 so | they', u'BI:-3 <START> | so']\n",
        "they       [u'they      ', u'WD:-1 they', u'BI:-2 so | they', u'BI:-1 they | need']\n",
        "need       [u'need      ', u'WD:0 need ', u'BI:0 need | to', u'BI:-1 they | need']\n",
        "to         [u'to        ', u'WD:1 to   ', u'BI:0 need | to', u'BI:1 to | have']\n",
        "have       [u'have      ', u'WD:2 have ', u'BI:1 to | have']\n",
        "light      [u'light     ', u'WD:3 light']\n",
        "\n",
        "[3]\n",
        "so         [u'so        ', u'WD:-3 so  ', u'BI:-3 so | they']\n",
        "they       [u'they      ', u'WD:-2 they', u'BI:-3 so | they', u'BI:-2 they | need']\n",
        "need       [u'need      ', u'WD:-1 need', u'BI:-1 need | to', u'BI:-2 they | need']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:3 to   ', u'BI:0 to | have', u'BI:-1 need | to', u'BI:2 light | to']\n",
        "have       [u'have      ', u'WD:1 have ', u'BI:0 to | have']\n",
        "light      [u'light     ', u'WD:2 light', u'BI:2 light | to']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:3 to   ', u'BI:0 to | have', u'BI:-1 need | to', u'BI:2 light | to']\n",
        "\n",
        "[4]\n",
        "they       [u'they      ', u'WD:-3 they', u'BI:-3 they | need']\n",
        "need       [u'need      ', u'WD:-2 need', u'BI:-2 need | to', u'BI:-3 they | need']\n",
        "to         [u'to        ', u'WD:2 to   ', u'WD:-1 to  ', u'BI:2 to | eat', u'BI:-1 to | have', u'BI:-2 need | to', u'BI:1 light | to']\n",
        "have       [u'have      ', u'WD:0 have ', u'BI:-1 to | have']\n",
        "light      [u'light     ', u'WD:1 light', u'BI:1 light | to']\n",
        "to         [u'to        ', u'WD:2 to   ', u'WD:-1 to  ', u'BI:2 to | eat', u'BI:-1 to | have', u'BI:-2 need | to', u'BI:1 light | to']\n",
        "eat        [u'eat       ', u'WD:3 eat  ', u'BI:2 to | eat']\n",
        "\n",
        "[5]\n",
        "need       [u'need      ', u'WD:-3 need', u'BI:-3 need | to']\n",
        "to         [u'to        ', u'WD:1 to   ', u'WD:-2 to  ', u'BI:1 to | eat', u'BI:-2 to | have', u'BI:-3 need | to', u'BI:0 light | to']\n",
        "have       [u'have      ', u'WD:-1 have', u'BI:-2 to | have']\n",
        "light      [u'light     ', u'WD:0 light', u'BI:0 light | to']\n",
        "to         [u'to        ', u'WD:1 to   ', u'WD:-2 to  ', u'BI:1 to | eat', u'BI:-2 to | have', u'BI:-3 need | to', u'BI:0 light | to']\n",
        "eat        [u'eat       ', u'WD:2 eat  ', u'BI:2 eat | .', u'BI:1 to | eat']\n",
        ".          [u'.         ', u'WD:3 .    ', u'BI:2 eat | .']\n",
        "\n",
        "[6]\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:-3 to  ', u'BI:0 to | eat', u'BI:-3 to | have', u'BI:-1 light | to']\n",
        "have       [u'have      ', u'WD:-2 have', u'BI:-3 to | have']\n",
        "light      [u'light     ', u'WD:-1 light', u'BI:-1 light | to']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:-3 to  ', u'BI:0 to | eat', u'BI:-3 to | have', u'BI:-1 light | to']\n",
        "eat        [u'eat       ', u'WD:1 eat  ', u'BI:1 eat | .', u'BI:0 to | eat']\n",
        ".          [u'.         ', u'WD:2 .    ', u'BI:1 eat | .', u'BI:2 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:3 <END>', u'BI:2 . | <END>']\n",
        "\n",
        "[7]\n",
        "have       [u'have      ', u'WD:-3 have']\n",
        "light      [u'light     ', u'WD:-2 light', u'BI:-2 light | to']\n",
        "to         [u'to        ', u'WD:-1 to  ', u'BI:-1 to | eat', u'BI:-2 light | to']\n",
        "eat        [u'eat       ', u'WD:0 eat  ', u'BI:0 eat | .', u'BI:-1 to | eat']\n",
        ".          [u'.         ', u'WD:1 .    ', u'BI:0 eat | .', u'BI:1 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:2 <END>', u'WD:3 <END>', u'BI:1 . | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:2 <END>', u'WD:3 <END>', u'BI:1 . | <END>', u'BI:2 <END> | <END>']\n",
        "\n",
        "[8]\n",
        "light      [u'light     ', u'WD:-3 light', u'BI:-3 light | to']\n",
        "to         [u'to        ', u'WD:-2 to  ', u'BI:-2 to | eat', u'BI:-3 light | to']\n",
        "eat        [u'eat       ', u'WD:-1 eat ', u'BI:-1 eat | .', u'BI:-2 to | eat']\n",
        ".          [u'.         ', u'WD:0 .    ', u'BI:-1 eat | .', u'BI:0 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split the Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_xs_ys(ixs, ixTOxs, ixTOys, codes):\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    for i in ixs:\n",
      "        xs_tmp = ixTOxs[i]\n",
      "        xs.extend(xs_tmp)\n",
      "        ysByCode_tmp = ixTOys[i]\n",
      "        for code in codes:\n",
      "            ysByCode[code].extend(ysByCode_tmp[code])\n",
      "    return (np.array(xs), ysByCode)\n",
      "\n",
      "num_train = int(len(sentences) * (1.0 - PCT_VALIDATION))\n",
      "\n",
      "ixtest  = ix2sents.keys()[:num_train]\n",
      "ixvalid = ix2sents.keys()[num_train:]\n",
      "\n",
      "# Extract flattened windows for training data as xs and ys\n",
      "x_t, yByCode_t = extract_xs_ys(ixtest,ix2xs, ix2ys, all_codes)\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"#Sentences : \" + str(len(sentences))\n",
      "print \"\"\n",
      "\n",
      "all_codes = sorted(all_codes, key= lambda s :(len(s), s))\n",
      "for code in all_codes:\n",
      "    print code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#Sentences : 2779\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "11\n",
        "12\n",
        "13\n",
        "14\n",
        "50\n",
        "5b\n",
        "it\n",
        "other\n",
        "Causer\n",
        "Result\n",
        "Anaphor\n",
        "explicit\n",
        "rhetorical\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train\n",
      "====="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class factMach(object):\n",
      "    def __init__(self):\n",
      "        self.fm = pylibfm.FM(num_factors=50, num_iter=10, task=\"regression\", shuffle_training=True)\n",
      "        \n",
      "    def __to_sparse_(self, xs):\n",
      "        from scipy import sparse\n",
      "        return sparse.csr_matrix(np.asarray(xs, dtype=np.double))\n",
      "    \n",
      "    def fit(self, xs, ys):\n",
      "        return self.fm.fit(self.__to_sparse_(xs), np.asarray(ys, dtype=np.double))\n",
      "    \n",
      "    def predict(self, xs):\n",
      "        return np.round(self.fm.predict(self.__to_sparse_(xs)))\n",
      "        #return self.fm.predict(self.__to_sparse_(xs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TRAIN \"\"\"\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "map_svm = lambda y: -1 if y < 0 else 1\n",
      "map_reg = lambda y: y\n",
      "\n",
      "#map_y = map_svm\n",
      "map_y = map_reg\n",
      "\n",
      "def make_cls():\n",
      "    #cls = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, criterion=\"entropy\")\n",
      "    #cls = DecisionTreeClassifier(criterion=\"entropy\")\n",
      "    #cls = LogisticRegression()\n",
      "    #cls = RidgeClassifier()\n",
      "    #cls = KNeighborsClassifier(n_neighbors=5) # TOO SLOW!\n",
      "    #cls = LDA()\n",
      "    #cls = SVC()\n",
      "    #cls = RandomForestClassifier(n_jobs=-1, max_depth=100, n_estimators=10)\n",
      "    #cls = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, max_depth=1)\n",
      "    #cls = Ridge()\n",
      "    cls = LinearSVC()\n",
      "    #cls = factMach()\n",
      "    return cls\n",
      "\n",
      "print \"Starting Training\"\n",
      "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "def train(codes, xs, yByCode, fn_create_cls):\n",
      "    code2classifier = {}\n",
      "    for code in codes:\n",
      "        print \"Training for :\", code   \n",
      "        cls = fn_create_cls()\n",
      "        code2classifier[code] = cls\n",
      "        ys = np.asarray(yByCode[code])    \n",
      "        ys = map(map_y, ys)\n",
      "        cls.fit(xs, ys)\n",
      "    return code2classifier\n",
      "\n",
      "code2cls = train(all_codes, x_t, yByCode_t, make_cls)\n",
      "print make_cls()\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Training\n",
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classify\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Get sentence level classification performance \"\"\"\n",
      "def test_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    cls = codeToClassifier[code]\n",
      "    \n",
      "    try:\n",
      "        cls.n_jobs = 1\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    act_ys  = []\n",
      "    pred_ys = []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys.append(max(ys))\n",
      "        pred_ys.append(max(pred))\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    print \"code:      \", code\n",
      "    print \"recall:    \", r\n",
      "    print \"precision: \", p\n",
      "    print \"f1:        \", f1\n",
      "    print \"accuracy:  \", a\n",
      "    print \"sentences: \", num_codes\n",
      "    print \"\"\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "print \"\"\n",
      "print \"total sent:\", len(ixvalid)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "total sent: 695\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training Data Performance\n",
      "-------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(codes, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    td_metrics = []\n",
      "    for c in codes:\n",
      "        cls = codeToClassifier[c]\n",
      "        td_metrics.append(test_for_code(c, ixs, ixToXs, ixToYs, codeToClassifier))\n",
      "    td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
      "    print type(cls), td_wt_mn_prfa\n",
      "    return td_wt_mn_prfa\n",
      "\n",
      "print \"Training Data: \"\n",
      "metrics = test(all_codes, ixtest, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.995515695067\n",
        "precision:  0.991071428571\n",
        "f1:         0.993288590604\n",
        "accuracy:   0.998560460653\n",
        "sentences:  223\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.884615384615\n",
        "precision:  0.901960784314\n",
        "f1:         0.893203883495\n",
        "accuracy:   0.99472168906\n",
        "sentences:  52\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.992957746479\n",
        "precision:  0.972413793103\n",
        "f1:         0.982578397213\n",
        "accuracy:   0.995201535509\n",
        "sentences:  284\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  48\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.99\n",
        "precision:  0.916666666667\n",
        "f1:         0.951923076923\n",
        "accuracy:   0.995201535509\n",
        "sentences:  100\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  27\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     1.0\n",
        "precision:  0.992307692308\n",
        "f1:         0.996138996139\n",
        "accuracy:   0.999520153551\n",
        "sentences:  129\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  51\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  24\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.968253968254\n",
        "precision:  0.983870967742\n",
        "f1:         0.976\n",
        "accuracy:   0.998560460653\n",
        "sentences:  63\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     1.0\n",
        "precision:  0.846153846154\n",
        "f1:         0.916666666667\n",
        "accuracy:   0.998080614203\n",
        "sentences:  22\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.993573264781\n",
        "precision:  0.991025641026\n",
        "f1:         0.992297817715\n",
        "accuracy:   0.99424184261\n",
        "sentences:  778\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "recall:     0.916666666667\n",
        "precision:  1.0\n",
        "f1:         0.95652173913\n",
        "accuracy:   0.999520153551\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  1\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "recall:     0.824175824176\n",
        "precision:  0.980392156863\n",
        "f1:         0.89552238806\n",
        "accuracy:   0.98320537428\n",
        "sentences:  182\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "recall:     0.96511627907\n",
        "precision:  0.936794582393\n",
        "f1:         0.950744558992\n",
        "accuracy:   0.979366602687\n",
        "sentences:  430\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "recall:     0.892938496583\n",
        "precision:  0.918032786885\n",
        "f1:         0.905311778291\n",
        "accuracy:   0.960652591171\n",
        "sentences:  439\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "recall:     0.96261682243\n",
        "precision:  1.0\n",
        "f1:         0.980952380952\n",
        "accuracy:   0.998080614203\n",
        "sentences:  107\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.975391498881\n",
        "precision:  0.979775280899\n",
        "f1:         0.977578475336\n",
        "accuracy:   0.990403071017\n",
        "sentences:  447\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "recall:     0.986111111111\n",
        "precision:  0.959459459459\n",
        "f1:         0.972602739726\n",
        "accuracy:   0.996161228407\n",
        "sentences:  144\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.9638, Precision: 0.9671, F1: 0.9649, Accuracy: 0.9884, Codes:  3563\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validation Data Performance\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data: \"\n",
      "test(reg_codes, ixvalid, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.924731182796\n",
        "precision:  0.774774774775\n",
        "f1:         0.843137254902\n",
        "accuracy:   0.953956834532\n",
        "sentences:  93\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.633333333333\n",
        "precision:  0.612903225806\n",
        "f1:         0.622950819672\n",
        "accuracy:   0.96690647482\n",
        "sentences:  30\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.862903225806\n",
        "precision:  0.640718562874\n",
        "f1:         0.735395189003\n",
        "accuracy:   0.889208633094\n",
        "sentences:  124\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.869565217391\n",
        "precision:  0.689655172414\n",
        "f1:         0.769230769231\n",
        "accuracy:   0.98273381295\n",
        "sentences:  23\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.795454545455\n",
        "precision:  0.555555555556\n",
        "f1:         0.654205607477\n",
        "accuracy:   0.946762589928\n",
        "sentences:  44\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  0.4\n",
        "f1:         0.571428571429\n",
        "accuracy:   0.987050359712\n",
        "sentences:  6\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.815384615385\n",
        "precision:  0.552083333333\n",
        "f1:         0.658385093168\n",
        "accuracy:   0.920863309353\n",
        "sentences:  65\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.8\n",
        "precision:  1.0\n",
        "f1:         0.888888888889\n",
        "accuracy:   0.995683453237\n",
        "sentences:  15\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.833333333333\n",
        "precision:  0.909090909091\n",
        "f1:         0.869565217391\n",
        "accuracy:   0.995683453237\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.923076923077\n",
        "precision:  0.727272727273\n",
        "f1:         0.813559322034\n",
        "accuracy:   0.984172661871\n",
        "sentences:  26\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     0.8\n",
        "precision:  0.75\n",
        "f1:         0.774193548387\n",
        "accuracy:   0.989928057554\n",
        "sentences:  15\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.978873239437\n",
        "precision:  0.902597402597\n",
        "f1:         0.939189189189\n",
        "accuracy:   0.948201438849\n",
        "sentences:  284\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.603658536585\n",
        "precision:  0.480582524272\n",
        "f1:         0.535135135135\n",
        "accuracy:   0.752517985612\n",
        "sentences:  164\n",
        "\n",
        "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.8446, Precision: 0.7100, F1: 0.7681, Accuracy: 0.9079, Codes:   901\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "Recall: 0.8446, Precision: 0.7100, F1: 0.7681, Accuracy: 0.9079, Codes:   901"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Window - 5, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LDA:      Recall: 0.8927, Precision: 0.6974, F1: 0.7675, Accuracy: 0.8823, Codes:   792\n",
      "LinSVC:   Recall: 0.7601, Precision: 0.7655, F1: 0.7563, Accuracy: 0.9048, Codes:   792\n",
      "DT:       Recall: 0.7462, Precision: 0.6890, F1: 0.7063, Accuracy: 0.8766, Codes:   792\n",
      "RidgeClf: Recall: 0.6843, Precision: 0.8359, F1: 0.6874, Accuracy: 0.9036, Codes:   795\n",
      "\n",
      "LinSVC\n",
      "Window - 7, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LinSVC: Recall: 0.7937, Precision: 0.7522, F1: 0.7677, Accuracy: 0.9037, Codes:   795\n",
      "\n",
      "Window - 9, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.7887, Precision: 0.7338, F1: 0.7555, Accuracy: 0.8929, Codes:   795\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.8058, Precision: 0.7559, F1: 0.7756, Accuracy: 0.9046, Codes:   798\n",
      "\n",
      "-- Starting adding new features, messing with feature freq\n",
      "Window - 5, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8008, Precision: 0.7369, F1: 0.7625, Accuracy: 0.9008, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8120, Precision: 0.7535, F1: 0.7779, Accuracy: 0.9066, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - ***15***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8145, Precision: 0.7460, F1: 0.7744, Accuracy: 0.9040, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 20\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7982, Precision: 0.7496, F1: 0.7685, Accuracy: 0.9025, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 25\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8070, Precision: 0.7485, F1: 0.7732, Accuracy: 0.9047, Codes:   798\n",
      "\n",
      "***\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 5 ***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "    + Positional BI-GRAMS ****\n",
      "Recall: 0.8145, Precision: 0.7642, F1: 0.7831, Accuracy: 0.9070, Codes:   798\n",
      "***\n",
      "\n",
      "Logistic Regression\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7306, Precision: 0.8298, F1: 0.7672, Accuracy: 0.9150, Codes:   798\n",
      "\n",
      "Window - 9, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7206, Precision: 0.8232, F1: 0.7580, Accuracy: 0.9121, Codes:   798\n",
      "\n",
      "RF\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit')\n",
      "Recall: 0.6028, Precision: 0.8071, F1: 0.6570, Accuracy: 0.8978, Codes:   798"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train Stacked Classifier\n",
      "========================"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def codes_2_features(set_codes):\n",
      "    return [1 if code in set_codes else 0 for code in all_codes]\n",
      "\n",
      "print codes_2_features({\"50\", \"5\"})\n",
      "print codes_2_features({})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Create Data Using Previous Classifier \"\"\"\n",
      "ix2newxs = {}\n",
      "ix2newys = {} #dict to dict to list\n",
      "\n",
      "CAUSAL_REL = \"CRel\"\n",
      "RESULT_REL = \"RRel\"\n",
      "CAUSE_RESULT = \"C->R\"\n",
      "\n",
      "cr_codes = [CAUSAL_REL, RESULT_REL, CAUSE_RESULT]\n",
      "tally = defaultdict(lambda: defaultdict(int))\n",
      "codes_per_row = []\n",
      "str_codes = []\n",
      "\n",
      "essay_ix = ix2essays[0]\n",
      "codes4essay = []\n",
      "\n",
      "# for each sentence\n",
      "for i,xs in ix2xs.items():\n",
      "    # i = sentence index\n",
      "    # xs = windows in sentence\n",
      "    if ix2essays[i] != essay_ix:\n",
      "        # if new essay, reset history of tags for essay\n",
      "        essay_ix = ix2essays[i]\n",
      "        codes4essay = []\n",
      "\n",
      "    # COMPUTE XS\n",
      "    tmp_xs = []\n",
      "    tmp_ys = []\n",
      "    tmp_ys_by_code = defaultdict(list)\n",
      "\n",
      "    # add BOW features\n",
      "    #tmp_xs.extend(ix2vector[i])\n",
      "    \n",
      "    un_codes = set() # correct labels - YS\n",
      "    un_pred_codes = set() # predicted ys\n",
      "    s_codes = \"|\"\n",
      "    for code in all_codes:\n",
      "        cls = code2cls[code]\n",
      "        pred = cls.decision_function(xs)\n",
      "        # add min and max values\n",
      "        mx = max(pred)\n",
      "        mn = min(pred)\n",
      "        diff = mx - mn\n",
      "        yes_no = max(cls.predict(xs))\n",
      "        \n",
      "        tmp_xs.append(mx)\n",
      "        tmp_xs.append(mn)\n",
      "        #tmp_xs.append(diff)\n",
      "        tmp_xs.append(yes_no)\n",
      "        \n",
      "        y_val = max(ix2ys[i][code])\n",
      "        tmp_ys_by_code[code] = np.array([y_val])\n",
      "        if y_val > 0:\n",
      "            un_codes.add(code)\n",
      "        \n",
      "        # full prediction\n",
      "        if yes_no > 0:\n",
      "            un_pred_codes.add(code)\n",
      "            s_codes += code + \"|\"\n",
      "    \n",
      "    # look at codes for previous sentence\n",
      "    # look back at most 2 items\n",
      "    look_back = min(2, len(codes4essay))\n",
      "    for jj in range(look_back):\n",
      "        fts = codes_2_features(codes4essay[-1-jj])\n",
      "        tmp_xs.extend(fts)\n",
      "\n",
      "    # if less than 2, fill with 0's\n",
      "    for jj in range(2-look_back):\n",
      "        tmp_xs.extend(codes_2_features(set()))\n",
      "    \n",
      "    #end for each code\n",
      "    codes_per_row.append(un_pred_codes)\n",
      "    str_codes.append(s_codes)\n",
      "    \n",
      "    #add 2 way feature combos\n",
      "    for a in all_codes:\n",
      "        for b in all_codes:\n",
      "            if b < a:\n",
      "                if a in un_pred_codes and b in un_pred_codes:\n",
      "                    tmp_xs.append(1)\n",
      "                else:\n",
      "                    tmp_xs.append(0)\n",
      "            #if (\"|%s|%s|\" %(a,b)) in s_codes:\n",
      "            #    tmp_xs.append(1)\n",
      "            #else:\n",
      "            #    tmp_xs.append(0)            \n",
      "    \n",
      "    if len(un_pred_codes) > 0:\n",
      "        codes4essay.append(un_pred_codes)\n",
      "    \n",
      "    # COMPUTE YS\n",
      "    tmp_ys_by_code[CAUSAL_REL]   = np.array([ 1 if  \"Causer\" in un_codes and \"explicit\" in un_codes else 0 ])\n",
      "    tmp_ys_by_code[RESULT_REL]   = np.array([ 1 if  \"Result\" in un_codes and \"explicit\" in un_codes else 0 ])\n",
      "    tmp_ys_by_code[CAUSE_RESULT] = np.array([ 1 if (\"Result\" in un_codes and \"explicit\" in un_codes and \"Causer\" in un_codes) else 0 ])\n",
      "    \n",
      "    for k,v in tmp_ys_by_code.items():\n",
      "        tally[k][max(v)] += 1\n",
      "        \n",
      "    ix2newxs[i] = np.array([tmp_xs])\n",
      "    ix2newys[i] = tmp_ys_by_code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals = ix2newxs.values()\n",
      "rlens = map(lambda a: a.shape[0], vals)\n",
      "clens = map(lambda a: a.shape[1], vals)\n",
      "assert min(clens) == max(clens), \"All xs' should be the same length\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Visualize how common multiple codes co-occur with C-R's \"\"\"\n",
      "for i,s in enumerate(str_codes):\n",
      "    print str(i).ljust(5), s\n",
      "    if i > 200:\n",
      "        break\n",
      "#print ix2newxs[15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0     |50|\n",
        "1     |50|\n",
        "2     |\n",
        "3     |\n",
        "4     |\n",
        "5     |1|\n",
        "6     |\n",
        "7     |\n",
        "8     |\n",
        "9     |\n",
        "10    |\n",
        "11    |\n",
        "12    |\n",
        "13    |\n",
        "14    |\n",
        "15    |7|50|Causer|Result|explicit|\n",
        "16    |50|\n",
        "17    |1|\n",
        "18    |2|\n",
        "19    |\n",
        "20    |\n",
        "21    |\n",
        "22    |\n",
        "23    |\n",
        "24    |\n",
        "25    |7|50|Causer|Result|explicit|\n",
        "26    |50|\n",
        "27    |7|50|Causer|Result|explicit|\n",
        "28    |50|\n",
        "29    |50|\n",
        "30    |7|50|Causer|Result|explicit|\n",
        "31    |\n",
        "32    |50|\n",
        "33    |3|rhetorical|\n",
        "34    |\n",
        "35    |\n",
        "36    |\n",
        "37    |6|50|Causer|Result|explicit|rhetorical|\n",
        "38    |\n",
        "39    |\n",
        "40    |\n",
        "41    |\n",
        "42    |\n",
        "43    |50|\n",
        "44    |\n",
        "45    |\n",
        "46    |50|\n",
        "47    |3|\n",
        "48    |1|50|Causer|Result|explicit|\n",
        "49    |1|50|Causer|Result|explicit|\n",
        "50    |1|3|50|Causer|Result|explicit|rhetorical|\n",
        "51    |3|\n",
        "52    |2|3|Causer|Result|explicit|\n",
        "53    |1|3|Causer|Result|explicit|\n",
        "54    |3|50|explicit|\n",
        "55    |50|Result|\n",
        "56    |\n",
        "57    |Causer|Result|Anaphor|explicit|rhetorical|\n",
        "58    |1|2|Anaphor|\n",
        "59    |explicit|\n",
        "60    |1|other|rhetorical|\n",
        "61    |3|explicit|rhetorical|\n",
        "62    |\n",
        "63    |1|50|\n",
        "64    |1|\n",
        "65    |\n",
        "66    |\n",
        "67    |3|50|Causer|Result|explicit|\n",
        "68    |50|\n",
        "69    |\n",
        "70    |50|\n",
        "71    |50|\n",
        "72    |50|\n",
        "73    |50|\n",
        "74    |50|\n",
        "75    |50|\n",
        "76    |50|\n",
        "77    |\n",
        "78    |50|\n",
        "79    |\n",
        "80    |\n",
        "81    |\n",
        "82    |\n",
        "83    |\n",
        "84    |7|50|Causer|explicit|\n",
        "85    |\n",
        "86    |7|50|other|Causer|Result|explicit|rhetorical|\n",
        "87    |\n",
        "88    |\n",
        "89    |\n",
        "90    |\n",
        "91    |50|\n",
        "92    |50|\n",
        "93    |\n",
        "94    |\n",
        "95    |\n",
        "96    |\n",
        "97    |\n",
        "98    |50|\n",
        "99    |\n",
        "100   |\n",
        "101   |\n",
        "102   |1|other|\n",
        "103   |50|Causer|Result|explicit|rhetorical|\n",
        "104   |\n",
        "105   |50|\n",
        "106   |7|50|Causer|Result|explicit|\n",
        "107   |\n",
        "108   |50|\n",
        "109   |\n",
        "110   |50|\n",
        "111   |\n",
        "112   |7|50|other|Causer|Result|explicit|\n",
        "113   |50|Causer|Result|explicit|rhetorical|\n",
        "114   |\n",
        "115   |\n",
        "116   |Result|rhetorical|\n",
        "117   |\n",
        "118   |3|4|Causer|Result|explicit|\n",
        "119   |7|50|Result|explicit|\n",
        "120   |3|50|Causer|Result|explicit|\n",
        "121   |1|\n",
        "122   |\n",
        "123   |\n",
        "124   |50|\n",
        "125   |50|\n",
        "126   |50|\n",
        "127   |7|50|Causer|Result|explicit|\n",
        "128   |7|50|Causer|Result|explicit|\n",
        "129   |\n",
        "130   |\n",
        "131   |11|\n",
        "132   |50|\n",
        "133   |50|\n",
        "134   |\n",
        "135   |\n",
        "136   |\n",
        "137   |\n",
        "138   |50|\n",
        "139   |\n",
        "140   |\n",
        "141   |\n",
        "142   |3|\n",
        "143   |\n",
        "144   |\n",
        "145   |50|\n",
        "146   |50|\n",
        "147   |\n",
        "148   |\n",
        "149   |50|\n",
        "150   |\n",
        "151   |50|Result|\n",
        "152   |1|Causer|Result|explicit|rhetorical|\n",
        "153   |3|\n",
        "154   |50|\n",
        "155   |1|3|\n",
        "156   |1|\n",
        "157   |3|\n",
        "158   |\n",
        "159   |4|50|other|Causer|Result|explicit|\n",
        "160   |4|50|Causer|Result|explicit|rhetorical|\n",
        "161   |50|\n",
        "162   |\n",
        "163   |\n",
        "164   |\n",
        "165   |\n",
        "166   |Anaphor|\n",
        "167   |\n",
        "168   |Anaphor|\n",
        "169   |Anaphor|\n",
        "170   |\n",
        "171   |3|50|Result|explicit|\n",
        "172   |\n",
        "173   |50|\n",
        "174   |50|\n",
        "175   |1|50|\n",
        "176   |50|\n",
        "177   |\n",
        "178   |1|50|Causer|Result|explicit|\n",
        "179   |\n",
        "180   |\n",
        "181   |\n",
        "182   |1|\n",
        "183   |\n",
        "184   |\n",
        "185   |50|Result|\n",
        "186   |\n",
        "187   |\n",
        "188   |50|\n",
        "189   |50|\n",
        "190   |50|\n",
        "191   |50|\n",
        "192   |50|\n",
        "193   |50|\n",
        "194   |50|\n",
        "195   |50|\n",
        "196   |50|\n",
        "197   |\n",
        "198   |\n",
        "199   |\n",
        "200   |\n",
        "201   |50|\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Search for good C values\n",
      "\n",
      "dct = {}\n",
      "codes = cr_codes + [\"explicit\"] #[CAUSE_RESULT]\n",
      "for c in [1.0, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5]:\n",
      "    print \"C\", c\n",
      "    new_code2cls = train(codes, newx_t, yByCode_t, lambda : LinearSVC(C = float(c)))\n",
      "    dct[c] = test(codes, ixvalid, ix2newxs, ix2newys, new_code2cls)\n",
      "\n",
      "print \"\"\n",
      "for k,v in sorted(dct.items()):\n",
      "    print \"C\", str(k).ljust(5), \"Metric:\",v\n",
      "    \n",
      "\"\"\"\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_plus_cr = all_codes + cr_codes\n",
      "newx_t, yByCode_t = extract_xs_ys(ixtest, ix2newxs, ix2newys, all_plus_cr)\n",
      "print newx_t[0].shape, \"features\"\n",
      "\n",
      "# SVM Is best when using joint features\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda: GradientBoostingClassifier(subsample=1.0))\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda: RandomForestClassifier())\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, factMach)\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, LDA)\n",
      "new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda : LinearSVC(C=1.8))\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, LogisticRegression)\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, DecisionTreeClassifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(290,) features\n",
        "Training for : CRel\n",
        "Training for : RRel\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training for : explicit\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test Data #2: \"\n",
      "metrics = test(cr_codes + [\"explicit\"], ixtest, ix2newxs, ix2newys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test Data #2: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CRel\n",
        "recall:     0.997641509434\n",
        "precision:  1.0\n",
        "f1:         0.998819362456\n",
        "accuracy:   0.999520153551\n",
        "sentences:  424\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RRel\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  426\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "recall:     0.995133819951\n",
        "precision:  1.0\n",
        "f1:         0.99756097561\n",
        "accuracy:   0.999040307102\n",
        "sentences:  411\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.997762863535\n",
        "precision:  0.997762863535\n",
        "f1:         0.997762863535\n",
        "accuracy:   0.999040307102\n",
        "sentences:  447\n",
        "\n",
        "<class 'sklearn.ensemble.forest.RandomForestClassifier'> Recall: 0.9977, Precision: 0.9994, F1: 0.9985, Accuracy: 0.9994, Codes:  1708\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data #2: \"\n",
      "metrics = test(cr_codes + [\"explicit\"], ixvalid, ix2newxs, ix2newys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data #2: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CRel\n",
        "recall:     0.677215189873\n",
        "precision:  0.578378378378\n",
        "f1:         0.623906705539\n",
        "accuracy:   0.814388489209\n",
        "sentences:  158\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RRel\n",
        "recall:     0.660377358491\n",
        "precision:  0.527638190955\n",
        "f1:         0.586592178771\n",
        "accuracy:   0.787050359712\n",
        "sentences:  159\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "recall:     0.592356687898\n",
        "precision:  0.596153846154\n",
        "f1:         0.594249201278\n",
        "accuracy:   0.81726618705\n",
        "sentences:  157\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.676829268293\n",
        "precision:  0.504545454545\n",
        "f1:         0.578125\n",
        "accuracy:   0.76690647482\n",
        "sentences:  164\n",
        "\n",
        "<class 'sklearn.ensemble.forest.RandomForestClassifier'> Recall: 0.6520, Precision: 0.5511, F1: 0.5955, Accuracy: 0.7961, Codes:   638\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Using Min\\Max Distance from Decision Plane\n",
      "  Single Features\n",
      "***\n",
      "<class 'LDA'> Recall: 0.6797, Precision: 0.6151, F1: 0.6458, Accuracy: 0.7784, Codes:   637\n",
      "***\n",
      "<class 'LinearSVC'> Recall: 0.6562, Precision: 0.6069, F1: 0.6297, Accuracy: 0.7704, Codes:   637\n",
      "<class 'logistic.LogisticRegression'> Recall: 0.6578, Precision: 0.6207, F1: 0.6379, Accuracy: 0.7784, Codes:   637\n",
      "<class 'GradientBoostingClassifier'> Recall: 0.6185, Precision: 0.6458, F1: 0.6288, Accuracy: 0.7835, Codes:   637\n",
      "<class 'DecisionTreeClassifier'> Recall: 0.5950, Precision: 0.6334, F1: 0.6118, Accuracy: 0.7755, Codes:   637\n",
      "    \n",
      "  Joint Features\n",
      "<class 'sklearn.lda.LDA'> Recall: 0.5667, Precision: 0.6260, F1: 0.5934, Accuracy: 0.7700, Codes:   637\n",
      "***\n",
      "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.6703, Precision: 0.6323, F1: 0.6502, Accuracy: 0.7853, Codes:   637\n",
      "***\n",
      "code:       C->R\n",
      "recall:     0.677215189873\n",
      "precision:  0.685897435897\n",
      "f1:         0.68152866242\n",
      "accuracy:   0.813432835821\n",
      "sentences:  158\n",
      "<class 'LogisticRegression'> Recall: 0.6342, Precision: 0.6539, F1: 0.6424, Accuracy: 0.7900, Codes:   637\n",
      "<class 'GradientBoostingClassifier'> Recall: 0.6075, Precision: 0.6396, F1: 0.6196, Accuracy: 0.7793, Codes:   637\n",
      "<class 'DecisionTreeClassifier'> Recall: 0.5651, Precision: 0.6236, F1: 0.5896, Accuracy: 0.7672, Codes:   637"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*** TODO *** - Peter\\Simon 5.14.2014\n",
      "Match Explicit + Cause or Result (or both)\n",
      "Try reading this: http://ceur-ws.org/Vol-1109/paper4.pdf\n",
      "Read Peter's paper\n",
      "\n",
      "1. <s>Try removing commas and '\"''s and other punctuation</s>\n",
      "2. <s>Try skip gram features (Peter)</s>\n",
      "3. Dependency parse \n",
      "    - span of words for the explicit and the concept\n",
      "    - find any depencencies tha join those two groups\n",
      "    - dependency type\n",
      "        - NSUBJ or PREP_TO or CONJ_AND\n",
      "        - OR advmod, conj_and, dobj, prep_of, prep_in\n",
      "        - OR acomp,  advmod\n",
      "4. <s>Read Peter's latest paper </s>\n",
      "5. Read related papers - Semeval 2007 (or close), Rink et al, Girju et al, etc\n",
      "5. <s>Try training a second classifier based on the output of the first including the predictions for the previous and next word</s>\n",
      "6. <s>Add in sentence level features, such as BOW</s>\n",
      "7. <s>Do some cross validation</s>\n",
      "8. **Add in predicted codes from previous \\ next sentence** - recent sentence parsing improvements have decreased F1 score. Suggests that this information helps\n",
      "9. Try blending multiple window based classifiers (Log R, SVM, LDA, DT) using a regression model\n",
      "\"\"\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hits_misses_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "\n",
      "    cls = codeToClassifier[code]    \n",
      "    try:\n",
      "        cls.n_jobs = 1\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    tp, tn, fp, fn = [], [], [], []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys  = round(max(ys))\n",
      "        pred_ys = round(max(pred))\n",
      "        \n",
      "        sent = ix2sents[ix]\n",
      "        if pred_ys == 1.0:\n",
      "            if act_ys == pred_ys:\n",
      "                tp.append(sent)\n",
      "            else:\n",
      "                fp.append(sent)\n",
      "        else: # negative prediction\n",
      "            if act_ys == pred_ys:\n",
      "                tn.append(sent)\n",
      "            else:\n",
      "                fn.append(sent)\n",
      "            \n",
      "    return (tp, tn, fp, fn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examine some of the Errors Made in Tagging the Test Set Sentences\n",
      "-----------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tp, tn, fp, fn = hits_misses_for_code(CAUSE_RESULT, ixvalid, ix2newxs, ix2newys, new_code2cls)\n",
      "tp, tn, fp, fn = hits_misses_for_code(\"5\", ixvalid, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentence_text(sentence):\n",
      "    wds, codes = zip(*sentence)\n",
      "    return \" \".join([wd for wd in wds if wd != SENTENCE_START and wd != SENTENCE_END])\n",
      "\n",
      "print \"tp\"\n",
      "for i, sentence in enumerate(tp[:5]):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\"\n",
      "\n",
      "print \"fn\"\n",
      "for i, sentence in enumerate(fn):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\"\n",
      "\n",
      "print \"fp\"\n",
      "for i, sentence in enumerate(fp):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tp\n",
        "0   coral and zooxanthellae algae have a very strong relationship with each other with that because in tropical oceans there are not enough nutrients to allow photosynthesis .\n",
        "1   coral live in shallow waters because they need light for photosynthesis for the zooxanthallae , when water temperature increases , the solubility or co2 in water decreases .\n",
        "2   as water temperature increases , the decrease in carbon dioxide creates a disruption in the process of photosynthesis\n",
        "3   when water temperatures increase coral can not do the process of photo synthesis . photo synthesis only in temperatures of 70 - 85f .\n",
        "4   the rates of coral bleaching vary from the amount light it recieve if a coral does not have enough light it will not be able to do photosynthesis .\n",
        "\n",
        "fn\n",
        "0   this disruption threatens the delicate balance of photosynthesis .\n",
        "1   coral reefs need to go through photosynthesis to help survive .\n",
        "2   corals need temperatures between 70 - 85f in order to go through photosynthesis .\n",
        "3   next , coral bleaching rates vary at different times because of photosynthesis for example during photosynthesis , zooxanthallae use energy from sunlight to combine carbon dioxide in the ocean with water .\n",
        "4   the zo algae helps the coral go through photosynthesis .\n",
        "5   if the water is not clear then the algae , zo live in the coral tissues and also need light for photosynthesis .\n",
        "6   this disruption threatens the delicate balance required to keep coral healthy . \"\n",
        "7   coral bleaching happens by changes in temperature or not having zooxanthallae for a long time .\n",
        "8   this disruption threatens the delicate balance required to keep corals healthy .\n",
        "\n",
        "fp\n",
        "0   the pass some of the food they make from the sun s energy to the coral .\n",
        "1   when the coral loses zooxanthellae the bleaching may result in death because it can t go as long with out the zooxanthellae .\n",
        "2   corals lose their color because of the ejection or death of the zooxanthellae algae , which passes food made by the sun energy to the coral .\n",
        "3   coral bleaching rates vary at different times of the year , because sometimes of the year they have enough zooxanthella which is algae .\n",
        "4   it is dependent on this because the zooxanthellae passes some of the food they make from the suns energy .\n",
        "5   in the reading coral % photosynthesis it tells us that coral reefs need water temperatures between 70 - 85f .\n",
        "6   this is partly because the process of photosynthesis is sensitive to changes in water temperature .\n",
        "7   the enviromental force can damage the nutrients for the coral . in result , the coral will starve and be vulnerable to disease in the state of bleaching and die off .\n",
        "8   since the decrease of carbon dioxide causes disruption in coral , the coral does not get enough nutrients to allow photosynthesis with zooxanthelle and when zooxanthelle is absent , it the coral from color and white , and eventually results in death .\n",
        "9   coral bleaching rates vary at different times because if the water temperatures are either too high or too low the photosynthesis rate also changes .\n",
        "10  corals receive 50 - 95 % of their energy from photosynthesis .\n",
        "11  the zooxanthallae passes some of the food they make from the suns energy to the coral .\n",
        "12  because of the high temperature change zooxantheallae isn able to work properly and give the coral the sugar and oxygen needed .\n",
        "13  i believe coral bleaching rates vary at different times because of the environment its in , photosynthesis and zooxanthallae .\n",
        "14  coral bleach rates vary at different times because of the environment that it is in for example corals mostly live in the ocean , feed off sugars made by algae that live among the corals polyps .\n",
        "15  during this process , carbon is passed from the algae to the coral in the form of glycerol or glucose .\n",
        "16  these chemicals provide the coral with the energy it needs to survive .\n",
        "17  in order for coral to keep it color , it needs to be in water that is 70 - 80f .\n",
        "18  it is stated that that \" coral feeds off of sugars made from algae living in side of them .\n",
        "19  there for coral bleaching is caused by the change in trade causing the algae stress and producing anchor kind of sugar that the coral is causing coral bleaching to happen .\n",
        "20  zo , according to the text , give \" some of the food they make from the sun energy to the coral \" , as well as giving corals their color .\n",
        "21  but , since the shift happened it changes the temperature and that up their photosynthesis process .\n",
        "22  another reason why the rates of coral bleaching may vary is because the coral is not receiving any energy in order to live .\n",
        "23  the coral turns white because the zo passed some of the food energy made from the sun to the corals .\n",
        "24  coral bleaching is when a coral loses its color completely and turns white .\n",
        "25  zooxanthellae affect the corals by increasing stress on the corals from the environment that cause the coral to eject their food producing algae which could result in coral bleaching .\n",
        "26  coral bleaching rates vary at different times , such as in 1998 , when the coral breaking was at its highest , wind currents or shifting winds , the ability to create photosynthesis , and zooxanthallae algae , all contribute to why and how coral bleaching varies .\n",
        "27  in the article it says \" as water temperature increases carbon dioxide decreases . \"\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Coral Bleaching Codelist\n",
      "\n",
      "1  \u2013 Decrease in trade winds\n",
      "2  \u2013 Warm water moving east\n",
      "3  \u2013 Increase in water temperature\n",
      "4  \u2013 Decrease in the solubility of CO2\n",
      "5  \u2013 Decrease/Disruption in the process of Photosynthesis\n",
      "6  \u2013 Coral stress\n",
      "7  \u2013 Ejection/Death of Algae\n",
      "11 \u2013 Storms/Rainfall\n",
      "12 \u2013 Increase in Freshwater\n",
      "13 \u2013 Decrease in Salinity\n",
      "50 \u2013 Coral Bleaching\n",
      "\n",
      "Causal Model\n",
      "1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> 50\n",
      "\n",
      "                      -> 6\n",
      "11 -> 12 -> 13"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "2778"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}