{
 "metadata": {
  "name": "",
  "signature": "sha256:8d7406d84ff288bbd014bf1f04bf95f08db11967e90caadf7cade0a31046ce70"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from Decorators import timeit, memoize, memoize_to_disk\n",
      "from BrattEssay import load_bratt_essays\n",
      "from processessays import process_sentences, process_essays\n",
      "from wordtagginghelper import flatten_to_wordlevel_feat_tags, get_wordlevel_ys_by_code\n",
      "\n",
      "from featureextractortransformer import FeatureExtractorTransformer\n",
      "from featurevectorizer import FeatureVectorizer\n",
      "from featureextractionfunctions import *\n",
      "from CrossValidation import cross_validation\n",
      "from wordtagginghelper import *\n",
      "from IterableFP import flatten\n",
      "\n",
      "# Classifiers\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "# END Classifiers\n",
      "\n",
      "import pickle\n",
      "import Settings\n",
      "import os\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "logger = logging.getLogger()\n",
      "\n",
      "MIN_SENTENCE_FREQ   = 2        # i.e. df. Note this is calculated BEFORE creating windows\n",
      "REMOVE_INFREQUENT   = False    # if false, infrequent words are replaced with \"INFREQUENT\"\n",
      "SPELLING_CORRECT    = True\n",
      "STEM                = True\n",
      "REPLACE_NUMS        = True     # 1989 -> 0000, 10 -> 00\n",
      "MIN_SENTENCE_LENGTH = 3\n",
      "REMOVE_STOP_WORDS   = False\n",
      "REMOVE_PUNCTUATION  = True\n",
      "LOWER_CASE          = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "settings = Settings.Settings()\n",
      "processed_essay_filename_prefix = settings.data_directory + \"CoralBleaching/BrattData/proc_essays_pickled_\"\n",
      "features_filename_prefix = settings.data_directory + \"CoralBleaching/BrattData/feats_pickled_\"\n",
      "\n",
      "@memoize_to_disk(filename_prefix=processed_essay_filename_prefix)\n",
      "def load_and_process_essays(min_df=5,\n",
      "                           remove_infrequent=False, spelling_correct=True,\n",
      "                           replace_nums=True, stem=False, remove_stop_words=False,\n",
      "                           remove_punctuation=True, lower_case=True):\n",
      "\n",
      "    logger.info(\"Loading Essays\")\n",
      "    essays = load_bratt_essays()\n",
      "    logger.info(\"Processing Essays\")\n",
      "    return process_essays(essays,\n",
      "                                   min_df=min_df,\n",
      "                                   remove_infrequent=remove_infrequent,\n",
      "                                   spelling_correct=spelling_correct,\n",
      "                                   replace_nums=replace_nums,\n",
      "                                   stem=stem,\n",
      "                                   remove_stop_words=remove_stop_words,\n",
      "                                   remove_punctuation=remove_punctuation,\n",
      "                                   lower_case=lower_case)\n",
      "\n",
      "tagged_essays = load_and_process_essays(min_df=MIN_SENTENCE_FREQ, remove_infrequent= REMOVE_INFREQUENT, spelling_correct= SPELLING_CORRECT,\n",
      "                        replace_nums= REPLACE_NUMS, stem=STEM, remove_stop_words= REMOVE_STOP_WORDS, remove_punctuation= REMOVE_PUNCTUATION, lower_case=LOWER_CASE)\n",
      "\n",
      "# FEATURE SETTINGS\n",
      "WINDOW_SIZE         = 7\n",
      "POS_WINDOW_SIZE     = 1\n",
      "MIN_FEAT_FREQ       = 5        # 5 best so far\n",
      "CV_FOLDS            = 5\n",
      "# END FEATURE SETTINGS\n",
      "\n",
      "offset = (WINDOW_SIZE-1) / 2\n",
      "unigram_window = fact_extract_positional_word_features(offset)\n",
      "biigram_window = fact_extract_ngram_features(offset, 2)\n",
      "pos_tag_window = fact_extract_positional_POS_features((POS_WINDOW_SIZE-1/2))\n",
      "#TODO - add POS TAGS (positional)\n",
      "#TODO - add dep parse feats\n",
      "#TODO - memoize features above for speed\n",
      "#extractors = [unigram_window, biigram_window, pos_tag_window]\n",
      "extractors = [unigram_window, biigram_window, pos_tag_window]\n",
      "\n",
      "# most params below exist ONLY for the purposes of the hashing to and from disk\n",
      "@memoize_to_disk(filename_prefix=features_filename_prefix)\n",
      "def extract_features(min_df,\n",
      "                     rem_infreq, spell_correct,\n",
      "                     replace_nos, stem, rem_stop_wds,\n",
      "                     rem_punc, l_case,\n",
      "                     win_size, pos_win_size, min_feat_freq,\n",
      "                     extractors):\n",
      "    feature_extractor = FeatureExtractorTransformer(extractors)\n",
      "    return feature_extractor.transform(tagged_essays)\n",
      "\n",
      "essay_feats = extract_features(min_df=MIN_SENTENCE_FREQ, rem_infreq=REMOVE_INFREQUENT,\n",
      "                               spell_correct=SPELLING_CORRECT,\n",
      "                               replace_nos=REPLACE_NUMS, stem=STEM, rem_stop_wds=REMOVE_STOP_WORDS,\n",
      "                               rem_punc=REMOVE_PUNCTUATION, l_case=LOWER_CASE,\n",
      "                               win_size=WINDOW_SIZE, pos_win_size=POS_WINDOW_SIZE,\n",
      "                               min_feat_freq=MIN_FEAT_FREQ, extractors=extractors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use more tags for training for sentence level classifier\n",
      "_, lst_all_tags = flatten_to_wordlevel_feat_tags(essay_feats)\n",
      "all_tags = set(flatten(lst_all_tags))\n",
      "\n",
      "train_tags = [c for c in all_tags if c != \"it\"]\n",
      "test_tags  = [c for c in all_tags if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "folds = cross_validation(essay_feats, CV_FOLDS)\n",
      "lst_td_wt_mean_prfa, lst_vd_wt_mean_prfa, lst_td_mean_prfa, lst_vd_mean_prfa = [], [], [], []\n",
      "td_all_metricsByTag = defaultdict(list)\n",
      "vd_all_metricsByTag = defaultdict(list)\n",
      "\n",
      "def merge_metrics(src, tgt):\n",
      "    for k, metric in src.items():\n",
      "        tgt[k].append(metric)\n",
      "\n",
      "def agg_metrics(src, agg_fn):\n",
      "    agg = dict()\n",
      "    for k, metrics in src.items():\n",
      "        agg[k] = agg_fn(metrics)\n",
      "    return agg\n",
      "\n",
      "#fn_create_cls = lambda : LinearSVC(C=1.0)\n",
      "# LR seems to do better\n",
      "fn_create_cls = lambda: LinearSVC(C=1.0)\n",
      "\n",
      "for i,(TD, VD) in enumerate(folds):\n",
      "    print \"\\nFold %s\" % i\n",
      "    \"\"\" Data Partitioning and Training \"\"\"\n",
      "    td_feats, td_tags = flatten_to_wordlevel_feat_tags(TD)\n",
      "    vd_feats, vd_tags = flatten_to_wordlevel_feat_tags(VD)\n",
      "\n",
      "    feature_transformer = FeatureTransformer(min_feature_freq=MIN_FEAT_FREQ)\n",
      "    td_X = feature_transformer.fit_transform(td_feats)\n",
      "    vd_X = feature_transformer.transform(vd_feats)\n",
      "    td_ys_bycode = get_wordlevel_ys_by_code(td_tags)\n",
      "    vd_ys_bycode = get_wordlevel_ys_by_code(vd_tags)\n",
      "\n",
      "    \"\"\" TRAIN \"\"\"\n",
      "    tag2Classifier = train_wordlevel_classifier(td_X, td_ys_bycode, fn_create_cls, train_tags)\n",
      "\n",
      "    \"\"\" TEST \"\"\"\n",
      "    td_metricsByTag, td_wt_mean_prfa, td_mean_prfa = test_word_level_classifiers(td_X, td_ys_bycode, tag2Classifier, test_tags)\n",
      "    vd_metricsByTag, vd_wt_mean_prfa, vd_mean_prfa = test_word_level_classifiers(vd_X, vd_ys_bycode, tag2Classifier, test_tags)\n",
      "\n",
      "    lst_td_wt_mean_prfa.append(td_wt_mean_prfa), lst_td_mean_prfa.append(td_mean_prfa)\n",
      "    lst_vd_wt_mean_prfa.append(vd_wt_mean_prfa), lst_vd_mean_prfa.append(vd_mean_prfa)\n",
      "    merge_metrics(td_metricsByTag, td_all_metricsByTag)\n",
      "    merge_metrics(vd_metricsByTag, vd_all_metricsByTag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fold 0\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "\n",
        "Fold 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "\n",
        "Fold 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "\n",
        "Fold 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "\n",
        "Fold 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fn_create_cls()\n",
      "# print results for each code\n",
      "mean_td_metrics = agg_metrics(td_all_metricsByTag, mean_rpfa)\n",
      "mean_vd_metrics = agg_metrics(vd_all_metricsByTag, mean_rpfa)\n",
      "\n",
      "print_metrics_for_codes(mean_td_metrics, mean_vd_metrics)\n",
      "\n",
      "# print macro measures\n",
      "print \"\\nTraining   Performance\"\n",
      "print \"Weighted:\" + str(mean_rpfa(lst_td_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(mean_rpfa(lst_td_mean_prfa))\n",
      "\n",
      "print \"\\nValidation Performance\"\n",
      "print \"Weighted:\" + str(mean_rpfa(lst_vd_wt_mean_prfa))\n",
      "print \"Mean    :\" + str(mean_rpfa(lst_vd_mean_prfa))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)\n",
        "TAG:        1                   \n",
        "recall:     0.989705233976       0.681705204586      \n",
        "precision:  0.991514606471       0.722231620327      \n",
        "f1:         0.990604771045       0.700092526211      \n",
        "accuracy:   0.999472363533       0.983555264696      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        11                  \n",
        "recall:     0.992119748914       0.527419786096      \n",
        "precision:  0.99837398374        0.710365769496      \n",
        "f1:         0.995204162043       0.572426472943      \n",
        "accuracy:   0.999968718599       0.997389390879      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        12                  \n",
        "recall:     0.956507936508       0.53000697384       \n",
        "precision:  0.986770646048       0.777819327731      \n",
        "f1:         0.971299666522       0.625134052388      \n",
        "accuracy:   0.999805519612       0.997679697495      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        13                  \n",
        "recall:     0.993050150641       0.520365418895      \n",
        "precision:  0.988055969348       0.656030874786      \n",
        "f1:         0.990537492389       0.574084780382      \n",
        "accuracy:   0.999842856639       0.993713059302      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        14                  \n",
        "recall:     0.963060665543       0.627843137255      \n",
        "precision:  0.950004836224       0.642426114692      \n",
        "f1:         0.956140756522       0.597473718054      \n",
        "accuracy:   0.999378237117       0.995559527058      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        2                   \n",
        "recall:     0.920497404583       0.543201293962      \n",
        "precision:  0.96103481088        0.486921630149      \n",
        "f1:         0.940241505272       0.445760288166      \n",
        "accuracy:   0.998661063489       0.988777860616      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        3                   \n",
        "recall:     0.956135111827       0.568947493706      \n",
        "precision:  0.966625866641       0.609039035539      \n",
        "f1:         0.961333484618       0.58186074953       \n",
        "accuracy:   0.996723490563       0.965639843363      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        4                   \n",
        "recall:     0.997090255033       0.679417172385      \n",
        "precision:  0.989811738945       0.803731162293      \n",
        "f1:         0.993434743192       0.728649338423      \n",
        "accuracy:   0.999887052485       0.995813068569      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        5                   \n",
        "recall:     0.947095141207       0.444046093825      \n",
        "precision:  0.987343053268       0.528671706339      \n",
        "f1:         0.966786248178       0.467933565358      \n",
        "accuracy:   0.998705871463       0.980988711378      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        50                  \n",
        "recall:     0.978474054737       0.805745431048      \n",
        "precision:  0.982568511683       0.834197254075      \n",
        "f1:         0.980507846505       0.81906961746       \n",
        "accuracy:   0.997382011412       0.976226700029      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        6                   \n",
        "recall:     0.974748760059       0.463818119221      \n",
        "precision:  0.978170626519       0.700357142857      \n",
        "f1:         0.976426891463       0.519780786589      \n",
        "accuracy:   0.999849546575       0.997481479779      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        7                   \n",
        "recall:     0.965651206671       0.50343690792       \n",
        "precision:  0.987724134931       0.571884638874      \n",
        "f1:         0.976549650643       0.524928699121      \n",
        "accuracy:   0.998912099168       0.979231872329      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "TAG:        explicit            \n",
        "recall:     0.908127474294       0.29615826995       \n",
        "precision:  0.958551902386       0.361651697341      \n",
        "f1:         0.932630506478       0.313455779806      \n",
        "accuracy:   0.996638068995       0.968010971851      \n",
        "sentences:  5.0                  5.0                 \n",
        "\n",
        "\n",
        "Training   Performance\n",
        "Weighted:Recall: 0.9631, Precision: 0.9779, F1: 0.9703, Accuracy: 0.9980, Codes:     5\n",
        "Mean    :Recall: 0.9648, Precision: 0.9790, F1: 0.9717, Accuracy: 0.9989, Codes:     5\n",
        "\n",
        "Validation Performance\n",
        "Weighted:Recall: 0.5933, Precision: 0.6711, F1: 0.6199, Accuracy: 0.9774, Codes:     5\n",
        "Mean    :Recall: 0.5532, Precision: 0.6466, F1: 0.5747, Accuracy: 0.9862, Codes:     5\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "VD\n",
      "\n",
      "Logistic Regression:\n",
      "Weighted:Recall: 0.5154, Precision: 0.7704, F1: 0.5950, Accuracy: 0.9797, Codes:     5\n",
      "Mean    :Recall: 0.4444, Precision: 0.7643, F1: 0.5325, Accuracy: 0.9875, Codes:     5\n",
      "\n",
      "LinearSVC(C=1.0)\n",
      "Weighted:Recall: 0.5705, Precision: 0.6637, F1: 0.6035, Accuracy: 0.9770, Codes:     5\n",
      "Mean    :Recall: 0.5276, Precision: 0.6265, F1: 0.5548, Accuracy: 0.9858, Codes:     5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "'<function extract_ngram_features at 0x111d74938>'"
       ]
      }
     ],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}