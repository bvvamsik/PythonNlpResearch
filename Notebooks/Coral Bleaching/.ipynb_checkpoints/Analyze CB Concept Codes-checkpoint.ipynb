{
 "metadata": {
  "name": "",
  "signature": "sha256:eee626cb1533ad0b7d57ba8c012d6a2e91c4f14d8a1cc5bf89403be532726bdb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from BrattEssay import load_bratt_essays\n",
      "from collections import defaultdict\n",
      "from IterableFP import flatten\n",
      "\n",
      "#essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA_Pre_Post_Merged/\")\n",
      "\n",
      "wd_sent_freq = defaultdict(int)\n",
      "all_codes = set()\n",
      "#Stores all words for the spelling corrector\n",
      "words = []\n",
      "all_sentences = []\n",
      "sentencesForCode = defaultdict(list)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wdsInSent = set()\n",
      "        codes4sentence = set()\n",
      "        sent = []\n",
      "        for w, tags in sentence:\n",
      "            words.append(w)\n",
      "            all_codes.update(tags)\n",
      "            codes4sentence.update(tags)\n",
      "            if w not in wdsInSent:\n",
      "                wdsInSent.add(w)\n",
      "                wd_sent_freq[w] += 1\n",
      "            sent.append(w)\n",
      "        all_sentences.append(sent)\n",
      "        for code in codes4sentence:\n",
      "            sentencesForCode[code].append(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "213 files found\n",
        "213 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the Occurences of the Causal Relations with respect to the Particular Codes\n",
      "-----------------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for essay in essays:\n",
      "    #print essay.file_name\n",
      "    for s_ix, tags in enumerate(essay.sentence_tags):\n",
      "        if len(tags) > 0:\n",
      "            tags= set(\",\".join(\n",
      "                sorted(tags)).replace(\"Causer\",\"\").replace(\"Result\",\"\") \\\n",
      "                    .replace(\"explicit\",\"\").replace(\":\", \"\").replace(\",,\",\",\").split(\",\"))\n",
      "            if \"\" in tags:\n",
      "                tags.remove(\"\")\n",
      "            to_prn = \"\\t\".join(sorted(tags, key = len)).replace(\"\\t\\t\",\"\\t\")\n",
      "            if \"->\" in to_prn:\n",
      "                print to_prn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "3\trhetorical\t3->rhetorical\n",
        "Anaphor\trhetorical\trhetorical->Anaphor\n",
        "3\t50\t3->50\n",
        "3\t50\t3->50\tAnaphor\tAnaphor->Anaphor\n",
        "1\t50\t1->50\n",
        "1\t50\t1->50\n",
        "1\t3\t50\t1->3\t3->50\trhetorical\trhetorical->50\n",
        "3\t50\t3->50\n",
        "Anaphor\trhetorical\tAnaphor->rhetorical\n",
        "50\tother\tother->50\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\tother\trhetorical\n",
        "50\trhetorical\trhetorical->50\n",
        "7\t50\t7->50\n",
        "50\t7->50\n",
        "7\t50\t7->50\tother\n",
        "50\trhetorical\trhetorical->50\n",
        "rhetorical\t4->rhetorical\n",
        "4->rhetorical\n",
        "3\t4\t14\t3->4\t4->14\t4->rhetorical\n",
        "3\t50\t3->50\n",
        "7\t50\t7->50\n",
        "14\t50\t14->50\n",
        "1\trhetorical\t1->rhetorical\n",
        "4\t50\tother\t4->50\n",
        "4\t50\t4->50\trhetorical\n",
        "3\t50\t3->50\n",
        "1\t50\t1->50\n",
        "11\t50\t11->50\n",
        "1\t50\t1->50\n",
        "50\tAnaphor\tAnaphor->50\n",
        "3\trhetorical\trhetorical->3\n",
        "11\t12\t11->12\n",
        "3\t50\t3->50\n",
        "1\t50\t1->50\n",
        "7\t6\t6->7\n",
        "1\t50\t1->50\n",
        "1\t50\t1->50\n",
        "3\t50\t3->50\n",
        "1\t3\t50\tother\t3->50\n",
        "50\trhetorical\trhetorical->50\n",
        "1\t50\t1->50\n",
        "1\t3\t50\t1->50\n",
        "1\t3\t50\t1->50\n",
        "1\t50\t1->50\n",
        "1\trhetorical\t1->rhetorical\n",
        "7\tother\trhetorical\trhetorical->7\n",
        "50\trhetorical\trhetorical->50\n",
        "50\t7->50\t13->50\n",
        "11\t13\t12\t7->50\t13->50\t11->12\t12->13\n",
        "7->50\n",
        "50\t7->50\n",
        "7->50\n",
        "7\t50\t7->50\n",
        "3\t4\tother\tother->other\n",
        "13\tother\tother->13\n",
        "3\t5\t50\t3->5\t5->50\n",
        "5\t4\t4->5\n",
        "50\tother\tother->50\n",
        "1\t50\t1->50\n",
        "3\trhetorical\trhetorical->3\n",
        "11\t13\t12\t11->7\n",
        "13\t14\t11->7\t13->14\n",
        "50\t11->7\n",
        "11->7\n",
        "11->7\n",
        "11->7\n",
        "11->7\n",
        "11->7\n",
        "50\t11->7\n",
        "50\t11->7\tAnaphor\n",
        "7\t6\t6->7\t11->7\tAnaphor\t7->Anaphor\n",
        "50\t11->7\tAnaphor\tAnaphor->50\n",
        "7\t50\t11->7\n",
        "50\t5b\t5b->50\n",
        "7\t50\t7->50\n",
        "3\t50\t3->50\trhetorical\n",
        "1\t3\t5\t3->5\t1->3\n",
        "1\t50\t1->50\n",
        "1\t50\t1->50\trhetorical\n",
        "1\t50\t1->50\trhetorical\n",
        "1\t3\t1->3\n",
        "3\t5\t3->5\n",
        "1\t50\t1->50\n",
        "3\t4\t14\t50\t3->4\t3->50\t4->14\t14->50\n",
        "11\t13\t12\t14\t11->12\t12->13\t13->14\n",
        "3\t4\t14\t50\t4->50\t4->14\n",
        "11\t13\t11->13\n",
        "3\t50\t3->50\n",
        "1\t3\t50\t3->50\tother\t1->50\n",
        "7\t50\t7->50\tother\n",
        "5\t4\t4->5\tother\n",
        "50\tother\tAnaphor\tAnaphor->50\n",
        "6\t50\t5b\t6->5b\t6->50\tother\t5b->50\tAnaphor\n",
        "50\tAnaphor\tCOMPILED\tAnaphor->50\n",
        "6\t50\t6->50\n",
        "7\t6\t6->7\tother\n",
        "1\t50\t1->50\n",
        "7\t50\t7->50\tother\n",
        "7\t6\t50\t7->50\tAnaphor\tAnaphor->7\n",
        "50\tAnaphor\tAnaphor->50\n",
        "50\tother\tAnaphor\trhetorical\tAnaphor->50\n",
        "4\t50\t4->50\n",
        "1\t50\t1->50\trhetorical\n",
        "1\t3\t50\tother\t3->50\n",
        "1\t50\t1->50\n",
        "7\tAnaphor\t7->Anaphor\n",
        "5\t4\t50\t4->50\t5->50\tother\n",
        "1\t50\t1->50\n",
        "1\t50\t1->50\n",
        "11\t13\t12\t11->13\n",
        "11\t13\t12\t11->12\n",
        "7\t50\t7->50\n",
        "1\t3\trhetorical\t1->rhetorical\t3->rhetorical\n",
        "3\t5\t3->5\tCOMPILED\n",
        "6\t50\t6->50\n",
        "1\t3\t1->3\n",
        "3\t5\t3->5\n",
        "3\t5\t4\t4->5\n",
        "7\t50\t7->50\n",
        "7\t6\t50\t7->50\n",
        "50\tAnaphor\tAnaphor->50\n",
        "3\trhetorical\trhetorical->7\n",
        "1\t3\t1->3\trhetorical->7\n",
        "50\trhetorical->7\n",
        "3\t50\trhetorical->7\n",
        "rhetorical->7\n",
        "rhetorical->7\n",
        "rhetorical->7\n",
        "rhetorical->7\n",
        "3\trhetorical->7\n",
        "7\trhetorical->7\n",
        "50\tother\trhetorical\trhetorical->7\n",
        "rhetorical->7\n",
        "7\t50\trhetorical->7\n",
        "7\t50\t7->50\n",
        "11\t13\t11->13\n",
        "7\t6\t6->7\n",
        "50\trhetorical\trhetorical->50\n",
        "3\t50\t3->50\n",
        "13\t50\t13->50\n",
        "3\t4\t50\t4->50\n",
        "5\t4\t4->5\n",
        "7\t6\t50\t6->7\t7->50\n",
        "3\t13\t50\t13->50\n",
        "3\t13\t14\t3->14\n",
        "11\t13\t11->13\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "7\t6\t50\t6->7\t7->50\n",
        "50\trhetorical\trhetorical->50\n",
        "3\t50\t3->50\n",
        "nd\tAnaphor\tnd->Anaphor\n",
        "1\t50\t1->50\n",
        "1\t3\t1->3\n",
        "3\t3->50\n",
        "50\t3->50\tCOMPILED\n",
        "1\t3\t50\t1->3\t3->50\tother\trhetorical\n",
        "13\t50\t13->50\tAnaphor\n",
        "11\t12\t11->12\tAnaphor\t12->Anaphor\n",
        "7\t50\t7->50\tother\n",
        "11\t13\t12\t11->12\n",
        "3\trhetorical\t3->rhetorical\n",
        "1\t50\t1->50\n",
        "11\t13\t12\t11->12\n",
        "7\t6\t6->7\tother\n",
        "1\t50\t1->50\n",
        "3\t50\t3->50\n",
        "6\t3\t5\t50\t5->6\t3->5\t3->50\t6->50\tother\tAnaphor\n",
        "1\t3\t50\t3->50\t1->50\n",
        "3\t50\t3->50\n",
        "7\t3\t4\t4->7\t3->4\n",
        "11\t13\t5b\t11->13\t13->5b\n",
        "1\t3\t1->3\n",
        "1\t3\t1->3\n",
        "1\t3\t1->3\n",
        "3\t50\t3->50\n",
        "7\t6\t6->7\n",
        "3\t50\t3->50\n",
        "3\t5\t3->5\n",
        "3\t50\tother\t3->50\tAnaphor\n",
        "1\t50\t1->50\n",
        "50\trhetorical\trhetorical->50\n",
        "5\t4\t50\t4->5\t5->50\n",
        "7\t6\t50\t6->7\t7->50\tother\tAnaphor\n",
        "2\tAnaphor\tAnaphor->2\n",
        "3\t2\t2->3\n",
        "7\t6\t50\t6->7\tother\t7->other\n",
        "7\t50\t7->50\tAnaphor\n",
        "50\trhetorical\trhetorical->50\n",
        "7\t6\t6->7\n",
        "50\tAnaphor\tAnaphor->50\n",
        "3\t4\t3->4\n",
        "5\t4\t4->5\n",
        "14\tAnaphor\tAnaphor->14\n",
        "3\t50\t3->50\n",
        "50\trhetorical\trhetorical->50\n",
        "7\t50\t7->50\n",
        "3\t50\t3->50\n",
        "1\t50\t1->50\n",
        "5\t50\t5->50\tother\tAnaphor\tAnaphor->50\n",
        "7\t50\t7->50\n",
        "11\t50\t11->50\n",
        "3\t50\t3->50\n",
        "7\t50\t7->50\tother\n",
        "7\t6\t6->7\tother\n",
        "1\t3\t2\t50\t3->50\t2->50\t1->50\n",
        "7\t50\t7->50\tother\tAnaphor\n",
        "3\t50\t3->50\n",
        "3\t50\t3->50\n",
        "7\tAnaphor\tAnaphor->7\n",
        "7\t50\t50->7\n",
        "7\t50\t7->50\n",
        "1\t50\t1->50\n",
        "11\t13\t50\tother\t11->13\n",
        "1\t3\t50\t3->50\t1->50\n",
        "1\t50\t1->50\n",
        "3\t50\t3->50\n",
        "Anaphor\trhetorical\trhetorical->Anaphor\n",
        "1\t50\t1->50\trhetorical\n",
        "1\t50\t1->50\trhetorical\n",
        "1\t3\t1->3\n",
        "1\t50\tother\t1->50\n",
        "7\t6\t6->7\tother\tAnaphor\t7->Anaphor\t6->Anaphor\n",
        "7\t50\t5b\t7->50\tother\tAnaphor\n",
        "3\t4\t3->4\tother\tAnaphor\n",
        "3\t4\t50\t4->50\tother\tAnaphor\n",
        "1\t3\t50\t3->50\t1->50\n",
        "1\t7\t6\t6->7\t1->6\tAnaphor\t7->Anaphor\t1->Anaphor\n",
        "5\t13\t13->5\n",
        "3\t4\t3->4\n",
        "6\t50\t6->50\n",
        "1\t3\t2\t50\t3->50\tother\t2->50\t1->50\n",
        "7\t50\t7->50\tother\n",
        "7\t50\t7->50\tAnaphor\n",
        "5\t14\t50\t5->14\t5->50\tAnaphor\tAnaphor->5\n",
        "11\t13\t11->13\n",
        "7\t50\t7->50\tother\tAnaphor\n",
        "11\t13\t12\t50\t13->50\t11->12\t12->13\n",
        "5\t7\t50\t7->50\tother\tAnaphor\n",
        "7\t50\t7->50\n",
        "1\t7\t50\t7->50\n",
        "7\t50\t7->50\tother\tAnaphor\n",
        "3\t7\t50\t3->7\t7->50\tother\n",
        "13\t14\tother\t13->14\n",
        "7\t50\t7->50\n",
        "Anaphor\trhetorical\trhetorical->Anaphor\n",
        "1\t2\t50\t2->50\t1->50\trhetorical\n",
        "3\tAnaphor\tAnaphor->3\n",
        "3\t50\t3->50\n",
        "3\t5\t50\t3->5\t5->50\tother\tAnaphor\n",
        "5\t5b\t5->5b\n",
        "3\t4\t3->4\n",
        "1\t3\t1->3\n",
        "1\t3\t1->3\t3->50\n",
        "50\t3->50\n",
        "1\t3\t5\t3->5\t1->3\tother\n",
        "7\t50\t50->7\n",
        "1\t3\t7\t6\t6->7\t3->6\tother\tAnaphor\n",
        "5\t4\t14\t4->5\t4->14\n",
        "11\t12\t14\t11->12\t12->14\n",
        "7\t6\t50\t6->7\n",
        "50\tAnaphor\tAnaphor->50\n",
        "1\t3\t2\t50\t3->50\t2->50\t1->50\n",
        "7\t6\t6->7\n",
        "1\t5\t14\t50\t5->50\t1->50\t14->50\n",
        "1\t3\t1->3\tAnaphor\n",
        "1\t3\t1->3\n",
        "3\t50\t3->50\n",
        "14\t50\t14->50\n",
        "5\t50\t5->50\n",
        "3\t4\t3->4\n",
        "5\t4\t4->5\n",
        "3\t50\t3->50\n",
        "3\t5\t4\t4->5\t3->4\n",
        "14\tAnaphor\tAnaphor->14\n",
        "13\t50\t13->50\n",
        "11\t13\t12\t11->12\t12->13\n",
        "13\t14\t13->14\tCOMPILED\n",
        "1\t50\t1->50\n",
        "7\t50\t7->50\n",
        "5\t7\t50\t5b\t7->5\t5->7\t5->5b\t7->50\tother\t5b->50\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "7\t5b\tother\t5b->7\tAnaphor\n",
        "1\t50\t1->50\n",
        "50\tother\tAnaphor\tAnaphor->50\n",
        "1\t50\tAnaphor\t1->Anaphor\n",
        "1\t50\t1->50\n",
        "Anaphor\tAnaphor->Anaphor\n",
        "3\t50\tother\t3->50\n",
        "1\t3\t1->3\n",
        "3\t5\t5b\t5->5b\tAnaphor\n",
        "3\t7\t3->7\n",
        "7\t50\t7->50\n",
        "3\t50\t3->50\n",
        "11\t13\t11->13\n",
        "5\t4\t4->5\n",
        "14\tAnaphor\tAnaphor->14\n",
        "13\t14\tother\t13->14\n",
        "Anaphor\trhetorical\trhetorical->Anaphor\n",
        "5b\tAnaphor\t5b->Anaphor\n",
        "3\t50\t3->50\n",
        "1\t50\tother\t1->50\n",
        "5\t50\t5->50\n",
        "3\t4\t3->4\n",
        "5\t4\t4->5\n",
        "5\t14\t5->14\n",
        "1\t3\t50\t3->50\t1->50\n",
        "3\t50\tother\t3->50\n",
        "3\t50\t3->50\n",
        "7\t6\t6->7\n",
        "11\t13\t13->11\n",
        "11\t13\t11->13\n",
        "13\t14\t13->14\n",
        "7\t50\t7->50\n",
        "3\t50\t3->50\n",
        "3\t4\t3->4\n",
        "5\t4\t4->5\n",
        "14\tAnaphor\tAnaphor->14\n",
        "11\t13\t11->13\n",
        "11\t13\t12\t11->12\t12->13\n",
        "13\t14\tother\t13->14\n",
        "1\t3\tAnaphor\t1->Anaphor\t3->Anaphor\n",
        "1\t3\t1->3\n",
        "13\t14\t13->14\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "7\t6\t6->7\n",
        "50\tAnaphor\tAnaphor->50\n",
        "1\t3\t50\t3->50\t1->50\n",
        "5\t4\t4->5\trhetorical\n",
        "14\tAnaphor\tAnaphor->14\n",
        "1\t3\t1->3\n",
        "1\t50\t1->50\n",
        "3\t2\t2->3\n",
        "50\tAnaphor\tAnaphor->50\n",
        "7\tAnaphor\t7->Anaphor\n",
        "7\t50\t7->50\n",
        "7\t50\t7->50\n",
        "3\t50\t3->50\n",
        "6\t3\t5\t50\t5->6\t3->5\n",
        "1\t3\t50\t3->50\t1->50\n",
        "3\t50\t3->50\n",
        "3\t5\t4\t3->4\t3->5\n",
        "3\t50\t3->50\n",
        "7\t6\t6->7\n",
        "3\t50\t3->50\n",
        "50\t5b\tother\t5b->50\tAnaphor\n",
        "1\t50\t1->50\n",
        "2\t50\t2->50\n",
        "1\t50\t1->50\n",
        "7\t50\t7->50\n",
        "3\t4\t50\t3->4\t4->50\n",
        "5\t4\t4->5\n",
        "14\tAnaphor\tAnaphor->14\n",
        "7\t50\t7->50\n",
        "4\t50\t4->50\n",
        "Anaphor\trhetorical\trhetorical->Anaphor\n",
        "11\t13\t50\t13->50\tAnaphor\trhetorical\n",
        "50\t3->50\n",
        "3\t11\t11->3\t3->50\n",
        "7\t50\t7->50\n",
        "7\t6\t6->7\tother\tAnaphor\n",
        "11\t50\t11->50\n",
        "13\t14\t13->14\n",
        "7\t6\t6->7\n",
        "50\tother->50\n",
        "11\tother\tother->50\t11->other\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Which Codes Co-Occur?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_tags = []\n",
      "for essay in essays:\n",
      "    #print essay.file_name\n",
      "    for s_ix, tags in enumerate(essay.sentence_tags):\n",
      "        all_tags.append(tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cnt = float(len(all_tags))\n",
      "non_empty = filter(lambda s: len(s) > 0, all_tags)\n",
      "multiple =  filter(lambda s: len(s) > 1, all_tags)\n",
      "\n",
      "print cnt, len(non_empty)/cnt , len(multiple)/cnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2133.0 0.601031411158 0.283169245195\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IterableFP import flatten\n",
      "unique = set(flatten(all_tags))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reg_tags = map(lambda s: filter(lambda tag: tag[0].isdigit(), s), all_tags)\n",
      "reg_multiple =  filter(lambda s: len(s) > 1, reg_tags)\n",
      "\n",
      "#reg_multiple[0:100]\n",
      "#len(reg_multiple)/ cnt\n",
      "\n",
      "combo_cnts = defaultdict(int)\n",
      "for tags in reg_multiple:\n",
      "    stags = sorted(tags)\n",
      "    for a in stags:\n",
      "        for b in stags:\n",
      "            if a < b:\n",
      "                combo_cnts[(a,b)] += 1\n",
      "\n",
      "u_cnt = float(len(unique))\n",
      "common_combos = [(k,v) for k,v in combo_cnts.items() if v >= 5]\n",
      "cc_cnt = len(common_combos)\n",
      "print \"Unique Codes\",  u_cnt, \"Possible Combos\", 0.5 * (u_cnt * (u_cnt -1)), \"Observed Combos\", len(combo_cnts), \"Common Combos\", cc_cnt\n",
      "\n",
      "sorted(combo_cnts.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unique Codes 123.0 Possible Combos 7503.0 Observed Combos 47 Common Combos 25\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[(('1', '14'), 1),\n",
        " (('1', '2'), 11),\n",
        " (('1', '3'), 72),\n",
        " (('1', '5'), 4),\n",
        " (('1', '50'), 74),\n",
        " (('1', '6'), 2),\n",
        " (('1', '7'), 3),\n",
        " (('11', '12'), 13),\n",
        " (('11', '13'), 21),\n",
        " (('11', '14'), 2),\n",
        " (('11', '3'), 2),\n",
        " (('11', '50'), 7),\n",
        " (('11', '5b'), 1),\n",
        " (('12', '13'), 10),\n",
        " (('12', '14'), 2),\n",
        " (('12', '50'), 1),\n",
        " (('13', '14'), 10),\n",
        " (('13', '3'), 3),\n",
        " (('13', '5'), 1),\n",
        " (('13', '50'), 7),\n",
        " (('13', '5b'), 1),\n",
        " (('13', '7'), 1),\n",
        " (('14', '3'), 4),\n",
        " (('14', '4'), 4),\n",
        " (('14', '5'), 4),\n",
        " (('14', '50'), 6),\n",
        " (('2', '3'), 14),\n",
        " (('2', '50'), 7),\n",
        " (('3', '4'), 21),\n",
        " (('3', '5'), 26),\n",
        " (('3', '50'), 86),\n",
        " (('3', '5b'), 1),\n",
        " (('3', '6'), 3),\n",
        " (('3', '7'), 5),\n",
        " (('4', '5'), 18),\n",
        " (('4', '50'), 12),\n",
        " (('4', '7'), 1),\n",
        " (('5', '50'), 19),\n",
        " (('5', '5b'), 5),\n",
        " (('5', '6'), 2),\n",
        " (('5', '7'), 3),\n",
        " (('50', '5b'), 8),\n",
        " (('50', '6'), 16),\n",
        " (('50', '7'), 70),\n",
        " (('5b', '6'), 2),\n",
        " (('5b', '7'), 6),\n",
        " (('6', '7'), 26)]"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(common_combos)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "[(('1', '2'), 11),\n",
        " (('1', '3'), 72),\n",
        " (('1', '50'), 74),\n",
        " (('11', '12'), 13),\n",
        " (('11', '13'), 21),\n",
        " (('11', '50'), 7),\n",
        " (('12', '13'), 10),\n",
        " (('13', '14'), 10),\n",
        " (('13', '50'), 7),\n",
        " (('14', '50'), 6),\n",
        " (('2', '3'), 14),\n",
        " (('2', '50'), 7),\n",
        " (('3', '4'), 21),\n",
        " (('3', '5'), 26),\n",
        " (('3', '50'), 86),\n",
        " (('3', '7'), 5),\n",
        " (('4', '5'), 18),\n",
        " (('4', '50'), 12),\n",
        " (('5', '50'), 19),\n",
        " (('5', '5b'), 5),\n",
        " (('50', '5b'), 8),\n",
        " (('50', '6'), 16),\n",
        " (('50', '7'), 70),\n",
        " (('5b', '7'), 6),\n",
        " (('6', '7'), 26)]"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "common_combos_inv = [((b,a),num) for ((a,b),num) in common_combos]\n",
      "sorted(common_combos_inv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[(('12', '11'), 13),\n",
        " (('13', '11'), 21),\n",
        " (('13', '12'), 10),\n",
        " (('14', '13'), 10),\n",
        " (('2', '1'), 11),\n",
        " (('3', '1'), 72),\n",
        " (('3', '2'), 14),\n",
        " (('4', '3'), 21),\n",
        " (('5', '3'), 26),\n",
        " (('5', '4'), 18),\n",
        " (('50', '1'), 74),\n",
        " (('50', '11'), 7),\n",
        " (('50', '13'), 7),\n",
        " (('50', '14'), 6),\n",
        " (('50', '2'), 7),\n",
        " (('50', '3'), 86),\n",
        " (('50', '4'), 12),\n",
        " (('50', '5'), 19),\n",
        " (('5b', '5'), 5),\n",
        " (('5b', '50'), 8),\n",
        " (('6', '50'), 16),\n",
        " (('7', '3'), 5),\n",
        " (('7', '50'), 70),\n",
        " (('7', '5b'), 6),\n",
        " (('7', '6'), 26)]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Only a relatively small number of pairings occur in actuality. Compare these to the causal model. Most of them are either close together, or combined with a 50 (jumps straight to the effect) **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" How many unique combos (not pairwise but any)? \"\"\"\n",
      "nary_combos = defaultdict(int)\n",
      "for tags in reg_multiple:\n",
      "    stags = tuple(sorted(tags))\n",
      "    nary_combos[stags] +=1\n",
      "\n",
      "freq_nary = [(tpl,num) for tpl, num in nary_combos.items() if num >= 5]\n",
      "print \"Unique 2-3 way\", len(nary_combos), \"Frequent 2-3 way\", len(freq_nary)\n",
      "\n",
      "for tpl, num in sorted(freq_nary):\n",
      "    s = \"\"\n",
      "    for item in tpl[:-1]:\n",
      "        s += str(item).rjust(2) + \" & \"\n",
      "    s += str(tpl[-1]).rjust(2)\n",
      "    print s.ljust(15), \"Count:\", num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unique 2-3 way 65 Frequent 2-3 way 18\n",
        " 1 &  3         Count: 43\n",
        " 1 &  3 & 50    Count: 17\n",
        " 1 & 50         Count: 50\n",
        "11 & 12 & 13    Count: 8\n",
        "11 & 13         Count: 8\n",
        "13 & 14         Count: 8\n",
        " 2 &  3         Count: 6\n",
        " 3 &  4         Count: 11\n",
        " 3 &  5         Count: 14\n",
        " 3 & 50         Count: 52\n",
        " 4 &  5         Count: 12\n",
        " 4 & 50         Count: 5\n",
        " 5 & 50         Count: 8\n",
        "50 & 5b         Count: 5\n",
        "50 &  6         Count: 5\n",
        "50 &  6 &  7    Count: 8\n",
        "50 &  7         Count: 56\n",
        " 6 &  7         Count: 16\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Note that these are not within a sentence, but within a word, so we could reframe as a multi-class tagging problem. Also, no word has more than 3 tags (at least frequently). **"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Best thing to do is probably this. Train all of the binary classifiers in concert, i.e. train a model per code, but feed in the predictions from all the other models as features **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_window(win):\n",
      "    def set2str(st):\n",
      "        return \"{\" + str(t)[5:-2] + \"}\"\n",
      "    \n",
      "    w, tg = zip(*win)\n",
      "    lens = [max(len(wd),len(set2str(t))) for wd,t in win]\n",
      "    \n",
      "    for i, wd in enumerate(w):\n",
      "        print wd.ljust(lens[i]) , \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "    for i, t in enumerate(tg):\n",
      "        print set2str(t).ljust(lens[i]), \"|\",\n",
      "    print \"\"\n",
      "\n",
      "def print_code(sentence, code):\n",
      "    wds, tags = zip(*sentence)\n",
      "    tagged = []\n",
      "    for i, (w,t) in enumerate(sentence):\n",
      "        if code in t:\n",
      "            if i > 0 and len(tagged) > 0 and code not in sentence[i-1][1]:\n",
      "                tagged.append(\"|\")\n",
      "            tagged.append(w)\n",
      "    print \" \".join(tagged).ljust(60)#, \" \".join(wds)\n",
      "\n",
      "\"\"\" Prints a unique string for each set of consecutive tags \"\"\"\n",
      "def print_unique_strings_for_code(sentences, code):\n",
      "    tally = defaultdict(int)\n",
      "    cnt = 0 \n",
      "    for sentence in sentences:\n",
      "        wds, tags = zip(*sentence)\n",
      "        tagged = []\n",
      "        for i, (wd,t) in enumerate(sentence):\n",
      "            if code in t:\n",
      "                if i > 0 and len(tagged) > 0 and code not in sentence[i-1][1]:\n",
      "                    tagged.append(\"|\")\n",
      "                tagged.append(wd)\n",
      "                \n",
      "        if len(tagged) > 0:\n",
      "            for phrase in \" \".join(tagged).split(\"|\"):\n",
      "                tally[phrase.strip().replace(\"  \", \" \")] +=1\n",
      "            cnt += 1\n",
      "    \n",
      "    s = sorted(tally.items(), key = lambda (k,v): -v)\n",
      "    print \"Total Sentences for code:\".ljust(30), cnt\n",
      "    print \"\"\n",
      "    for k,v in s:\n",
      "        print k.ljust(30), v\n",
      "\n",
      "\"\"\" Prints a single pattern per sentence \"\"\"\n",
      "def print_unique_patterns_for_code(sentences, code):\n",
      "    tally = defaultdict(int)\n",
      "    cnt = 0 \n",
      "    for sentence in sentences:\n",
      "        wds, tags = zip(*sentence)\n",
      "        tagged = []\n",
      "        for i, (wd,t) in enumerate(sentence):\n",
      "            if code in t:\n",
      "                if i > 0 and len(tagged) > 0 and code not in sentence[i-1][1]:\n",
      "                    tagged.append(\"|\")\n",
      "                tagged.append(wd)\n",
      "                \n",
      "        if len(tagged) > 0:\n",
      "            tally[\" \".join(tagged)] +=1\n",
      "            cnt += 1\n",
      "    \n",
      "    s = sorted(tally.items(), key = lambda (k,v): -v)\n",
      "    print \"Total Sentences for code:\".ljust(30), cnt\n",
      "    print \"\"\n",
      "    for k,v in s:\n",
      "        print k.ljust(30), v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at Concept Codes\n",
      "---------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "code = \"explicit\"\n",
      "sentences = sentencesForCode[code]\n",
      "#for sentence in sentences:\n",
      "#    print_code(sentence, code)\n",
      "\n",
      "print_unique_strings_for_code(sentences, code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total Sentences for code:      1008\n",
        "\n",
        "because                        98\n",
        ",                              92\n",
        "when                           59\n",
        "causes                         55\n",
        "If                             51\n",
        "When                           49\n",
        "because of                     47\n",
        "due to                         46\n",
        "if                             31\n",
        "cause                          30\n",
        "threatens                      25\n",
        "the                            25\n",
        "then                           21\n",
        "causing                        17\n",
        "As                             17\n",
        "can                            16\n",
        "force                          13\n",
        "disrupts                       13\n",
        "drop                           12\n",
        "caused by                      12\n",
        "are                            11\n",
        "affects                        11\n",
        "can cause                      10\n",
        "is                             10\n",
        "affect                         8\n",
        "makes                          8\n",
        "creates                        8\n",
        "which                          8\n",
        "why                            8\n",
        "happens                        8\n",
        "occurs                         7\n",
        "lead to                        7\n",
        "to                             6\n",
        "from                           6\n",
        "happens when                   6\n",
        "so                             6\n",
        "during                         6\n",
        "effect                         6\n",
        "is sensitive to                6\n",
        "result in                      6\n",
        "bring                          5\n",
        "the higher                     5\n",
        "for                            5\n",
        "by                             5\n",
        "as                             5\n",
        "because when                   4\n",
        "caused                         4\n",
        "more                           4\n",
        "results in                     4\n",
        "happen                         4\n",
        "increase                       4\n",
        "the more                       4\n",
        "is because                     4\n",
        "threatening                    4\n",
        ", the                          4\n",
        "will                           4\n",
        "it                             4\n",
        "make                           4\n",
        "occurs when                    4\n",
        "forces                         4\n",
        "becuase                        4\n",
        "results                        3\n",
        "depending on                   3\n",
        "that                           3\n",
        "The                            3\n",
        ", then                         3\n",
        "is caused by                   3\n",
        "changes                        3\n",
        "With                           3\n",
        "reason why                     3\n",
        "Due to                         3\n",
        "depends on                     2\n",
        "Another reason                 2\n",
        "ruin                           2\n",
        "is why                         2\n",
        "would                          2\n",
        "drops                          2\n",
        "produce                        2\n",
        "because if                     2\n",
        "that affects                   2\n",
        "affected by                    2\n",
        "could                          2\n",
        "may force                      2\n",
        "which is                       2\n",
        "Then                           2\n",
        "the lower                      2\n",
        "sensitive to                   2\n",
        "forced to                      2\n",
        "leads to                       2\n",
        "may cause                      2\n",
        "cause of                       2\n",
        "when it                        2\n",
        "happens because of             2\n",
        "Because of                     2\n",
        "hits                           2\n",
        "there is                       2\n",
        "and                            2\n",
        "it causes                      2\n",
        "comes from                     2\n",
        "can make                       2\n",
        "it can                         2\n",
        "increases                      2\n",
        "which makes                    2\n",
        "happens by                     2\n",
        "can effect                     2\n",
        "will be                        2\n",
        "meaning                        2\n",
        "which causes                   2\n",
        "leads                          1\n",
        "having such                    1\n",
        "that can                       1\n",
        "One reason                     1\n",
        "Another Reason                 1\n",
        "and stress                     1\n",
        ", causing                      1\n",
        "will make                      1\n",
        "putting                        1\n",
        "bringing                       1\n",
        "behind all                     1\n",
        "that's how                     1\n",
        "are due to                     1\n",
        "because it is needed           1\n",
        "Can effect                     1\n",
        "so it doesn't                  1\n",
        "will occur                     1\n",
        "that comes from                1\n",
        "also occurs when               1\n",
        "what                           1\n",
        "creates a                      1\n",
        "giving                         1\n",
        "which results in               1\n",
        "might cause                    1\n",
        "During                         1\n",
        ";                              1\n",
        "increasing                     1\n",
        "creates a distruption          1\n",
        "equals                         1\n",
        "water                          1\n",
        "witch                          1\n",
        "becuase of                     1\n",
        "which is a                     1\n",
        "causeing                       1\n",
        "happens of                     1\n",
        "change                         1\n",
        "arrive                         1\n",
        "can result in                  1\n",
        "contribute of                  1\n",
        "so did                         1\n",
        "brought                        1\n",
        "causal                         1\n",
        ", making                       1\n",
        "sends                          1\n",
        "therefore                      1\n",
        "lowering                       1\n",
        "becomes                        1\n",
        "disruption                     1\n",
        "are another cause of           1\n",
        "factor to                      1\n",
        "tends to                       1\n",
        "making this another factor to  1\n",
        "same year                      1\n",
        "wich makes                     1\n",
        "making                         1\n",
        "this same year there was       1\n",
        "may happen because of          1\n",
        "impacts                        1\n",
        "give                           1\n",
        "which then cause               1\n",
        "that's why                     1\n",
        "dropped                        1\n",
        "When the                       1\n",
        "thanks to                      1\n",
        "weaken                         1\n",
        "can lead to                    1\n",
        "provide                        1\n",
        "to keep                        1\n",
        "how                            1\n",
        "can start                      1\n",
        "stress                         1\n",
        "due too                        1\n",
        "also threatened by             1\n",
        "whenever                       1\n",
        "One                            1\n",
        "that's when                    1\n",
        "is important for coral to proceed 1\n",
        "happen for                     1\n",
        "increase when                  1\n",
        "are a few reasons why          1\n",
        "many force the coral           1\n",
        "which can                      1\n",
        "is more                        1\n",
        "can give                       1\n",
        "were related to eachother      1\n",
        "messes                         1\n",
        "affected                       1\n",
        "that also                      1\n",
        "What caused                    1\n",
        "has a lot to do with           1\n",
        "they'll                        1\n",
        "wich                           1\n",
        "will effect                    1\n",
        "occur s                        1\n",
        "thats why                      1\n",
        "reason                         1\n",
        "put                            1\n",
        "could cause                    1\n",
        "threat                         1\n",
        "is caused also because of      1\n",
        "could happen                   1\n",
        "and also                       1\n",
        "decides                        1\n",
        "blows                          1\n",
        "started to                     1\n",
        "which creates                  1\n",
        "needed                         1\n",
        "and drag                       1\n",
        "also causes                    1\n",
        "to get                         1\n",
        "which will                     1\n",
        "can start causing              1\n",
        "might aid                      1\n",
        "when most                      1\n",
        "than                           1\n",
        "caused to                      1\n",
        "begins                         1\n",
        "threathens                     1\n",
        "because during                 1\n",
        "Because as                     1\n",
        "result                         1\n",
        "all depends on                 1\n",
        "caused when                    1\n",
        "only be affected               1\n",
        "causes of                      1\n",
        "has to be                      1\n",
        "The weaker                     1\n",
        "the hotter                     1\n",
        "=                              1\n",
        "can happen                     1\n",
        "vary when                      1\n",
        "By the coral doing             1\n",
        "Without                        1\n",
        "The reasons                    1\n",
        "the weaker                     1\n",
        "\ufffd                              1\n",
        "how and why                    1\n",
        "and that why                   1\n",
        "reason for                     1\n",
        "it is                          1\n",
        "resulting in                   1\n",
        "explain                        1\n",
        "reasons on why                 1\n",
        "means                          1\n",
        "vary                           1\n",
        "The thing that                 1\n",
        "have a part in the cause for   1\n",
        "is from                        1\n",
        "Because                        1\n",
        "end up                         1\n",
        "without it                     1\n",
        "can happen from                1\n",
        "cause by                       1\n",
        "through                        1\n",
        "ruins                          1\n",
        "because of when                1\n",
        "has an effect on               1\n",
        "many causes                    1\n",
        "come in                        1\n",
        "effect the cause               1\n",
        "put under                      1\n",
        "happens most when              1\n",
        "then leads to                  1\n",
        "dilutes                        1\n",
        "can happendue to               1\n",
        ", which                        1\n",
        "and can make                   1\n",
        "was because of                 1\n",
        "resulted in                    1\n",
        "In these circumstances         1\n",
        "the stuff that                 1\n",
        "as a result                    1\n",
        "according                      1\n",
        "if there is                    1\n",
        "as the                         1\n",
        "and then                       1\n",
        "mess up                        1\n",
        "puts                           1\n",
        "due                            1\n",
        "which means                    1\n",
        "many reasons                   1\n",
        "threaten                       1\n",
        "which in the end               1\n",
        "is if                          1\n",
        "The higher                     1\n",
        "which then causes              1\n",
        "helps keep                     1\n",
        "while                          1\n",
        "depends of                     1\n",
        "in                             1\n",
        "damages                        1\n",
        "forced                         1\n",
        "changing                       1\n",
        "may contribute to              1\n",
        "causes this                    1\n",
        "can occur if                   1\n",
        "can be what the rates          1\n",
        "Ways                           1\n",
        "make the                       1\n",
        "can easly cause                1\n",
        "Along with                     1\n",
        "happens if                     1\n",
        "add                            1\n",
        "just because                   1\n",
        "As a result of                 1\n",
        "will also cause                1\n",
        "Because when                   1\n",
        "like                           1\n",
        "a cause of                     1\n",
        "happens because                1\n",
        "become                         1\n",
        "it'll cause                    1\n",
        "also a contribute to the causes 1\n",
        "are caused by                  1\n",
        "results to                     1\n",
        "will likely take place         1\n",
        "it will                        1\n",
        "disrupting                     1\n",
        "happens due to                 1\n",
        "may be                         1\n",
        "be                             1\n",
        "suring                         1\n",
        "so does                        1\n",
        "ends up going through          1\n",
        "may be a cause                 1\n",
        "could affect                   1\n",
        "can affect                     1\n",
        "drag                           1\n",
        "and because of                 1\n",
        "Make                           1\n",
        "which may                      1\n",
        "start                          1\n",
        "big factor                     1\n",
        "was                            1\n",
        "means they don't               1\n",
        "as another factor to           1\n",
        "distruption                    1\n",
        "threatens the corals health because 1\n",
        "happening                      1\n",
        "that is why                    1\n",
        "made                           1\n",
        "Under the circumstances        1\n",
        "can determine                  1\n",
        "vary are                       1\n",
        "bring in                       1\n",
        "which gets                     1\n",
        "may face                       1\n",
        "offset when                    1\n",
        "based off                      1\n",
        "what might be                  1\n",
        "Under these                    1\n",
        "force to                       1\n",
        "which caused                   1\n",
        "this disruption                1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train and Test Spelling Corrector\n",
      "================================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SpellingCorrector import SpellingCorrector \n",
      "corrector = SpellingCorrector(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corrections = defaultdict(int)\n",
      "for sentence in all_sentences:\n",
      "    for word in sentence:\n",
      "        wc = corrector.correct(word)\n",
      "        if word != wc:\n",
      "            corrections[(word,wc)] +=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "sort_by_value(corrections, reverse = True)[0:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(('Coral', 'coral'), 652),\n",
        " (('The', 'the'), 516),\n",
        " (('This', 'this'), 266),\n",
        " (('I', 'a'), 208),\n",
        " (('In', 'in'), 206),\n",
        " (('If', 'of'), 143),\n",
        " (('When', 'when'), 142),\n",
        " (('It', 'it'), 138),\n",
        " (('Pacific', 'pacific'), 128),\n",
        " (('Also', 'also'), 123),\n",
        " (('Corals', 'corals'), 90),\n",
        " (('So', 'to'), 88),\n",
        " (('Ocean', 'ocean'), 87),\n",
        " (('They', 'they'), 83),\n",
        " (('F', 'a'), 79),\n",
        " (('Bleaching', 'bleaching'), 77),\n",
        " ((\"it's\", 'its'), 72),\n",
        " (('Another', 'another'), 67),\n",
        " (('But', 'but'), 65),\n",
        " (('zox', 'zo'), 55),\n",
        " (('For', 'or'), 55),\n",
        " (('As', 'is'), 52),\n",
        " (('That', 'that'), 48),\n",
        " (('There', 'there'), 48),\n",
        " (('These', 'these'), 46)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Wordnet Synonyms\n",
      "========================"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "def convert2wn_pos(pos):\n",
      "    if pos.startswith(\"NN\"):\n",
      "        return wn.NOUN\n",
      "    elif pos.startswith(\"VB\"):\n",
      "        return wn.VERB\n",
      "    elif pos.startswith(\"JJ\"):\n",
      "        return wn.ADJ\n",
      "    elif pos.startswith(\"RB\"):\n",
      "        return wn.ADV\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "memoized = defaultdict(list)\n",
      "def __get_words_from_synset__(word, pos):\n",
      "    synsets = wn.synsets(word, pos=pos)\n",
      "    if len(synsets) == 0:\n",
      "        return [word]\n",
      "    \n",
      "    words = set()\n",
      "    for syn in synsets:\n",
      "        for lemma in syn.lemmas:\n",
      "            words.add(lemma.name)\n",
      "    return list(words)\n",
      "\n",
      "def get_word_synonyms(word, pos):\n",
      "    pair =  (word, pos)\n",
      "    if pair in memoized:\n",
      "        return memoized[pair]\n",
      "    \n",
      "    words = __get_words_from_synset__(word, pos)\n",
      "    memoized[pair] = words\n",
      "    return words\n",
      "    \n",
      "def get_synonyms(word, pos):\n",
      "    if pos is None:\n",
      "        return [word]\n",
      "    wn_pos = convert2wn_pos(pos)\n",
      "    if not wn_pos:\n",
      "        return [word]\n",
      "    return get_word_synonyms(word, wn_pos)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn_map2 = {}\n",
      "mapped = set()\n",
      "for sentence in all_sentences:\n",
      "    tags = nltk.pos_tag(sentence)\n",
      "    for wd, tag in tags:\n",
      "        pair = (wd, tag)\n",
      "        if pair in mapped:\n",
      "            continue\n",
      "        synonyms = [(s,tag) for s in get_synonyms(wd, tag) if s in wd_sent_freq]\n",
      "        if len(synonyms) >= 1:\n",
      "            matches = []\n",
      "            for spair in synonyms:\n",
      "                if spair in syn_map2:\n",
      "                    matches.append(syn_map2[spair])\n",
      "            if len(matches) == 0:\n",
      "                synset = set(synonyms)\n",
      "                synset.add(pair)\n",
      "                for p in synset:\n",
      "                    syn_map2[p] = synset                \n",
      "            elif len(matches) == 1:\n",
      "                matches[0].add(pair)\n",
      "                syn_map2[pair] = matches[0]\n",
      "            else:\n",
      "                #merge existing synonym lists\n",
      "                new_synset = set()\n",
      "                for m in matches:\n",
      "                    new_synset.update(m)\n",
      "                #update mapping to map to new larger set\n",
      "                for s in new_synset:\n",
      "                    syn_map2[s] = new_synset\n",
      "            mapped.add(pair)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'instancemethod' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-8ca879e9a2b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwd_sent_freq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-7e6194abea1d>\u001b[0m in \u001b[0;36mget_synonyms\u001b[0;34m(word, pos)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwn_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_word_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-8-7e6194abea1d>\u001b[0m in \u001b[0;36mget_word_synonyms\u001b[0;34m(word, pos)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmemoized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__get_words_from_synset__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmemoized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-7e6194abea1d>\u001b[0m in \u001b[0;36m__get_words_from_synset__\u001b[0;34m(word, pos)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'instancemethod' object is not iterable"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn_map2.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}