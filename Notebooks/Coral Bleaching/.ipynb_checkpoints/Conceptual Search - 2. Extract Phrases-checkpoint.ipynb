{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params\n",
    "ROOT_FOLDER = \"/Users/simon.hughes/Google Drive/PhD/Data/CoralBleaching/PhraseExtractionAnalysis\"\n",
    "\n",
    "DOCS_FOLDER = \"%s/ProcessedDocs\" % ROOT_FOLDER\n",
    "FILE_MASK = \".*\\.txt\"\n",
    "\n",
    "MIN_DOC_FREQ = 10\n",
    "MAX_PHRASE_LEN = 10\n",
    "STOP_WORDS_FILE = \"%s/en_stop_words.txt\" % ROOT_FOLDER\n",
    "PHRASES_FILE    = \"%s/Phrases.txt\" % ROOT_FOLDER\n",
    "PHRASE_FREQ_FILE = \"%s/phrase_freq.txt\" % ROOT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "full_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "re_collapse_spaces = re.compile(\"\\s+\")\n",
    "\n",
    "def collapse_spaces(s):\n",
    "    return re_collapse_spaces.sub(\" \", s).strip()\n",
    "\n",
    "re1 = re.compile(\"[;:\\'\\\"\\*/\\),\\(\\|\\s]+\")\n",
    "def clean_str(s):\n",
    "    s = str(s).replace(\"'s\",\" \")\n",
    "    #doesn't work in regex\n",
    "    s = s.replace(\"-\", \" \").replace(\"\\\\\",\" \")\n",
    "    s = re1.sub(\" \",s).strip()\n",
    "    return collapse_spaces(s)\n",
    "\n",
    "def compute_ngrams(tokens, max_len = None, min_len = 1):\n",
    "    \"\"\"\n",
    "    tokens  :   iterable of string\n",
    "                    a single sentence of tokens. Assumes start and stop tokens omitted\n",
    "    max_len :   int\n",
    "                    maximum ngram length\n",
    "    min_len :   int\n",
    "                    minimum ngram length\n",
    "\n",
    "    \"\"\"\n",
    "    if max_len == None:\n",
    "        max_len = len(tokens)\n",
    "\n",
    "    if min_len > max_len:\n",
    "        raise Exception(\"min_len cannot be more than max_len\")\n",
    "\n",
    "    ngrams = set()\n",
    "    # unigrams\n",
    "    for ngram_size in range(min_len, max_len + 1):\n",
    "        for start in range(0, len(tokens) - ngram_size + 1):\n",
    "            end = start + ngram_size -1\n",
    "            words = []\n",
    "            for i in range(start, end + 1):\n",
    "                words.append(tokens[i])\n",
    "            ngrams.add(tuple(words)) # make a tuple so hashable\n",
    "    return ngrams\n",
    "\n",
    "# is a valid token\n",
    "__bad_chars__ = \"<>{}[]~@\"\n",
    "__punct__ = set(\".?!,;:\")\n",
    "def is_valid_term(term):\n",
    "    # remove single char entries and only numeric\n",
    "    if len(term) == 0:\n",
    "        return False\n",
    "    if len(term) == 1:\n",
    "        #else misses c and r\n",
    "        if term.isalpha():\n",
    "            return True\n",
    "        return False\n",
    "    # no html or js terms\n",
    "    for c in __bad_chars__:\n",
    "        if c in term:\n",
    "            return False\n",
    "    if term[-1] in __punct__:\n",
    "        return False\n",
    "    if \"function(\" in term:\n",
    "        return False\n",
    "    if \"!\" in term or \"?\" in term:\n",
    "        return False\n",
    "    digit_chars = 0.0\n",
    "    for c in term:\n",
    "        if c.isdigit() or not c.isalnum():\n",
    "            digit_chars += 1.0\n",
    "    # 60% digits?\n",
    "    if (digit_chars / len(term)) >= 0.75:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "re1 = re.compile(\"[;:\\'\\\"\\*/\\),\\(\\-\\|\\s]+\")\n",
    "\n",
    "# we may want to keep some non-alpha characters, such as # in C# and + in C++, etc.\n",
    "def remove_punct(s):\n",
    "    s = s.replace(\"'s\",\" \")\n",
    "    return collapse_spaces(re1.sub(\" \",s).strip())\n",
    "\n",
    "def hash_string(s):\n",
    "    hash_object = hashlib.md5(b'%s' % s)\n",
    "    return str(hash_object.hexdigest())\n",
    "\n",
    "def find_files(folder, regex, remove_empty = False):\n",
    "    \"\"\"\n",
    "    Find all files matching the [regex] pattern in [folder]\n",
    "\n",
    "    folder  :   string\n",
    "                    folder to search (not recursive)\n",
    "    regex   :   string (NOT regex object)\n",
    "                    pattern to match\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder)\n",
    "    matches = [os.path.abspath(os.path.join(folder, f))\n",
    "               for f in files\n",
    "               if re.search(regex, f, re.IGNORECASE)]\n",
    "\n",
    "    if remove_empty:\n",
    "        matches = [f for f in matches if os.path.getsize(f) > 0]\n",
    "    matches.sort()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 stop words loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'all',\n",
       " 'along',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'between',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'each',\n",
       " 'either',\n",
       " 'for',\n",
       " 'from',\n",
       " 'has',\n",
       " 'have',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'infrequent',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'me',\n",
       " 'no',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'us',\n",
       " 'use',\n",
       " 'uses',\n",
       " 'was',\n",
       " 'were',\n",
       " 'when',\n",
       " 'which',\n",
       " 'will',\n",
       " 'with'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load stop words\n",
    "def load_stop_words(stop_words_file):\n",
    "    stop_words = set()\n",
    "    with open(stop_words_file) as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word[0] != \"#\":\n",
    "                    word = word.lower()\n",
    "                    stop_words.add(word)\n",
    "    return stop_words\n",
    "\n",
    "if STOP_WORDS_FILE:\n",
    "    stop_words = load_stop_words(STOP_WORDS_FILE)\n",
    "    print(\"%i stop words loaded\" % len(stop_words))\n",
    "else:\n",
    "    stop_words = set()\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093 files found in /Users/simon.hughes/Google Drive/PhD/Data/CoralBleaching/PhraseExtractionAnalysis/ProcessedDocs\n",
      "Loading 1093 documents took 0.246852874756 seconds\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "start = time.time()\n",
    "\n",
    "files = find_files(DOCS_FOLDER, FILE_MASK, True)\n",
    "print(\"%s files found in %s\" % (len(files), DOCS_FOLDER))\n",
    "documents = []\n",
    "for i, fname in enumerate(files):\n",
    "    with open(fname) as f:\n",
    "        contents = f.read()\n",
    "        documents.append(contents.split(\"\\n\"))\n",
    "end = time.time()\n",
    "print(\"Loading %i documents took %s seconds\" % (len(files), str(end - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Common Terms and Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.304957866669 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1090, 'docs', 1099, 'unique tokens')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Or use a counter here.\n",
    "doc_freq = defaultdict(int)\n",
    "\n",
    "# remove short docs\n",
    "tokenized_docs = []\n",
    "sent_id = 0\n",
    "sent_ids = set()\n",
    "lens = []\n",
    "hashed = set()\n",
    "\n",
    "for doc in documents:\n",
    "    un_tokens = set()\n",
    "    tok_sents = []\n",
    "    for sent in doc:\n",
    "        cl_sent = remove_punct(sent.lower())\n",
    "        hash_sent = hash_string(cl_sent)\n",
    "        # remove dupe sentences (not - will hurt df accuracy a little)\n",
    "        if hash_sent in hashed:\n",
    "            continue\n",
    "        hashed.add(hash_sent)\n",
    "\n",
    "        tokens = tuple(cl_sent.split(\" \"))\n",
    "        lens.append(len(tokens))\n",
    "        sent_id += 1\n",
    "        tok_sents.append((sent_id, tokens))\n",
    "        sent_ids.add(sent_id)\n",
    "        \n",
    "        # create inverted index and unique tokens (for doc freq calc)\n",
    "        proc_tokens = set()\n",
    "        for tok in tokens:\n",
    "            if not tok in proc_tokens:\n",
    "                proc_tokens.add(tok)\n",
    "                if not tok in un_tokens:\n",
    "                    un_tokens.add(tok)                    \n",
    "                    doc_freq[tok] += 1\n",
    "        \n",
    "    if len(tok_sents) > 0:\n",
    "        tokenized_docs.append(tok_sents)\n",
    "\n",
    "end = time.time()\n",
    "print end-start, \"secs\"\n",
    "len(tokenized_docs), \"docs\", len(doc_freq), \"unique tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719 frequent terms identified for building phrases\n"
     ]
    }
   ],
   "source": [
    "# Get really frequent items for removal\n",
    "num_docs = float(len(tokenized_docs))\n",
    "above_threshold = [k for k,v in doc_freq.items() if v >= MIN_DOC_FREQ]\n",
    "\n",
    "# remove really frequent terms (in 50% or more of documents)\n",
    "too_freq = set([k for k in above_threshold if (doc_freq[k]/num_docs) >= 0.70])\n",
    "\n",
    "freq_terms = [k for k in above_threshold \n",
    "              if  k not in stop_words and \n",
    "                  #k not in too_freq and #SH: REMOVES TOO MANY USEFUL TERMS\n",
    "                  is_valid_term(k)]\n",
    "print(\"%s frequent terms identified for building phrases\" % len(freq_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coral 1074\n",
      "bleaching 1000\n",
      "water 751\n",
      "corals 633\n",
      "algae 599\n",
      "trade 530\n",
      "winds 513\n",
      "temperature 512\n",
      "ocean 495\n",
      "photosynthesis 442\n",
      "color 430\n",
      "rates 411\n",
      "reefs 404\n",
      "white 369\n",
      "changes 365\n",
      "one 357\n",
      "zooxanthellae 343\n",
      "many 322\n",
      "pacific 312\n",
      "amount 311\n",
      "differences 307\n",
      "change 295\n",
      "different 291\n",
      "need 290\n",
      "most 276\n",
      "what 264\n",
      "sensitive 256\n",
      "environment 255\n",
      "leads 254\n",
      "process 254\n",
      "co2 253\n",
      "so 250\n",
      "live 248\n",
      "food 246\n",
      "waters 235\n",
      "temperatures 234\n",
      "healthy 234\n",
      "up 234\n",
      "bleached 233\n",
      "more 222\n",
      "polyps 215\n",
      "energy 215\n",
      "called 205\n",
      "stress 201\n",
      "salinity 199\n",
      "storms 196\n",
      "relationship 191\n",
      "things 191\n",
      "affect 190\n",
      "due 190\n",
      "animals 188\n",
      "shifting 188\n",
      "wind 187\n",
      "death 186\n",
      "another 184\n",
      "warm 182\n",
      "increase 182\n",
      "get 182\n",
      "like 179\n",
      "balance 177\n",
      "caused 173\n",
      "health 173\n",
      "loses 169\n",
      "carbon 168\n",
      "surface 168\n",
      "lead 168\n",
      "turn 167\n",
      "very 166\n",
      "stay 166\n",
      "reef 164\n",
      "makes 161\n",
      "drop 160\n",
      "during 159\n",
      "needs 158\n",
      "degrees 156\n",
      "phenomenon 155\n",
      "living 153\n",
      "tropical 153\n",
      "die 151\n",
      "survive 149\n",
      "found 149\n",
      "dioxide 149\n",
      "make 149\n",
      "f 148\n",
      "than 148\n",
      "problem 148\n",
      "example 145\n",
      "colors 145\n",
      "zooxanthallae 145\n",
      "made 145\n",
      "shallow 143\n",
      "about 139\n",
      "starvation 137\n",
      "increases 135\n",
      "sunlight 132\n",
      "bleach 131\n",
      "reason 131\n",
      "eject 131\n",
      "years 131\n",
      "year 130\n",
      "higher 125\n",
      "do 124\n",
      "east 124\n",
      "impact 122\n",
      "serious 120\n",
      "decreases 119\n",
      "why 119\n",
      "disease 119\n",
      "turns 119\n",
      "only 118\n",
      "being 118\n",
      "tiny 118\n",
      "decrease 117\n",
      "out 117\n",
      "west 117\n",
      "happens 117\n",
      "anchors 116\n",
      "damage 115\n",
      "types 115\n",
      "could 115\n",
      "vulnerable 114\n",
      "stressors 114\n",
      "thing 112\n",
      "regions 112\n",
      "every 112\n",
      "keep 112\n",
      "much 112\n",
      "environmental 111\n",
      "rate 111\n",
      "fishing 111\n",
      "normal 109\n",
      "ejection 109\n",
      "fresh 108\n",
      "enough 107\n",
      "blast 106\n",
      "you 106\n",
      "would 105\n",
      "weaker 105\n",
      "producing 103\n",
      "go 102\n",
      "plant 102\n",
      "important 101\n",
      "severe 100\n",
      "time 99\n",
      "become 97\n",
      "few 96\n",
      "lose 96\n",
      "people 96\n",
      "clear 95\n",
      "give 94\n",
      "eastern 94\n",
      "extreme 92\n",
      "high 92\n",
      "over 91\n",
      "where 91\n",
      "put 90\n",
      "we 90\n",
      "those 90\n",
      "oceans 89\n",
      "place 89\n",
      "salt 89\n",
      "been 89\n",
      "light 88\n",
      "enviroment 88\n",
      "climate 88\n",
      "may 87\n",
      "reasons 86\n",
      "body 85\n",
      "way 85\n",
      "colder 85\n",
      "event 85\n",
      "affects 85\n",
      "provides 85\n",
      "dies 85\n",
      "rise 84\n",
      "sea 84\n",
      "means 83\n",
      "rock 83\n",
      "direction 83\n",
      "effect 83\n",
      "delicate 83\n",
      "just 83\n",
      "dangerous 83\n",
      "really 83\n",
      "reports 82\n",
      "negatively 82\n",
      "since 81\n",
      "happen 81\n",
      "threaten 81\n",
      "building 81\n",
      "without 79\n",
      "however 79\n",
      "levels 79\n",
      "balanced 79\n",
      "big 78\n",
      "gives 77\n",
      "first 76\n",
      "both 76\n",
      "gets 76\n",
      "well 76\n",
      "major 75\n",
      "physical 75\n",
      "tourists 74\n",
      "who 74\n",
      "together 74\n",
      "difference 73\n",
      "think 73\n",
      "too 73\n",
      "off 73\n",
      "mistaken 73\n",
      "shift 72\n",
      "upwelling 72\n",
      "affected 71\n",
      "eastward 71\n",
      "bad 71\n",
      "article 70\n",
      "colonies 70\n",
      "lower 70\n",
      "salty 69\n",
      "occurs 69\n",
      "shows 69\n",
      "sometimes 69\n",
      "certain 69\n",
      "even 68\n",
      "countries 68\n",
      "around 68\n",
      "lives 68\n",
      "through 67\n",
      "weaken 67\n",
      "low 67\n",
      "according 66\n",
      "does 66\n",
      "down 65\n",
      "hard 64\n",
      "necessary 64\n",
      "nutrients 64\n",
      "ways 63\n",
      "world 63\n",
      "weak 62\n",
      "graph 61\n",
      "worst 61\n",
      "order 61\n",
      "forces 61\n",
      "walk 61\n",
      "plain 61\n",
      "tissues 61\n",
      "increased 60\n",
      "making 59\n",
      "something 59\n",
      "help 59\n",
      "upsets 59\n",
      "combine 59\n",
      "goes 58\n",
      "often 58\n",
      "provide 57\n",
      "leading 57\n",
      "cannot 57\n",
      "less 57\n",
      "within 57\n",
      "now 56\n",
      "sustain 56\n",
      "symbiotic 56\n",
      "temp 56\n",
      "mostly 56\n",
      "reverse 55\n",
      "events 55\n",
      "level 54\n",
      "varying 54\n",
      "know 54\n",
      "rely 54\n",
      "skeleton 53\n",
      "cold 53\n",
      "foot 53\n",
      "weather 53\n",
      "altered 53\n",
      "noticeable 51\n",
      "oxygen 51\n",
      "strength 51\n",
      "text 51\n",
      "had 51\n",
      "start 51\n",
      "results 50\n",
      "sun 50\n",
      "while 50\n",
      "rainfall 50\n",
      "factors 50\n",
      "85of 50\n",
      "used 50\n",
      "warmer 50\n",
      "required 50\n",
      "states 49\n",
      "main 49\n",
      "western 49\n",
      "see 49\n",
      "changing 49\n",
      "dying 49\n",
      "unhealthy 48\n",
      "movement 48\n",
      "humans 47\n",
      "alot 47\n",
      "benefit 47\n",
      "lot 47\n",
      "usually 46\n",
      "good 46\n",
      "result 46\n",
      "excessive 46\n",
      "hurricanes 46\n",
      "drag 46\n",
      "range 46\n",
      "animal 45\n",
      "threats 45\n",
      "getting 45\n",
      "nutrient 45\n",
      "occur 44\n",
      "destructive 44\n",
      "invertebrate 44\n",
      "build 44\n",
      "covers 44\n",
      "conclusion 44\n",
      "says 43\n",
      "ever 43\n",
      "type 43\n",
      "becomes 43\n",
      "effects 43\n",
      "chemicals 43\n",
      "westward 43\n",
      "factor 43\n",
      "same 43\n",
      "shifts 43\n",
      "blow 43\n",
      "entire 42\n",
      "might 42\n",
      "rises 42\n",
      "considered 42\n",
      "enviromental 42\n",
      "swell 42\n",
      "coastal 41\n",
      "globe 41\n",
      "undergo 41\n",
      "outside 41\n",
      "places 41\n",
      "said 41\n",
      "tend 41\n",
      "form 41\n",
      "reported 40\n",
      "worlds 40\n",
      "starts 40\n",
      "part 40\n",
      "recover 40\n",
      "near 40\n",
      "highest 40\n",
      "massive 39\n",
      "able 39\n",
      "happened 38\n",
      "needed 38\n",
      "tentacles 38\n",
      "walking 38\n",
      "practices 38\n",
      "central 38\n",
      "resulted 38\n",
      "atlantic 38\n",
      "known 37\n",
      "number 37\n",
      "rich 37\n",
      "puts 37\n",
      "happening 37\n",
      "completely 36\n",
      "stressed 36\n",
      "storm 36\n",
      "mouth 36\n",
      "going 36\n",
      "towards 36\n",
      "inside 36\n",
      "helps 35\n",
      "thats 35\n",
      "stinging 35\n",
      "surrounded 35\n",
      "temps 35\n",
      "size 35\n",
      "any 35\n",
      "graphs 35\n",
      "new 34\n",
      "diameter 34\n",
      "stated 34\n",
      "our 34\n",
      "discovered 34\n",
      "observed 34\n",
      "plants 34\n",
      "itself 34\n",
      "polyp 34\n",
      "increasing 33\n",
      "ejected 33\n",
      "marine 33\n",
      "protected 33\n",
      "come 33\n",
      "per 33\n",
      "after 32\n",
      "little 32\n",
      "limestone 32\n",
      "say 32\n",
      "america 32\n",
      "areas 32\n",
      "recieve 32\n",
      "human 32\n",
      "right 32\n",
      "depends 31\n",
      "85f 31\n",
      "drops 31\n",
      "reporting 31\n",
      "north 31\n",
      "believe 31\n",
      "died 31\n",
      "harm 30\n",
      "double 30\n",
      "dropping 30\n",
      "sugars 30\n",
      "life 30\n",
      "sugar 29\n",
      "therefore 29\n",
      "huge 29\n",
      "long 29\n",
      "passes 29\n",
      "basically 29\n",
      "stop 29\n",
      "though 29\n",
      "look 29\n",
      "two 29\n",
      "lack 28\n",
      "produce 28\n",
      "move 28\n",
      "glucose 28\n",
      "should 28\n",
      "alive 28\n",
      "specific 28\n",
      "under 27\n",
      "travel 27\n",
      "diseases 27\n",
      "take 27\n",
      "dragged 27\n",
      "show 27\n",
      "australia 27\n",
      "deeper 26\n",
      "others 26\n",
      "navigation 26\n",
      "contains 26\n",
      "must 26\n",
      "originally 26\n",
      "threatened 26\n",
      "threat 26\n",
      "south 26\n",
      "comes 26\n",
      "guide 26\n",
      "5of 26\n",
      "actually 25\n",
      "went 25\n",
      "begin 25\n",
      "h2o 25\n",
      "protects 25\n",
      "kill 25\n",
      "biologists 25\n",
      "losing 25\n",
      "still 24\n",
      "depending 24\n",
      "80of 24\n",
      "source 24\n",
      "changed 24\n",
      "fish 24\n",
      "passed 24\n",
      "pass 23\n",
      "got 23\n",
      "looses 23\n",
      "ejects 23\n",
      "inches 23\n",
      "upset 23\n",
      "guinea 23\n",
      "boat 23\n",
      "second 23\n",
      "protect 23\n",
      "effected 23\n",
      "once 23\n",
      "small 22\n",
      "damaged 22\n",
      "seawater 22\n",
      "trading 22\n",
      "finally 22\n",
      "background 22\n",
      "examples 22\n",
      "3of 22\n",
      "zooxanthelle 22\n",
      "dead 22\n",
      "maybe 22\n",
      "amounts 22\n",
      "role 22\n",
      "problems 22\n",
      "merchant 21\n",
      "takes 21\n",
      "my 21\n",
      "boats 21\n",
      "grow 21\n",
      "turning 21\n",
      "benefits 21\n",
      "fact 21\n",
      "explain 21\n",
      "affecting 21\n",
      "eventually 21\n",
      "threatens 21\n",
      "cooler 21\n",
      "hot 20\n",
      "natural 20\n",
      "later 20\n",
      "mainly 20\n",
      "almost 20\n",
      "perform 20\n",
      "meaning 20\n",
      "loss 20\n",
      "although 20\n",
      "eat 20\n",
      "case 19\n",
      "likely 19\n",
      "degree 19\n",
      "strong 19\n",
      "negative 19\n",
      "next 19\n",
      "document 19\n",
      "allow 19\n",
      "extremely 19\n",
      "above 19\n",
      "create 19\n",
      "area 19\n",
      "depend 18\n",
      "passage 18\n",
      "loose 18\n",
      "directions 18\n",
      "beautiful 18\n",
      "several 18\n",
      "wich 18\n",
      "population 18\n",
      "starve 18\n",
      "include 17\n",
      "did 17\n",
      "tissue 17\n",
      "figure 17\n",
      "worse 17\n",
      "instead 17\n",
      "information 17\n",
      "alter 17\n",
      "combination 17\n",
      "always 17\n",
      "decreasing 17\n",
      "shown 17\n",
      "done 17\n",
      "tourist 17\n",
      "chart 17\n",
      "bring 17\n",
      "colorful 17\n",
      "trades 17\n",
      "else 17\n",
      "receive 17\n",
      "papua 17\n",
      "having 16\n",
      "large 16\n",
      "zooanthellae 16\n",
      "decreased 16\n",
      "formation 16\n",
      "ran 16\n",
      "find 16\n",
      "probably 16\n",
      "rose 16\n",
      "bottom 16\n",
      "o2 16\n",
      "own 16\n",
      "play 16\n",
      "mild 15\n",
      "anywhere 15\n",
      "giving 15\n",
      "scientists 15\n",
      "end 15\n",
      "overall 15\n",
      "data 15\n",
      "side 15\n",
      "mean 15\n",
      "harmful 15\n",
      "occuring 15\n",
      "report 15\n",
      "nature 15\n",
      "conditions 15\n",
      "rain 15\n",
      "relies 15\n",
      "eachother 15\n",
      "biggest 14\n",
      "survival 14\n",
      "until 14\n",
      "hurt 14\n",
      "dropped 14\n",
      "times 14\n",
      "ones 14\n",
      "keeps 14\n",
      "produces 14\n",
      "documents 14\n",
      "anchor 14\n",
      "moving 14\n",
      "feed 14\n",
      "seen 14\n",
      "especially 14\n",
      "based 14\n",
      "reading 14\n",
      "common 14\n",
      "skeletons 14\n",
      "cant 14\n",
      "read 14\n",
      "lost 14\n",
      "lots 14\n",
      "back 14\n",
      "recognized 14\n",
      "leave 14\n",
      "hotter 14\n",
      "moves 14\n",
      "sick 14\n",
      "land 14\n",
      "plays 13\n",
      "asia 13\n",
      "cases 13\n",
      "work 13\n",
      "pretty 13\n",
      "versus 13\n",
      "least 13\n",
      "drastically 13\n",
      "purpose 13\n",
      "away 13\n",
      "various 13\n",
      "before 13\n",
      "thrive 13\n",
      "killing 13\n",
      "last 13\n",
      "protection 13\n",
      "occured 13\n",
      "left 13\n",
      "location 13\n",
      "kills 13\n",
      "couple 13\n",
      "lastly 13\n",
      "10of 13\n",
      "risk 12\n",
      "pool 12\n",
      "creatures 12\n",
      "learned 12\n",
      "leaving 12\n",
      "underwater 12\n",
      "killed 12\n",
      "noticable 12\n",
      "impacts 12\n",
      "finish 12\n",
      "zooxanthalle 12\n",
      "seem 12\n",
      "nothing 12\n",
      "region 12\n",
      "conclude 12\n",
      "seems 12\n",
      "throughout 12\n",
      "weakened 12\n",
      "provided 12\n",
      "gone 12\n",
      "partly 12\n",
      "bright 11\n",
      "let 11\n",
      "danger 11\n",
      "started 11\n",
      "greatly 11\n",
      "wrong 11\n",
      "located 11\n",
      "rocks 11\n",
      "ejecting 11\n",
      "costal 11\n",
      "articles 11\n",
      "damaging 11\n",
      "forced 11\n",
      "self 11\n",
      "pinhead 11\n",
      "stepping 11\n",
      "close 11\n",
      "biologist 11\n",
      "versa 11\n",
      "possible 11\n",
      "hurricane 11\n",
      "everything 11\n",
      "moderate 11\n",
      "phenomena 11\n",
      "heat 11\n",
      "issue 11\n",
      "multiple 10\n",
      "paragraph 10\n",
      "leaves 10\n",
      "resulting 10\n",
      "sudden 10\n",
      "messes 10\n",
      "red 10\n",
      "estimate 10\n",
      "given 10\n",
      "showed 10\n",
      "final 10\n",
      "matter 10\n",
      "better 10\n",
      "equatorial 10\n",
      "weakest 10\n",
      "unbalanced 10\n",
      "circumstances 10\n",
      "etc 10\n",
      "add 10\n",
      "possibly 10\n",
      "deal 10\n",
      "home 10\n",
      "charts 10\n",
      "noticed 10\n",
      "lowest 10\n",
      "becoming 10\n",
      "tradewinds 10\n",
      "deaths 10\n",
      "int 10\n",
      "normally 10\n"
     ]
    }
   ],
   "source": [
    "for t in sorted(freq_terms, key = lambda t: -doc_freq[t]):\n",
    "    print t, doc_freq[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase_len 2\n",
      "1090 docs 719 terms 9203 sentences\n",
      "\t1127 phrases found\n",
      "\ttook 0.277586936951 seconds\n",
      "phrase_len 3\n",
      "1089 docs 339 terms 7366 sentences\n",
      "\t1265 phrases found\n",
      "\ttook 0.0694930553436 seconds\n",
      "phrase_len 4\n",
      "1020 docs 167 terms 2321 sentences\n",
      "\t1306 phrases found\n",
      "\ttook 0.0194158554077 seconds\n",
      "phrase_len 5\n",
      "575 docs 76 terms 591 sentences\n",
      "\t1322 phrases found\n",
      "\ttook 0.00446677207947 seconds\n",
      "phrase_len 6\n",
      "193 docs 40 terms 199 sentences\n",
      "\t1328 phrases found\n",
      "\ttook 0.00134992599487 seconds\n",
      "phrase_len 7\n",
      "104 docs 27 terms 103 sentences\n",
      "\n",
      "took 0.387850999832 seconds\n",
      "1328 phrases found\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Find all phrases up to length MAX_PHRASE_LEN at or above the defined MIN_DOC_FREQ above\n",
    "phrase_doc_freq = defaultdict(int)\n",
    "for term in freq_terms:\n",
    "    phrase_doc_freq[tuple([term])] = doc_freq[term]\n",
    "    \n",
    "# data structure is a list of list (document) of pairs - sentences: (int, list  (of tokens))\n",
    "# each item is a doc, a list of sents. each sent is a list of valid remaining phrases\n",
    "# seed with one valid phrase per sent\n",
    "\n",
    "#working_docs = [map(lambda sent: [sent], d) for d in tokenized_docs]\n",
    "working_docs = [map(lambda (sid, sent): (sid, [sent]), d) for d in tokenized_docs]\n",
    "working_freq_terms = set(freq_terms)\n",
    "\n",
    "# sentences with one or more phrases that are frequent enough (under the apriori algorithm closure priniciple)\n",
    "working_sent_ids = set(sent_ids)\n",
    "# don't bother whitling this down further at the start, almost all sentences have at least on freq term in them\n",
    "\n",
    "for phrase_len in range(2, MAX_PHRASE_LEN + 1):\n",
    "    phrase_start = time.time()\n",
    "    print \"phrase_len\", phrase_len\n",
    "    print len(working_docs), \"docs\", len(working_freq_terms), \"terms\", len(working_sent_ids), \"sentences\"\n",
    "    # for current phrase_len\n",
    "    current_phrase_doc_freq = defaultdict(int)\n",
    "    \n",
    "    # used to look up sentence ids by phrase\n",
    "    phrase2sentids = defaultdict(set)\n",
    "    \n",
    "    new_work_docs = []\n",
    "    for doc in working_docs:\n",
    "        new_work_sents = []\n",
    "        unique_potential_phrases = set()\n",
    "        for sent_id, phrases in doc:\n",
    "            if sent_id not in working_sent_ids:\n",
    "                continue\n",
    "            \n",
    "            new_work_phrases = []\n",
    "            for phrase in phrases:\n",
    "                current_phrase = []\n",
    "                for term in phrase:\n",
    "                    if term in working_freq_terms:\n",
    "                        current_phrase.append(term)\n",
    "                    else:\n",
    "                        if len(current_phrase) >= phrase_len:\n",
    "                            new_work_phrases.append(current_phrase)\n",
    "                        current_phrase = []\n",
    "                \n",
    "                if len(current_phrase) >= phrase_len:\n",
    "                    new_work_phrases.append(current_phrase)\n",
    "            \n",
    "            if len(new_work_phrases) > 0:\n",
    "                for phrase in new_work_phrases:\n",
    "                    ngrams = compute_ngrams(phrase, phrase_len, phrase_len)\n",
    "                    for tpl_ngram in ngrams:                        \n",
    "                        unique_potential_phrases.add(tpl_ngram)\n",
    "                        phrase2sentids[tpl_ngram].add(sent_id)\n",
    "                        \n",
    "                new_work_sents.append((sent_id, new_work_phrases))\n",
    "        \n",
    "        # end for sent in doc\n",
    "        # for doc freq, need to compute unique phrases in document\n",
    "        for unique_tpl_phrase in unique_potential_phrases:\n",
    "            current_phrase_doc_freq[unique_tpl_phrase] +=1\n",
    "        \n",
    "        if len(new_work_sents) > 0:\n",
    "            new_work_docs.append(new_work_sents)\n",
    "\n",
    "    new_working_freq_terms = set()\n",
    "    new_working_sent_ids = set()\n",
    "    for tuple_phrase, freq in current_phrase_doc_freq.items():\n",
    "        if freq < MIN_DOC_FREQ:\n",
    "            continue            \n",
    "        phrase_doc_freq[tuple_phrase] = freq\n",
    "        new_working_sent_ids.update(phrase2sentids[tuple_phrase])\n",
    "        for tok in tuple_phrase:\n",
    "            new_working_freq_terms.add(tok)\n",
    "    \n",
    "    if len(new_working_freq_terms) <= 1 or len(new_work_docs) == 0 or len(new_working_sent_ids) == 0:\n",
    "        break\n",
    "    working_docs = new_work_docs\n",
    "    working_freq_terms = new_working_freq_terms\n",
    "    working_sent_ids = new_working_sent_ids\n",
    "    phrase_end = time.time()\n",
    "    print \"\\t\", len(phrase_doc_freq), \"phrases found\"\n",
    "    print \"\\ttook\", phrase_end - phrase_start, \"seconds\"\n",
    "    \n",
    "print \"\"\n",
    "end = time.time()\n",
    "print \"took\", end - start, \"seconds\"\n",
    "print len(phrase_doc_freq), \"phrases found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Sub-Phrases that Are Of Near Equivalent Frequencies and REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are a lot of short phrases that always or nearly always have the same DF as longer phrases that they constitute\n",
    "\n",
    "def find_sub_phrases_to_remove(tpl_phrase, valid_phrases, doc_freq, to_rem):\n",
    "    if len(tpl_phrase) <= 1:\n",
    "        return\n",
    "    phrase_df = doc_freq[tpl_phrase]\n",
    "    ngrams = compute_ngrams(tpl_phrase, len(tpl_phrase)-1, 1)\n",
    "    for tpl_ngram in ngrams:\n",
    "        if tpl_ngram in valid_phrases and tpl_ngram not in to_rem:\n",
    "            sub_phr_df = doc_freq[tpl_ngram]\n",
    "            # if sub_phrase_df is close to the same frequency\n",
    "            if phrase_df >= (0.9 * sub_phr_df):\n",
    "                to_rem.add(tpl_ngram)\n",
    "                #to_rem_dbg.add((tpl_phrase, tpl_ngram, phrase_df, sub_phr_df))\n",
    "                find_sub_phrases_to_remove(tpl_ngram, valid_phrases, doc_freq, to_rem)\n",
    "\n",
    "# don't process unigrams\n",
    "valid_phrases = set(phrase_doc_freq.keys())\n",
    "phrases = filter(lambda k: len(k) > 1, phrase_doc_freq.keys())\n",
    "to_remove = set()\n",
    "\n",
    "for tpl_key in sorted(phrases, key = lambda k: -len(k)):\n",
    "    if tpl_key not in to_remove:\n",
    "        phrase_df = phrase_doc_freq[tpl_key]\n",
    "        # find all shorter sub-phrases\n",
    "        find_sub_phrases_to_remove(tpl_key, valid_phrases, phrase_doc_freq, to_remove)\n",
    "\n",
    "len(to_remove)#, len(to_remove_dbg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole process took 1.25229501724 seconds\n"
     ]
    }
   ],
   "source": [
    "#Dump phrases to file\n",
    "with open(PHRASES_FILE, \"w+\") as f:\n",
    "    for tpl in sorted(phrase_doc_freq.keys()):\n",
    "        # phrases only\n",
    "        if tpl not in to_remove:\n",
    "            joined = \" \".join(tpl)\n",
    "            f.write(joined + \"\\n\")\n",
    "    \n",
    "full_end = time.time()\n",
    "print(\"Whole process took %s seconds\" % str(full_end - full_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(PHRASE_FREQ_FILE, \"w+\") as f:\n",
    "    for tpl,v in sorted(phrase_doc_freq.items(), key = lambda (k,v): -v):\n",
    "        # phrases only\n",
    "        if tpl not in to_remove:\n",
    "            joined = \" \".join(tpl)\n",
    "            f.write(joined + \",\" + str(v) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reef', 'building', 'corals', 'need', 'water', 'temperatures') 37\n",
      "('winds', 'drag', 'warm', 'surface', 'waters', 'westward') 10\n",
      "('reef', 'building', 'corals', 'need', 'water', 'temperature') 11\n",
      "('during', 'bleaching', 'corals', 'turn', 'white', 'due') 23\n",
      "('blast', 'fishing', 'tourists', 'who', 'drop', 'anchors') 12\n",
      "('surface', 'waters', 'shift', 'eastward', 'ocean', 'water') 10\n"
     ]
    }
   ],
   "source": [
    "for k,v in phrase_doc_freq.items():\n",
    "    if len(k) == 6:\n",
    "        print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
