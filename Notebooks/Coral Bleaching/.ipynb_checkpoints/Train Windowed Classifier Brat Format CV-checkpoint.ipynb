{
 "metadata": {
  "name": "",
  "signature": "sha256:86d67402aad63df86791b78e582aca5c94c08255d868eea076bf95c271ca41df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa, mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "    *** replace low frequency words with either UNKNOWN or predicted POS ***\n",
      "\"\"\"\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 7 #7 is best\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 2\n",
      "PCT_VALIDATION  = 0.2\n",
      "MIN_FEAT_FREQ = 5     #15 best so far\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\"\n",
      "STEM = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "CAUSAL_REL   = \"CRel\"\n",
      "RESULT_REL   = \"RRel\"\n",
      "CAUSE_RESULT = \"C->R\"\n",
      "\n",
      "cr_codes = [CAUSAL_REL, RESULT_REL, CAUSE_RESULT]\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            elif w == \"&\":\n",
      "                cw = \"and\"\n",
      "            else:\n",
      "                cw = corrector.correct(w)\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "                sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "297 files found\n",
        "297 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "VALID_CHARS = {\".\", \"?\", \"!\", \"=\", \"/\", \":\", \";\", \"&\", \"+\",  \"-\", \"=\",  \"%\", \"'\", \",\", \"\\\\\", \"(\", \")\", \"\\\"\"}\n",
      "\"\"\" Remove bad chars (see above - e.g. '\\x93') \"\"\"\n",
      "removed = set()\n",
      "def valid_wd(wd):\n",
      "    wd = wd.strip()\n",
      "    if len(wd) != 1:\n",
      "        return True\n",
      "    if wd in removed:\n",
      "        return False\n",
      "    if wd.isalpha() or wd.isdigit() or wd in VALID_CHARS:\n",
      "        return True\n",
      "    removed.add(wd)\n",
      "    return False\n",
      "    \n",
      "def filterout_punctuation(sentence):\n",
      "    return filter(lambda (w, tags4word): valid_wd(w), sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "ix2sentTags = {}\n",
      "\n",
      "sentences = []\n",
      "tokenized_sentences = []\n",
      "\n",
      "i = 0\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        modified_sentence = filterout_punctuation(modified_sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size= WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        # tagged words\n",
      "        sentences.append(sentence)\n",
      "        # words only\n",
      "        wds, tags = zip(*sentence)\n",
      "        tokenized_sentences.append(wds)\n",
      "        ix2sentTags[i] = set(flatten(tags))\n",
      "    \n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        i += 1\n",
      "        \n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Removed Characters\n",
      "------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(removed))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract Features \"\"\"\n",
      "from WindowFeatures import extract_positional_word_features, extract_word_features\n",
      "from NgramGenerator import compute_ngrams\n",
      "\n",
      "def extract_positional_bigram_features(window, mid_ix, feature_val = 1):\n",
      "    bi_grams = compute_ngrams(window, max_len = 2, min_len = 2)\n",
      "    d = {}\n",
      "    for i, bi_gram in enumerate(bi_grams):\n",
      "        d[\"BI\" + \":\" + str(-mid_ix + i) + \" \" + bi_gram[0] + \" | \" + bi_gram[1]] = feature_val\n",
      "    return d\n",
      "\n",
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "idgen = IdGenerator()\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def extract_features(words):\n",
      "    \n",
      "    if STEM:\n",
      "        words = [stemmer.stem(w) for w in words]\n",
      "    \n",
      "    #Extract features for words\n",
      "    features = {}\n",
      "    pos_features = extract_positional_word_features(words, MID_IX, feature_val=1)    \n",
      "    word_features  = extract_word_features(words, feature_val=1)\n",
      "    pos_bi_grams = extract_positional_bigram_features(words, MID_IX, feature_val = 1)\n",
      "    \n",
      "    features.update(pos_features)\n",
      "    features.update(word_features)\n",
      "    features.update(pos_bi_grams)\n",
      "    return features\n",
      "\n",
      "def extract_ys_by_code(tags, ysByCode):\n",
      "    for code in all_codes:\n",
      "        ysByCode[code].append(1 if code in tags else 0 )\n",
      "    \n",
      "    ysByCode[CAUSAL_REL].append(  1 if  \"Causer\" in tags and \"explicit\" in tags else 0)\n",
      "    ysByCode[RESULT_REL].append(  1 if  \"Result\" in tags and \"explicit\" in tags else 0)\n",
      "    ysByCode[CAUSE_RESULT].append(1 if (\"Result\" in tags and \"explicit\" in tags and \"Causer\" in tags) else 0)\n",
      "\n",
      "def sentence2feats(sentence):\n",
      "    d = {}\n",
      "    for wd in sentence:\n",
      "        d[\"BOW:\" + wd] = 1\n",
      "    return d\n",
      "    \n",
      "ix2ys = {}\n",
      "ix2feats = {}\n",
      "feat_counts = defaultdict(int)\n",
      "def tally_features(feats):\n",
      "    for k,v in feats.items():\n",
      "        feat_counts[k] += 1\n",
      "\n",
      "for i, windows in ix2windows.items():\n",
      "    feats = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    \n",
      "    ix2feats[i] = feats\n",
      "    ix2ys[i] = ysByCode\n",
      "    \n",
      "    #bow = sentence2feats(tokenized_sentences[i])\n",
      "    for window in windows:\n",
      "        # Get the words minus tags\n",
      "        words, tags = zip(*window)                \n",
      "        feat = extract_features(words)\n",
      "        # Add bow for sentence\n",
      "        #feat.update(bow)\n",
      "\n",
      "        tally_features(feat)\n",
      "        feats.append(feat.items())\n",
      "        \n",
      "        #Tags for middle word (target)\n",
      "        tags4word = tags[MID_IX]\n",
      "        extract_ys_by_code(tags4word, ysByCode)\n",
      "    assert len(windows) == len(feats)\n",
      "    assert all(map(lambda (k,v): len(v) == len(feats), ysByCode.items()))\n",
      "        \n",
      "\"\"\" Convert sparse dictionary features to sparse arrays \"\"\"\n",
      "ix2xs = {}\n",
      "for i, feature_lists in ix2feats.items():\n",
      "    xs = []\n",
      "    ix2xs[i] = xs\n",
      "    for feats in feature_lists:\n",
      "        x = [(idgen.get_id(f),v) \n",
      "             for f,v in feats \n",
      "             if feat_counts[f] >= MIN_FEAT_FREQ or f.startswith(\"WD:0\" )] #above min freq or is word\n",
      "        xs.append(x)        \n",
      "\n",
      "num_features = idgen.max_id() + 1\n",
      "print \"Number of features:\", num_features\n",
      "\n",
      "\"\"\" Convert to dense numpy arrays \"\"\"\n",
      "for i in ix2xs.keys():\n",
      "    xs = ix2xs[i]\n",
      "    xs = np.array([matutils.sparse2full(x, num_features) for x in xs])        \n",
      "    ix2xs[i] = xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 14183\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "\n",
      "def count_above(ft_counts, threshold):\n",
      "    above = [ v for k,v in ft_counts.items() if v >= threshold]\n",
      "    return (len(above), len(ft_counts))\n",
      "\n",
      "cnt_above, cnt_all = count_above(feat_counts, MIN_FEAT_FREQ)\n",
      "\n",
      "print \"Counts\"\n",
      "print \"all:     \", cnt_all\n",
      "print \"above:   \", cnt_above\n",
      "print \"% above: \", str(100.0 * cnt_above / float(cnt_all))+ \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counts\n",
        "all:      68809\n",
        "above:    13879\n",
        "% above:  20.1703265561%\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract ys for sentence (including causal codes) \"\"\"\n",
      "ix2ys_sent = {}\n",
      "for i, tags in ix2sentTags.items():\n",
      "    ix2ys_sent[i] = defaultdict(list)\n",
      "    extract_ys_by_code(tags, ix2ys_sent[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Create Data Using Previous Classifier \"\"\"\n",
      "\n",
      "def to_sentence_level_predictions(ix2xs, code2cls, codes):\n",
      "    ix2newxs = {}\n",
      "    \n",
      "    for i, xs in ix2xs.items():\n",
      "        tmp_xs = []\n",
      "        tmp_ys = []\n",
      "        tmp_ys_by_code = defaultdict(list)\n",
      "\n",
      "        un_codes = set()\n",
      "        un_pred_codes = set()\n",
      "        for code in codes:\n",
      "            cls = code2cls[code]\n",
      "\n",
      "            #SVM\n",
      "            pred = cls.decision_function(xs)\n",
      "            \n",
      "            #Probabilistic classifier\n",
      "            #pred = cls.predict_proba(xs)\n",
      "            # add min and max values\n",
      "            mx = np.max(pred, axis=0)\n",
      "            mn = np.min(pred, axis=0)\n",
      "        \n",
      "            tmp_xs.append(mx)\n",
      "            tmp_xs.append(mn)\n",
      "            #for val in mx:\n",
      "                #tmp_xs.append(val)\n",
      "            #tmp_xs.append(mx[-1])\n",
      "            \n",
      "            #for val in mn:\n",
      "                #tmp_xs.append(val)\n",
      "            #tmp_xs.append(mn[-1])\n",
      "\n",
      "            yes_no = np.max(cls.predict(xs))\n",
      "            tmp_xs.append(yes_no)\n",
      "\n",
      "            if yes_no > 0.0:\n",
      "                un_pred_codes.add(code)\n",
      "\n",
      "        #add 2 way feature combos\n",
      "        for a in all_codes:\n",
      "            for b in all_codes:\n",
      "                if b < a:\n",
      "                    if a in un_pred_codes and b in un_pred_codes:\n",
      "                        tmp_xs.append(1)\n",
      "                    else:\n",
      "                        tmp_xs.append(0)\n",
      "\n",
      "        ix2newxs[i] = np.array([tmp_xs])\n",
      "    return ix2newxs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Get sentence level classification performance \"\"\"\n",
      "def test_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    cls = codeToClassifier[code]\n",
      "    \n",
      "    act_ys  = []\n",
      "    pred_ys = []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys.append(max(ys))\n",
      "        pred_ys.append(max(pred))\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "def test(codes, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    td_metrics = []\n",
      "    for c in codes:\n",
      "        cls = codeToClassifier[c]\n",
      "        td_metrics.append(test_for_code(c, ixs, ixToXs, ixToYs, codeToClassifier))\n",
      "    td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
      "    print type(cls), td_wt_mn_prfa\n",
      "    return td_wt_mn_prfa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_xs_ys(ixs, ixTOxs, ixTOys, codes):\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    for i in ixs:\n",
      "        xs_tmp = ixTOxs[i]\n",
      "        xs.extend(xs_tmp)\n",
      "        ysByCode_tmp = ixTOys[i]\n",
      "        for code in codes:\n",
      "            ysByCode[code].extend(ysByCode_tmp[code])\n",
      "    return (np.array(xs), ysByCode)\n",
      "\n",
      "def train(codes, xs, yByCode, fn_create_cls):\n",
      "    code2classifier = {}\n",
      "    for code in codes:\n",
      "        print \"Training for :\", code   \n",
      "        cls = fn_create_cls()\n",
      "        code2classifier[code] = cls\n",
      "        ys = np.asarray(yByCode[code])\n",
      "        cls.fit(xs, ys)\n",
      "    return code2classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from CrossValidation import cross_validation\n",
      "\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "#fn_classifier1 = LinearSVC\n",
      "fn_classifier1 = LogisticRegression\n",
      "fn_classifier2 = LogisticRegression\n",
      "\n",
      "SPLITS = 10\n",
      "#causal_codes = cr_codes + [\"explicit\"]\n",
      "causal_codes = [CAUSE_RESULT]\n",
      "\n",
      "ixs = range(len(sentences))\n",
      "np.random.shuffle(ixs)\n",
      "\n",
      "folds = cross_validation(ixs, SPLITS)\n",
      "win_td_metrics = []\n",
      "win_vd_metrics = []\n",
      "\n",
      "td_metrics = []\n",
      "vd_metrics = []\n",
      "\n",
      "#TODO try with these codes (it and other) but for 10 or more folds\n",
      "train_codes = [c for c in all_codes if c != \"it\"]\n",
      "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "for num, (ix_train, ix_valid) in enumerate(folds):\n",
      "    print \"Fold:\", num + 1\n",
      "    \n",
      "    # Train sequential classifier\n",
      "    xs_t, yByCode_t = extract_xs_ys(ix_train, ix2xs, ix2ys, all_codes)\n",
      "    \n",
      "    print \"Training Window Tagger\"\n",
      "    code2cls = train(train_codes, xs_t, yByCode_t, fn_classifier1)\n",
      "    print \"Training performance\"\n",
      "    win_td_metrics.append(test(reg_codes, ix_train, ix2xs, ix2ys, code2cls))\n",
      "    print \"Validation performance\"\n",
      "    win_vd_metrics.append(test(reg_codes, ix_valid, ix2xs, ix2ys, code2cls))\n",
      "    \n",
      "    print \"Training Sentence Classifier\"\n",
      "    # Extract new data points and target classes\n",
      "    ix2xs_sent = to_sentence_level_predictions(ix2xs, code2cls, train_codes)\n",
      "    newxs_t, newyByCode_t = extract_xs_ys(ix_train, ix2xs_sent, ix2ys_sent, causal_codes)\n",
      "    \n",
      "    new_code2cls = train(causal_codes, newxs_t, newyByCode_t, fn_classifier2)\n",
      "    # Evaluate\n",
      "    print \"Training performance\"\n",
      "    td_metrics.append(test(causal_codes, ix_train, ix2xs_sent, ix2ys_sent, new_code2cls))\n",
      "    print \"Validation performance\"\n",
      "    vd_metrics.append(test(causal_codes, ix_valid, ix2xs_sent, ix2ys_sent, new_code2cls))\n",
      "\n",
      "print \"\\NFinished\\n\"\n",
      "print \"Word Tagger:\"\n",
      "print \"MEAN Test Performance      \", mean_rpfa(win_td_metrics)\n",
      "print \"MEAN Validation Performance\", mean_rpfa(win_vd_metrics)\n",
      "\n",
      "print \"Sentence Classifier:\"\n",
      "print \"MEAN Test Performance      \", mean_rpfa(td_metrics)\n",
      "print \"MEAN Validation Performance\", mean_rpfa(vd_metrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fold: 1\n",
        "Training Window Tagger"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for : Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training performance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9012, Precision: 0.9371, F1: 0.9146, Accuracy: 0.9706, Codes:  2853\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.7500, Precision: 0.7643, F1: 0.7467, Accuracy: 0.9276, Codes:   296\n",
        "Training Sentence Classifier\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9415, Precision: 0.9324, F1: 0.9370, Accuracy: 0.9740, Codes:   513\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.5357, Precision: 0.6000, F1: 0.5660, Accuracy: 0.8345, Codes:    56\n",
        "Fold: 2\n",
        "Training Window Tagger"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for : Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training performance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.8988, Precision: 0.9377, F1: 0.9138, Accuracy: 0.9708, Codes:  2856\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.7577, Precision: 0.7987, F1: 0.7709, Accuracy: 0.9287, Codes:   293\n",
        "Training Sentence Classifier\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9353, Precision: 0.9226, F1: 0.9289, Accuracy: 0.9708, Codes:   510\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.6441, Precision: 0.8444, F1: 0.7308, Accuracy: 0.8993, Codes:    59\n",
        "Fold: 3\n",
        "Training Window Tagger"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for : Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training performance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9031, Precision: 0.9388, F1: 0.9168, Accuracy: 0.9714, Codes:  2878\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.7712, Precision: 0.7990, F1: 0.7747, Accuracy: 0.9382, Codes:   271\n",
        "Training Sentence Classifier\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9331, Precision: 0.9313, F1: 0.9322, Accuracy: 0.9716, Codes:   523\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.5000, Precision: 0.6389, F1: 0.5610, Accuracy: 0.8705, Codes:    46\n",
        "Fold: 4\n",
        "Training Window Tagger"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for : Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training performance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9023, Precision: 0.9391, F1: 0.9164, Accuracy: 0.9716, Codes:  2826\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.7616, Precision: 0.8133, F1: 0.7770, Accuracy: 0.9268, Codes:   323\n",
        "Training Sentence Classifier\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Recall: 0.9337, Precision: 0.9265, F1: 0.9301, Accuracy: 0.9712, Codes:   513\n",
        "Validation performance\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.5357, Precision: 0.6977, F1: 0.6061, Accuracy: 0.8597, Codes:    56\n",
        "Fold: 5\n",
        "Training Window Tagger"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training for : Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "Training for :"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l = []\n",
      "code = \"CRel\"\n",
      "for i in ix2ys_sent.keys()[:400]:\n",
      "    l.append(ix2ys_sent[i][code][0])\n",
      "print min(l), max(l), len([1 for i in l if i > 0.0]), len(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Logistic Regression - Min and Max + 2 way features\n",
      "\n",
      "Word Tagger:\n",
      "MEAN Test Performance       Recall: 0.9058, Precision: 0.9513, F1: 0.9240, Accuracy: 0.9734, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.7663, Precision: 0.8257, F1: 0.7840, Accuracy: 0.9333, Codes:    10\n",
      "Sentence Classifier:\n",
      "MEAN Test Performance       Recall: 0.9021, Precision: 0.9317, F1: 0.9166, Accuracy: 0.9642, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.6291, Precision: 0.7314, F1: 0.6735, Accuracy: 0.8693, Codes:    10\n",
      "\n",
      "Newer Data - Log Reg (Window) SVM (Sentence) - Min and Max + 2 way features\n",
      "Word Tagger:\n",
      "MEAN Test Performance       Recall: 0.9085, Precision: 0.9491, F1: 0.9247, Accuracy: 0.9733, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.7738, Precision: 0.8326, F1: 0.7912, Accuracy: 0.9338, Codes:    10\n",
      "Sentence Classifier:\n",
      "MEAN Test Performance       Recall: 0.9340, Precision: 0.9496, F1: 0.9417, Accuracy: 0.9744, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.6140, Precision: 0.7129, F1: 0.6578, Accuracy: 0.8596, Codes:    10\n",
      "\n",
      "Newer Data - SVM (Window) Log Reg (Sentence) - Min and Max + 2 way features\n",
      "Word Tagger:\n",
      "MEAN Test Performance       Recall: 0.9902, Precision: 0.9815, F1: 0.9856, Accuracy: 0.9954, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.8301, Precision: 0.7422, F1: 0.7756, Accuracy: 0.9187, Codes:    10\n",
      "Sentence Classifier:\n",
      "MEAN Test Performance       Recall: 0.9873, Precision: 0.9872, F1: 0.9872, Accuracy: 0.9944, Codes:    10\n",
      "MEAN Validation Performance Recall: 0.6617, Precision: 0.6400, F1: 0.6497, Accuracy: 0.8438, Codes:    10"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TODO\n",
      "\n",
      "Use broader essay and sentence level features at every step. So train a window based classifier. \n",
      "Then train a second model using the first, but for each binary classifier, feed in information about all the other codes before and after in the same sentence (sentence level), and in previous and next sentences, and all tags (word level) 2-3 tags before and after within the current sentence."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}