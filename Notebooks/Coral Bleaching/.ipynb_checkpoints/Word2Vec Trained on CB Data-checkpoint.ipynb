{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "('Pickle Key:', 'folder_/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/_include_normal_False_include_vague_True_lower_case_True_min_df_5_min_sentence_length_3_remove_infrequent_False_remove_punctuation_True_remove_stop_words_True_replace_nums_True_spelling_correct_True_stem_False_window_size_7')\n",
      "1154 files found\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05571.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_AEKD_4_CB_ES-05904.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_BGJD_1_CB_ES-05733.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_ERSK_7_CB_ES-05798.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_KYLS_5_CB_ES-05671.ann file as .txt file is no essay //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_LRJE_5_CB_ES-05128.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05612.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_2_CB_ES-05617.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05632.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SVJJ_4_CB_ES-05640.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_SWSP_4_CB_ES-05459.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05484.ann file as .txt file is no essay. //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_1_CB_ES-05485.ann file as .txt file is no essay.  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFBM_2_CB_ES-05548.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TFMV_3_CB_ES-05845.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05715.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_11_CB_ES-05721.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06128.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRDJ_2_CB_ES-06132.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05025.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TRKM_1_CB_ES-05030.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_2_CB_ES-06140.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06149.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTCM_910_CB_ES-06153.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415_TTKP_7-8_CB_ES-06174.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/EBA1415post_TWNB_2_CB_ES-04948.ann file as .txt file is no essay.'\n",
      "1128 essays processed\n"
     ]
    }
   ],
   "source": [
    "from Decorators import memoize_to_disk\n",
    "from load_data import load_process_essays\n",
    "import Settings\n",
    "from window_based_tagger_config import get_config\n",
    "\n",
    "settings = Settings.Settings()\n",
    "folder =                            settings.data_directory + \"CoralBleaching/BrattData/EBA1415_Merged/\"\n",
    "processed_essay_filename_prefix =   settings.data_directory + \"CoralBleaching/BrattData/Pickled/essays_proc_pickled_\"\n",
    "\n",
    "config = get_config(folder)\n",
    "config[\"stem\"] = False\n",
    "config[\"remove_punctuation\"] = True\n",
    "config[\"spelling_correct\"] = True\n",
    "config[\"remove_stop_words\"] = True\n",
    "config[\"lower_case\"] = True\n",
    "\n",
    "mem_process_essays = memoize_to_disk(filename_prefix=processed_essay_filename_prefix)(load_process_essays)\n",
    "tagged_essays = mem_process_essays( **config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        sentences.append(zip(*sentence)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('leads', 'differences', 'rates', 'coral', 'bleaching')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# ex mikolov  settings time ./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 1 -sample 1e-3 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1 \n",
    "# from paragraph2vec discussion https://groups.google.com/forum/m/#!msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ\n",
    "#model = Word2Vec(sentences, iter=100, size=100, window=5, min_count=5, workers=8, sample=1e-6, hs=0, negative=20)\n",
    "model = Word2Vec(sentences, iter=20, size=100, window=5, min_count=2, workers=40,  sample=1e-6, hs=1, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hot', 0.3460378646850586),\n",
       " ('nice', 0.2867373824119568),\n",
       " ('polyp', 0.27999699115753174),\n",
       " ('puts', 0.2729570269584656),\n",
       " ('death', 0.2682577967643738),\n",
       " ('allow', 0.2589418292045593),\n",
       " ('water', 0.2572776973247528),\n",
       " ('turn', 0.23588168621063232),\n",
       " ('trade', 0.23569533228874207),\n",
       " ('seems', 0.23534996807575226)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"coral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 0.35793280601501465),\n",
       " ('white', 0.31779205799102783),\n",
       " ('bigger', 0.3059004843235016),\n",
       " ('decrease', 0.30360379815101624),\n",
       " ('long', 0.26957517862319946),\n",
       " ('destructive', 0.2645491063594818),\n",
       " ('dragged', 0.2621684968471527),\n",
       " ('would', 0.2562347650527954),\n",
       " ('fishes', 0.25381097197532654),\n",
       " ('parts', 0.24969466030597687)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"tentacles\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Results are not so great. Much better results are with the pre-trained vectors from the google news corpus **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
