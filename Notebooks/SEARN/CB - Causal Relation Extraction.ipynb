{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-06-18 12:24:35.131462\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "#regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "#vtags.add(EMPTY_TAG)\n",
    "cr_tags = vtags\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Anaphor',\n",
       " 'COMPILED',\n",
       " 'Causer',\n",
       " 'Causer:1',\n",
       " 'Causer:11',\n",
       " 'Causer:12',\n",
       " 'Causer:13',\n",
       " 'Causer:14',\n",
       " 'Causer:2',\n",
       " 'Causer:3',\n",
       " 'Causer:4',\n",
       " 'Causer:5',\n",
       " 'Causer:50',\n",
       " 'Causer:5b',\n",
       " 'Causer:6',\n",
       " 'Causer:7',\n",
       " 'Causer:Anaphor',\n",
       " 'Causer:other',\n",
       " 'Causer:rhetorical',\n",
       " 'Result',\n",
       " 'Result:1',\n",
       " 'Result:11',\n",
       " 'Result:12',\n",
       " 'Result:13',\n",
       " 'Result:14',\n",
       " 'Result:2',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:5',\n",
       " 'Result:50',\n",
       " 'Result:5b',\n",
       " 'Result:6',\n",
       " 'Result:7',\n",
       " 'Result:Anaphor',\n",
       " 'Result:other',\n",
       " 'Result:rhetorical',\n",
       " 'explicit',\n",
       " 'other',\n",
       " 'rhetorical']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in sorted(tag_freq.keys()) if \"->\" not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:1->Result:11',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:11',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays):\n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag = defaultdict(list)\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            unique_tags = set()\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)                \n",
    "                # remove unwanted tags\n",
    "                tags = vtags.intersection(tags)\n",
    "                unique_tags.update(tags)\n",
    "                # retain all tags for evaluation (not just most common)\n",
    "                # SKIP the START and END tags\n",
    "                if word != START and word != END:\n",
    "                    for t in (vtags - set([EMPTY_TAG])):\n",
    "                        if t in tags:\n",
    "                            ys_bytag[t].append(1)\n",
    "                        else:\n",
    "                            ys_bytag[t].append(0)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "                if len(tags) > 1:\n",
    "                    most_common = max(tags, key=lambda t: tag_freq[t])\n",
    "                    tags = set([most_common])\n",
    "                if len(tags) == 0:\n",
    "                    tags.add(EMPTY_TAG)\n",
    "\n",
    "                one_hot = []\n",
    "                for t in vtags:\n",
    "                    if t in tags:\n",
    "                        one_hot.append(1)\n",
    "                    else:\n",
    "                        one_hot.append(0)\n",
    "                y_seq.append(one_hot)\n",
    "            \n",
    "            for tag in vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_sent[tag].append(0)\n",
    "                \n",
    "            seq_lens.append(len(row)-2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "    \n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == maxlen, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag, ys_bytag_sent, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "config[\"window_size\"] = WINDOW_SIZE\n",
    "offset = int((config[\"window_size\"] - 1) / 2)\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featureextractionfunctions \\\n",
    "    import fact_extract_positional_word_features_stemmed, fact_extract_ngram_features_stemmed,\\\n",
    "    fact_extract_bow_ngram_features, extract_dependency_relation, extract_brown_cluster\n",
    "\n",
    "unigram_window_stemmed  = fact_extract_positional_word_features_stemmed(offset)\n",
    "biigram_window_stemmed  = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "triigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 3)\n",
    "unigram_bow_window      = fact_extract_bow_ngram_features(offset, 1)\n",
    "\n",
    "#TODO - add in full feature set, but keep simple for now\n",
    "#optimal CB feature set\n",
    "extractors = [\n",
    "    unigram_window_stemmed,\n",
    "    biigram_window_stemmed,\n",
    "    #triigram_window_stemmed,\n",
    "    #unigram_bow_window,\n",
    "    #extract_dependency_relation,\n",
    "    #extract_brown_cluster\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featureextractortransformer import FeatureExtractorTransformer\n",
    "\n",
    "feature_extractor = FeatureExtractorTransformer(extractors)\n",
    "essay_feats = feature_extractor.transform(tagged_essays)\n",
    "len(tagged_essays), len(essay_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(range(0))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAUSER = \"Causer\"\n",
    "RESULT = \"Result\"\n",
    "EXPLICIT = \"explicit\"\n",
    "CAUSER_EXPLICIT = \"Causer_Explicit\"\n",
    "RESULT_EXPLICIT = \"Result_Explicit\"\n",
    "CAUSER_EXPLICIT_RESULT = \"Causer_Explicit_Result\"\n",
    "\n",
    "class SearnModel(object):\n",
    "    Push_Cause = \"P_Cause\"\n",
    "    Push_Explicit = \"P_Explicit\"\n",
    "    Push_Result = \"P_Result\"\n",
    "    Reduce = \"Reduce\"\n",
    "    Skip = \"Skip\"\n",
    "    \n",
    "    def __init__(self, feat_extractor, tags, cr_tags, base_learner_fact, beta):\n",
    "        # init checks\n",
    "        assert CAUSER in tags, \"%s must be in tags\" % CAUSER\n",
    "        assert RESULT in tags, \"%s must be in tags\" % RESULT\n",
    "        assert EXPLICIT in tags, \"%s must be in tags\" % EXPLICIT\n",
    "\n",
    "        self.feat_extractor = feature_extractor    # feature extractor (for use later)\n",
    "        self.base_learner_fact = base_learner_fact # SKlearn classifier\n",
    "        self.tags = set(tags)                      # tags for basic tagging\n",
    "        self.cr_tags = set(cr_tags)                # causal relation tags\n",
    "            \n",
    "        self.actions = set([SearnModel.Push_Cause, SearnModel.Push_Explicit, SearnModel.Push_Result, \n",
    "                              SearnModel.Reduce, SearnModel.Skip])\n",
    "        self.epoch = -1\n",
    "        self.beta = beta\n",
    "        self.stack = []\n",
    "        self.tagging_models = {}\n",
    "        self.parser_model = None\n",
    "    \n",
    "    def get_conditional_feats(self, current_word, prev_words, history):\n",
    "        feats = {}\n",
    "        if len(history) > 0:\n",
    "            #TODO add feats for tags not predicted and actions not taken\n",
    "            prev_tags = history[-1]\n",
    "            for t in prev_tags:\n",
    "                feats[\"tag-1:\" + t] = 1\n",
    "                feats[\"tag-1:\" + t + \" wd:\"  + current_word] = 1\n",
    "                feats[\"tag-1:\" + t + \" wd-1:\"+ prev_words[-1]] = 1\n",
    "                \n",
    "            if len(history) > 1:\n",
    "                prev_prev_tags = history[-2]\n",
    "                for t in prev_prev_tags:\n",
    "                    # tag bigrams\n",
    "                    for prev_tag in prev_tags:\n",
    "                        feats[\"tag-1:\" + prev_tag + \" tag-2:\" + t] = 1\n",
    "                    feats[\"tag-2:\" + t] = 1\n",
    "                    feats[\"tag-2:\" + t + \" wd:\"   + current_word] = 1\n",
    "                    feats[\"tag-2:\" + t + \" wd-2:\" + prev_words[-2]] = 1\n",
    "        \n",
    "        for i in range(4):\n",
    "            offset = -(i+1)\n",
    "            if len(self.stack) < abs(offset):\n",
    "                break\n",
    "            s1_key = \"s\" + str(offset) + \":\" + self.stack[offset]\n",
    "            feats[s1_key] = 1z\n",
    "            # Stack Bigram feats\n",
    "            if len(self.stack) >= abs(offset) + 1:\n",
    "                next_offset = offset-1\n",
    "                s2_key = \"s\" + str(next_offset) + \":\" + self.stack[next_offset]\n",
    "                feats[s1_key + \" \" + s2_key] = 1\n",
    "        return feats\n",
    "    \n",
    "    def __oracle__(self, feats, ground_truth, ground_truth_history):\n",
    "        # filter to desired tags\n",
    "        ground_truth_tags = ground_truth.intersection(self.tags)\n",
    "        pred_tags = set()\n",
    "        #TODO - flip a coin to decide to use current model's predictions or ground truth\n",
    "        for tag in self.tags:\n",
    "            if tag in ground_truth_tags:\n",
    "                pred_tags.add(tag)\n",
    "        #Decide Actions\n",
    "        #TODO what if multiple causer, explicit or results tags assigned to one word?\n",
    "        if CAUSER in pred_tags:\n",
    "            pred_tags.add(SearnModel.Push_Cause)\n",
    "            self.stack.append(CAUSER)\n",
    "        elif EXPLICIT in pred_tags:\n",
    "            pred_tags.add(SearnModel.Push_Explicit)\n",
    "            self.stack.append(EXPLICIT)\n",
    "        elif RESULT in pred_tags:\n",
    "            pred_tags.add(SearnModel.Push_Result)\n",
    "            self.stack.append(RESULT)\n",
    "        else:\n",
    "            pred_tags.add(SearnModel.Skip)\n",
    "            \n",
    "        # Shift, Reduce or Skip?\n",
    "        # How to handle end of input (do we try and reduce what's remaining) ?\n",
    "        if len(ground_truth_history) > 0 and len(self.stack) >=2:\n",
    "            prev_ground_truth = ground_truth_history[-1]\n",
    "            for t in cr_tags:\n",
    "                #TODO: implement parsing actions\n",
    "                pass\n",
    "            \n",
    "    def train(self, essay_feats, epochs):\n",
    "        #essay_feats = self.feat_extractor.transform(tagged_essays)        \n",
    "        for i in range(0, epochs):\n",
    "            self.epoch +=1\n",
    "            tagging_models = {}\n",
    "            examples_with_loss = []\n",
    "            \n",
    "            for essay_ix, essay in enumerate(essay_feats):\n",
    "                for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "                    \n",
    "                    prediction_history = []\n",
    "                    ground_truth_history = []\n",
    "                    previous_words = []\n",
    "                    self.stack = []\n",
    "                    for i, (wd) in enumerate(taggged_sentence):\n",
    "                        \n",
    "                        # Don't mutate the feat dictionary\n",
    "                        tagger_feats = dict(wd.features.items())\n",
    "                        \n",
    "                        # get all tagger predictions for previous 2 tags\n",
    "                        cond_feats = self.get_conditional_feats(wd.word, previous_words, prediction_history, ground_truth_history)\n",
    "                        tagger_feats.update(cond_feats)\n",
    "\n",
    "                        # add more in depth features for this tag\n",
    "                        ground_truth = wd.tags\n",
    "                        oracle_predictions = self.__oracle__(tagger_feats, ground_truth)\n",
    "                        \n",
    "                        guess = self.model.predict(tagger_feats)\n",
    "                        self.model.update(actual, guess, tagger_feats)\n",
    "\n",
    "                        prev.append(guess)\n",
    "                        for cls in self.individual_tags:\n",
    "                            class2predictions[cls].append(  1 if cls in guess  else 0 )\n",
    "                            class2tags[cls].append(         1 if cls in actual else 0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
