{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Check mongo is running\n",
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM_Stacked/\"\n",
    "cv_folder = root_folder + \"CV_Data_Pickled/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-05-02 14:44:23.877770\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "cr_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "cr_vtags = set(cr_tags)\n",
    "cr_vtags.add(EMPTY_TAG)\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:1->Result:11',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:11',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cr_vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "\n",
    "ix2crtag = {}\n",
    "for ix, t in enumerate(cr_vtags):\n",
    "    ix2crtag[ix] = t\n",
    "    \n",
    "generator = idGen(seed=1) # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]\n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "        \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays):\n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag_concept_sent = defaultdict(list)\n",
    "    ys_bytag_cr_sent = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            unique_tags = set() # get all unique tags in sentence\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)                \n",
    "                unique_tags.update(tags)\n",
    "                \n",
    "                # remove unwanted tags, filter to concept tags\n",
    "                concept_tags = vtags.intersection(tags)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "                if len(concept_tags) > 1:\n",
    "                    most_common = max(concept_tags, key=lambda t: tag_freq[t])\n",
    "                    concept_tags = set([most_common])\n",
    "                if len(concept_tags) == 0:\n",
    "                    concept_tags.add(EMPTY_TAG)\n",
    "\n",
    "                one_hot = []\n",
    "                for t in vtags:\n",
    "                    if t in concept_tags:\n",
    "                        one_hot.append(1)\n",
    "                    else:\n",
    "                        one_hot.append(0)\n",
    "                y_seq.append(one_hot)\n",
    "                #end for each word\n",
    "            \n",
    "            # sentence level tags\n",
    "            for tag in vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_concept_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_concept_sent[tag].append(0)\n",
    "            \n",
    "            for tag in cr_vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_cr_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_cr_sent[tag].append(0)\n",
    "                \n",
    "            seq_lens.append(len(row)-2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "    \n",
    "    xs = sequence.pad_sequences(xs, maxlen=maxlen)\n",
    "    ys = sequence.pad_sequences(ys, maxlen=maxlen)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == maxlen, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag_concept_sent, ys_bytag_cr_sent, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        for pred_tag in pred_ys:\n",
    "            pred_ys_by_tag[pred_tag].append(1)\n",
    "            # for all other tags, a 0\n",
    "            for tag in(vtags - set([EMPTY_TAG, pred_tag])):\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results_sentence_level(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = set(pred_ys[1:-1])\n",
    "        for tag in vtags:\n",
    "            if tag == EMPTY_TAG:\n",
    "                continue\n",
    "            if tag in pred_ys:\n",
    "                pred_ys_by_tag[tag].append(1)\n",
    "            else:\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split(lst, dev_split):\n",
    "    # random shuffle\n",
    "    shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 s, sys: 322 ms, total: 6.1 s\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(cv_folder + \"td.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2training_data, f)\n",
    "\n",
    "with open(cv_folder + \"devd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2dev_data, f)\n",
    "\n",
    "with open(cv_folder + \"vd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1641 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05,size=(max_features, embedding_dim))\n",
    "    elif init =='zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1,keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict_classes(xs, batch_size=batch_size, verbose=0)   \n",
    "    pred_ys_by_tag = collapse_results_sentence_level(seq_len, preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2017-05-02 14:44:42.782940', '20170502_144442_782966')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from datetime import datetime\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merge_mode = \"sum\"\n",
    "if True:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "else:\n",
    "    embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "if True:\n",
    "    rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "else:\n",
    "    rnn_layer_fact = lambda : GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "for i in range(1):\n",
    "    model.add(rnn_layer_fact())\n",
    "\n",
    "model.add(TimeDistributedDense(out_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM_Stacked/fold_ix-0_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.h5'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    lcls = locals()\n",
    "    s = \"\"\n",
    "    for k, val in sorted(lcls.items(), key = lambda tpl: (0,tpl[0]) if tpl[0] == 'fold_ix' else (1,tpl[0])):\n",
    "        s += \"{key}-{val}_\".format(key=k, val=str(val))\n",
    "    return models_folder + s[:-1] + \".h5\"\n",
    "\n",
    "get_file_name(0, True, True, 2, merge_mode, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda : GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "    \n",
    "    X_train, y_train, train_ys_bytag_con_sent, train_ys_by_tag_cr_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev,   y_dev,   dev_ys_bytag_con_sent,   dev_ys_by_tag_cr_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "    X_test,  y_test,  test_ys_bytag_con_sent,  test_ys_by_tag_cr_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "    #for i in range(2):\n",
    "        #print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1 # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_bytag_con_sent, seq_len_dev)\n",
    "\n",
    "        #print(micro_metrics)\n",
    "        #print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fold2model = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        model = evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        fname = get_file_name(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        model.save(fname)    \n",
    "        fold2model[i] = model\n",
    "    return fold2model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Params 2017-05-02 16:56:16.423408 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=256\n",
      "Fold[0] - Best F1 Score=0.9119031607262945\n",
      "Fold[1] - Best F1 Score=0.8912547528517112\n",
      "Fold[2] - Best F1 Score=0.9234042553191488\n",
      "Fold[3] - Best F1 Score=0.9033816425120773\n",
      "Fold[4] - Best F1 Score=0.8991596638655462\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True]:\n",
    "    for bi_directional in [True]:\n",
    "        for num_rnns in [2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [256]:\n",
    "\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    fold2model = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    #print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_model(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fname = get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return keras.models.load_model(fname)\n",
    "\n",
    "def load_models(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    models_by_fold = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        model = load_model(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        models_by_fold[i] = model\n",
    "    return models_by_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699/1699 [==============================] - 11s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1704/1704 [==============================] - 11s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1662/1662 [==============================] - 10s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1618/1618 [==============================] - 10s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1609/1609 [==============================] - 10s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "predicts_by_fold = {}\n",
    "for fold_ix in range(CV_FOLDS):\n",
    "    X_test,  y_test,  test_ys_bytag_con_sent,  test_ys_by_tag_cr_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "    model = fold2model[fold_ix]\n",
    "    probs = model.predict_proba(X_test)\n",
    "    predicts_by_fold[fold_ix] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train and Test Data For Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_stacked_feats_by_fold(fold_ix, predicts_by_fold):\n",
    "    probs = predicts_by_fold[fold_ix]\n",
    "    xs = []\n",
    "    for i in range(len(probs)):\n",
    "        preds = probs[i,:]\n",
    "        #print(preds.shape)\n",
    "        #ix2tag\n",
    "        max_preds = np.max(preds, axis=0)\n",
    "        min_preds = np.max(preds, axis=0)\n",
    "        predicted_ixs = set(np.argwhere(max_preds > 0.5).flatten())\n",
    "        binary = [0] * len(max_preds)\n",
    "        for ix in predicted_ixs:\n",
    "            binary[ix] = 1\n",
    "        x = max_preds.tolist() + min_preds.tolist() + binary\n",
    "\n",
    "        # combination tags\n",
    "        ixs = ix2tag.keys()\n",
    "        for a in ixs:\n",
    "            for b in ixs:\n",
    "                if b < a:\n",
    "                    if a in predicted_ixs and b in predicted_ixs:\n",
    "                        x.append(1)\n",
    "                    else:\n",
    "                        x.append(0)\n",
    "        xs.append(x)\n",
    "    xs = np.asarray(xs)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked_feats_by_code = {}\n",
    "for fold_ix in range(CV_FOLDS):\n",
    "    stacked_feats_by_code[fold_ix] = get_stacked_feats_by_fold(fold_ix, predicts_by_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Thru Each Fold, Merge the Xs and Ys from the Other Folds as TD, and then Use Fold as VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "td_xs_by_fold = {}\n",
    "vd_xs_by_fold = {}\n",
    "\n",
    "td_ys_by_fold = {}\n",
    "vd_ys_by_fold = {}\n",
    "for vd_ix in range(CV_FOLDS):\n",
    "    td = []\n",
    "    \n",
    "    td_ys = defaultdict(list)\n",
    "    for td_ix in range(CV_FOLDS):\n",
    "        if td_ix == vd_ix:\n",
    "            continue\n",
    "        xs = stacked_feats_by_code[td_ix]\n",
    "        td.append(xs)\n",
    "        \n",
    "        _, _, _, td_ys_by_tag_cr_sent, _ = fold2test_data[td_ix]\n",
    "        merge_dictionaries(td_ys_by_tag_cr_sent, td_ys)\n",
    "\n",
    "    vd_xs_by_fold[vd_ix] = stacked_feats_by_code[vd_ix]\n",
    "    td_xs_by_fold[vd_ix] = np.vstack(td)\n",
    "    \n",
    "    td_ys_by_fold[vd_ix] = td_ys\n",
    "    _, _, _, vd_ys_by_tag_cr_sent, _ = fold2test_data[vd_ix]\n",
    "    vd_ys_by_fold[vd_ix] = vd_ys_by_tag_cr_sent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Validate the sizes of the arrays match expected\n",
    "for i in range(CV_FOLDS):\n",
    "    print(i)\n",
    "    print(td_xs_by_fold[i].shape)\n",
    "    print(vd_xs_by_fold[i].shape)\n",
    "    print(\"\")\n",
    "    key = \"Causer:1->Result:50\"\n",
    "    print(len(td_ys_by_fold[i][key]))\n",
    "    print(len(vd_ys_by_fold[i][key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for :Causer:1->Result:11\n",
      "Training for :Causer:1->Result:13\n",
      "Training for :Causer:1->Result:14\n",
      "Training for :Causer:1->Result:2\n",
      "Training for :Causer:1->Result:3\n",
      "Training for :Causer:1->Result:4\n",
      "Training for :Causer:1->Result:5\n",
      "Training for :Causer:1->Result:50\n",
      "Training for :Causer:1->Result:6\n",
      "Training for :Causer:1->Result:7\n",
      "Training for :Causer:11->Result:11\n",
      "Training for :Causer:11->Result:12\n",
      "Training for :Causer:11->Result:13\n",
      "Training for :Causer:11->Result:14\n",
      "Training for :Causer:11->Result:3\n",
      "Training for :Causer:11->Result:4\n",
      "Training for :Causer:11->Result:50\n",
      "Training for :Causer:11->Result:6\n",
      "Training for :Causer:12->Result:11\n",
      "Training for :Causer:12->Result:13\n",
      "Training for :Causer:12->Result:14\n",
      "Training for :Causer:12->Result:50\n",
      "Training for :Causer:12->Result:5b\n",
      "Training for :Causer:12->Result:7\n",
      "Training for :Causer:13->Result:11\n",
      "Training for :Causer:13->Result:12\n",
      "Training for :Causer:13->Result:14\n",
      "Training for :Causer:13->Result:4\n",
      "Training for :Causer:13->Result:5\n",
      "Training for :Causer:13->Result:50\n",
      "Training for :Causer:13->Result:6\n",
      "Training for :Causer:13->Result:7\n",
      "Training for :Causer:14->Result:50\n",
      "Training for :Causer:14->Result:6\n",
      "Training for :Causer:14->Result:7\n",
      "Training for :Causer:2->Result:1\n",
      "Training for :Causer:2->Result:3\n",
      "Training for :Causer:2->Result:50\n",
      "Training for :Causer:2->Result:6\n",
      "Training for :Causer:3->Result:1\n",
      "Training for :Causer:3->Result:13\n",
      "Training for :Causer:3->Result:14\n",
      "Training for :Causer:3->Result:2\n",
      "Training for :Causer:3->Result:4\n",
      "Training for :Causer:3->Result:5\n",
      "Training for :Causer:3->Result:50\n",
      "Training for :Causer:3->Result:5b\n",
      "Training for :Causer:3->Result:6\n",
      "Training for :Causer:3->Result:7\n",
      "Training for :Causer:4->Result:11\n",
      "Training for :Causer:4->Result:13\n",
      "Training for :Causer:4->Result:14\n",
      "Training for :Causer:4->Result:3\n",
      "Training for :Causer:4->Result:5\n",
      "Training for :Causer:4->Result:50\n",
      "Training for :Causer:4->Result:5b\n",
      "Training for :Causer:4->Result:6\n",
      "Training for :Causer:4->Result:7\n",
      "Training for :Causer:5->Result:13\n",
      "Training for :Causer:5->Result:14\n",
      "Training for :Causer:5->Result:3\n",
      "Training for :Causer:5->Result:4\n",
      "Training for :Causer:5->Result:50\n",
      "Training for :Causer:5->Result:5b\n",
      "Training for :Causer:5->Result:7\n",
      "Training for :Causer:50->Result:1\n",
      "Training for :Causer:50->Result:3\n",
      "Training for :Causer:50->Result:50\n",
      "Training for :Causer:50->Result:7\n",
      "Training for :Causer:5b->Result:14\n",
      "Training for :Causer:5b->Result:5\n",
      "Training for :Causer:5b->Result:50\n",
      "Training for :Causer:5b->Result:7\n",
      "Training for :Causer:6->Result:14\n",
      "Training for :Causer:6->Result:50\n",
      "Training for :Causer:6->Result:5b\n",
      "Training for :Causer:6->Result:7\n",
      "Training for :Causer:7->Result:14\n",
      "Training for :Causer:7->Result:4\n",
      "Training for :Causer:7->Result:5\n",
      "Training for :Causer:7->Result:50\n",
      "Training for :Causer:7->Result:5b\n",
      "Training for :Empty\n",
      "Training for :Causer:1->Result:11\n",
      "Training for :Causer:1->Result:13\n",
      "Training for :Causer:1->Result:14\n",
      "Training for :Causer:1->Result:2\n",
      "Training for :Causer:1->Result:3\n",
      "Training for :Causer:1->Result:4\n",
      "Training for :Causer:1->Result:5\n",
      "Training for :Causer:1->Result:50\n",
      "Training for :Causer:1->Result:6\n",
      "Training for :Causer:1->Result:7\n",
      "Training for :Causer:11->Result:11\n",
      "Training for :Causer:11->Result:12\n",
      "Training for :Causer:11->Result:13\n",
      "Training for :Causer:11->Result:14\n",
      "Training for :Causer:11->Result:3\n",
      "Training for :Causer:11->Result:4\n",
      "Training for :Causer:11->Result:50\n",
      "Training for :Causer:11->Result:6\n",
      "Training for :Causer:12->Result:11\n",
      "Training for :Causer:12->Result:13\n",
      "Training for :Causer:12->Result:14\n",
      "Training for :Causer:12->Result:50\n",
      "Training for :Causer:12->Result:5b\n",
      "Training for :Causer:12->Result:7\n",
      "Training for :Causer:13->Result:11\n",
      "Training for :Causer:13->Result:12\n",
      "Training for :Causer:13->Result:14\n",
      "Training for :Causer:13->Result:4\n",
      "Training for :Causer:13->Result:5\n",
      "Training for :Causer:13->Result:50\n",
      "Training for :Causer:13->Result:6\n",
      "Training for :Causer:13->Result:7\n",
      "Training for :Causer:14->Result:50\n",
      "Training for :Causer:14->Result:6\n",
      "Training for :Causer:14->Result:7\n",
      "Training for :Causer:2->Result:1\n",
      "Training for :Causer:2->Result:3\n",
      "Training for :Causer:2->Result:50\n",
      "Training for :Causer:2->Result:6\n",
      "Training for :Causer:3->Result:1\n",
      "Training for :Causer:3->Result:13\n",
      "Training for :Causer:3->Result:14\n",
      "Training for :Causer:3->Result:2\n",
      "Training for :Causer:3->Result:4\n",
      "Training for :Causer:3->Result:5\n",
      "Training for :Causer:3->Result:50\n",
      "Training for :Causer:3->Result:5b\n",
      "Training for :Causer:3->Result:6\n",
      "Training for :Causer:3->Result:7\n",
      "Training for :Causer:4->Result:11\n",
      "Training for :Causer:4->Result:13\n",
      "Training for :Causer:4->Result:14\n",
      "Training for :Causer:4->Result:3\n",
      "Training for :Causer:4->Result:5\n",
      "Training for :Causer:4->Result:50\n",
      "Training for :Causer:4->Result:5b\n",
      "Training for :Causer:4->Result:6\n",
      "Training for :Causer:4->Result:7\n",
      "Training for :Causer:5->Result:13\n",
      "Training for :Causer:5->Result:14\n",
      "Training for :Causer:5->Result:3\n",
      "Training for :Causer:5->Result:4\n",
      "Training for :Causer:5->Result:50\n",
      "Training for :Causer:5->Result:5b\n",
      "Training for :Causer:5->Result:7\n",
      "Training for :Causer:50->Result:1\n",
      "Training for :Causer:50->Result:3\n",
      "Training for :Causer:50->Result:50\n",
      "Training for :Causer:50->Result:7\n",
      "Training for :Causer:5b->Result:14\n",
      "Training for :Causer:5b->Result:5\n",
      "Training for :Causer:5b->Result:50\n",
      "Training for :Causer:5b->Result:7\n",
      "Training for :Causer:6->Result:14\n",
      "Training for :Causer:6->Result:50\n",
      "Training for :Causer:6->Result:5b\n",
      "Training for :Causer:6->Result:7\n",
      "Training for :Causer:7->Result:14\n",
      "Training for :Causer:7->Result:4\n",
      "Training for :Causer:7->Result:5\n",
      "Training for :Causer:7->Result:50\n",
      "Training for :Causer:7->Result:5b\n",
      "Training for :Empty\n",
      "Training for :Causer:1->Result:11\n",
      "Training for :Causer:1->Result:13\n",
      "Training for :Causer:1->Result:14\n",
      "Training for :Causer:1->Result:2\n",
      "Training for :Causer:1->Result:3\n",
      "Training for :Causer:1->Result:4\n",
      "Training for :Causer:1->Result:5\n",
      "Training for :Causer:1->Result:50\n",
      "Training for :Causer:1->Result:6\n",
      "Training for :Causer:1->Result:7\n",
      "Training for :Causer:11->Result:11\n",
      "Training for :Causer:11->Result:12\n",
      "Training for :Causer:11->Result:13\n",
      "Training for :Causer:11->Result:14\n",
      "Training for :Causer:11->Result:3\n",
      "Training for :Causer:11->Result:4\n",
      "Training for :Causer:11->Result:50\n",
      "Training for :Causer:11->Result:6\n",
      "Training for :Causer:12->Result:11\n",
      "Training for :Causer:12->Result:13\n",
      "Training for :Causer:12->Result:14\n",
      "Training for :Causer:12->Result:50\n",
      "Training for :Causer:12->Result:5b\n",
      "Training for :Causer:12->Result:7\n",
      "Training for :Causer:13->Result:11\n",
      "Training for :Causer:13->Result:12\n",
      "Training for :Causer:13->Result:14\n",
      "Training for :Causer:13->Result:4\n",
      "Training for :Causer:13->Result:5\n",
      "Training for :Causer:13->Result:50\n",
      "Training for :Causer:13->Result:6\n",
      "Training for :Causer:13->Result:7\n",
      "Training for :Causer:14->Result:50\n",
      "Training for :Causer:14->Result:6\n",
      "Training for :Causer:14->Result:7\n",
      "Training for :Causer:2->Result:1\n",
      "Training for :Causer:2->Result:3\n",
      "Training for :Causer:2->Result:50\n",
      "Training for :Causer:2->Result:6\n",
      "Training for :Causer:3->Result:1\n",
      "Training for :Causer:3->Result:13\n",
      "Training for :Causer:3->Result:14\n",
      "Training for :Causer:3->Result:2\n",
      "Training for :Causer:3->Result:4\n",
      "Training for :Causer:3->Result:5\n",
      "Training for :Causer:3->Result:50\n",
      "Training for :Causer:3->Result:5b\n",
      "Training for :Causer:3->Result:6\n",
      "Training for :Causer:3->Result:7\n",
      "Training for :Causer:4->Result:11\n",
      "Training for :Causer:4->Result:13\n",
      "Training for :Causer:4->Result:14\n",
      "Training for :Causer:4->Result:3\n",
      "Training for :Causer:4->Result:5\n",
      "Training for :Causer:4->Result:50\n",
      "Training for :Causer:4->Result:5b\n",
      "Training for :Causer:4->Result:6\n",
      "Training for :Causer:4->Result:7\n",
      "Training for :Causer:5->Result:13\n",
      "Training for :Causer:5->Result:14\n",
      "Training for :Causer:5->Result:3\n",
      "Training for :Causer:5->Result:4\n",
      "Training for :Causer:5->Result:50\n",
      "Training for :Causer:5->Result:5b\n",
      "Training for :Causer:5->Result:7\n",
      "Training for :Causer:50->Result:1\n",
      "Training for :Causer:50->Result:3\n",
      "Training for :Causer:50->Result:50\n",
      "Training for :Causer:50->Result:7\n",
      "Training for :Causer:5b->Result:14\n",
      "Training for :Causer:5b->Result:5\n",
      "Training for :Causer:5b->Result:50\n",
      "Training for :Causer:5b->Result:7\n",
      "Training for :Causer:6->Result:14\n",
      "Training for :Causer:6->Result:50\n",
      "Training for :Causer:6->Result:5b\n",
      "Training for :Causer:6->Result:7\n",
      "Training for :Causer:7->Result:14\n",
      "Training for :Causer:7->Result:4\n",
      "Training for :Causer:7->Result:5\n",
      "Training for :Causer:7->Result:50\n",
      "Training for :Causer:7->Result:5b\n",
      "Training for :Empty\n",
      "Training for :Causer:1->Result:11\n",
      "Training for :Causer:1->Result:13\n",
      "Training for :Causer:1->Result:14\n",
      "Training for :Causer:1->Result:2\n",
      "Training for :Causer:1->Result:3\n",
      "Training for :Causer:1->Result:4\n",
      "Training for :Causer:1->Result:5\n",
      "Training for :Causer:1->Result:50\n",
      "Training for :Causer:1->Result:6\n",
      "Training for :Causer:1->Result:7\n",
      "Training for :Causer:11->Result:11\n",
      "Training for :Causer:11->Result:12\n",
      "Training for :Causer:11->Result:13\n",
      "Training for :Causer:11->Result:14\n",
      "Training for :Causer:11->Result:3\n",
      "Training for :Causer:11->Result:4\n",
      "Training for :Causer:11->Result:50\n",
      "Training for :Causer:11->Result:6\n",
      "Training for :Causer:12->Result:11\n",
      "Training for :Causer:12->Result:13\n",
      "Training for :Causer:12->Result:14\n",
      "Training for :Causer:12->Result:50\n",
      "Training for :Causer:12->Result:5b\n",
      "Training for :Causer:12->Result:7\n",
      "Training for :Causer:13->Result:11\n",
      "Training for :Causer:13->Result:12\n",
      "Training for :Causer:13->Result:14\n",
      "Training for :Causer:13->Result:4\n",
      "Training for :Causer:13->Result:5\n",
      "Training for :Causer:13->Result:50\n",
      "Training for :Causer:13->Result:6\n",
      "Training for :Causer:13->Result:7\n",
      "Training for :Causer:14->Result:50\n",
      "Training for :Causer:14->Result:6\n",
      "Training for :Causer:14->Result:7\n",
      "Training for :Causer:2->Result:1\n",
      "Training for :Causer:2->Result:3\n",
      "Training for :Causer:2->Result:50\n",
      "Training for :Causer:2->Result:6\n",
      "Training for :Causer:3->Result:1\n",
      "Training for :Causer:3->Result:13\n",
      "Training for :Causer:3->Result:14\n",
      "Training for :Causer:3->Result:2\n",
      "Training for :Causer:3->Result:4\n",
      "Training for :Causer:3->Result:5\n",
      "Training for :Causer:3->Result:50\n",
      "Training for :Causer:3->Result:5b\n",
      "Training for :Causer:3->Result:6\n",
      "Training for :Causer:3->Result:7\n",
      "Training for :Causer:4->Result:11\n",
      "Training for :Causer:4->Result:13\n",
      "Training for :Causer:4->Result:14\n",
      "Training for :Causer:4->Result:3\n",
      "Training for :Causer:4->Result:5\n",
      "Training for :Causer:4->Result:50\n",
      "Training for :Causer:4->Result:5b\n",
      "Training for :Causer:4->Result:6\n",
      "Training for :Causer:4->Result:7\n",
      "Training for :Causer:5->Result:13\n",
      "Training for :Causer:5->Result:14\n",
      "Training for :Causer:5->Result:3\n",
      "Training for :Causer:5->Result:4\n",
      "Training for :Causer:5->Result:50\n",
      "Training for :Causer:5->Result:5b\n",
      "Training for :Causer:5->Result:7\n",
      "Training for :Causer:50->Result:1\n",
      "Training for :Causer:50->Result:3\n",
      "Training for :Causer:50->Result:50\n",
      "Training for :Causer:50->Result:7\n",
      "Training for :Causer:5b->Result:14\n",
      "Training for :Causer:5b->Result:5\n",
      "Training for :Causer:5b->Result:50\n",
      "Training for :Causer:5b->Result:7\n",
      "Training for :Causer:6->Result:14\n",
      "Training for :Causer:6->Result:50\n",
      "Training for :Causer:6->Result:5b\n",
      "Training for :Causer:6->Result:7\n",
      "Training for :Causer:7->Result:14\n",
      "Training for :Causer:7->Result:4\n",
      "Training for :Causer:7->Result:5\n",
      "Training for :Causer:7->Result:50\n",
      "Training for :Causer:7->Result:5b\n",
      "Training for :Empty\n",
      "Training for :Causer:1->Result:11\n",
      "Training for :Causer:1->Result:13\n",
      "Training for :Causer:1->Result:14\n",
      "Training for :Causer:1->Result:2\n",
      "Training for :Causer:1->Result:3\n",
      "Training for :Causer:1->Result:4\n",
      "Training for :Causer:1->Result:5\n",
      "Training for :Causer:1->Result:50\n",
      "Training for :Causer:1->Result:6\n",
      "Training for :Causer:1->Result:7\n",
      "Training for :Causer:11->Result:11\n",
      "Training for :Causer:11->Result:12\n",
      "Training for :Causer:11->Result:13\n",
      "Training for :Causer:11->Result:14\n",
      "Training for :Causer:11->Result:3\n",
      "Training for :Causer:11->Result:4\n",
      "Training for :Causer:11->Result:50\n",
      "Training for :Causer:11->Result:6\n",
      "Training for :Causer:12->Result:11\n",
      "Training for :Causer:12->Result:13\n",
      "Training for :Causer:12->Result:14\n",
      "Training for :Causer:12->Result:50\n",
      "Training for :Causer:12->Result:5b\n",
      "Training for :Causer:12->Result:7\n",
      "Training for :Causer:13->Result:11\n",
      "Training for :Causer:13->Result:12\n",
      "Training for :Causer:13->Result:14\n",
      "Training for :Causer:13->Result:4\n",
      "Training for :Causer:13->Result:5\n",
      "Training for :Causer:13->Result:50\n",
      "Training for :Causer:13->Result:6\n",
      "Training for :Causer:13->Result:7\n",
      "Training for :Causer:14->Result:50\n",
      "Training for :Causer:14->Result:6\n",
      "Training for :Causer:14->Result:7\n",
      "Training for :Causer:2->Result:1\n",
      "Training for :Causer:2->Result:3\n",
      "Training for :Causer:2->Result:50\n",
      "Training for :Causer:2->Result:6\n",
      "Training for :Causer:3->Result:1\n",
      "Training for :Causer:3->Result:13\n",
      "Training for :Causer:3->Result:14\n",
      "Training for :Causer:3->Result:2\n",
      "Training for :Causer:3->Result:4\n",
      "Training for :Causer:3->Result:5\n",
      "Training for :Causer:3->Result:50\n",
      "Training for :Causer:3->Result:5b\n",
      "Training for :Causer:3->Result:6\n",
      "Training for :Causer:3->Result:7\n",
      "Training for :Causer:4->Result:11\n",
      "Training for :Causer:4->Result:13\n",
      "Training for :Causer:4->Result:14\n",
      "Training for :Causer:4->Result:3\n",
      "Training for :Causer:4->Result:5\n",
      "Training for :Causer:4->Result:50\n",
      "Training for :Causer:4->Result:5b\n",
      "Training for :Causer:4->Result:6\n",
      "Training for :Causer:4->Result:7\n",
      "Training for :Causer:5->Result:13\n",
      "Training for :Causer:5->Result:14\n",
      "Training for :Causer:5->Result:3\n",
      "Training for :Causer:5->Result:4\n",
      "Training for :Causer:5->Result:50\n",
      "Training for :Causer:5->Result:5b\n",
      "Training for :Causer:5->Result:7\n",
      "Training for :Causer:50->Result:1\n",
      "Training for :Causer:50->Result:3\n",
      "Training for :Causer:50->Result:50\n",
      "Training for :Causer:50->Result:7\n",
      "Training for :Causer:5b->Result:14\n",
      "Training for :Causer:5b->Result:5\n",
      "Training for :Causer:5b->Result:50\n",
      "Training for :Causer:5b->Result:7\n",
      "Training for :Causer:6->Result:14\n",
      "Training for :Causer:6->Result:50\n",
      "Training for :Causer:6->Result:5b\n",
      "Training for :Causer:6->Result:7\n",
      "Training for :Causer:7->Result:14\n",
      "Training for :Causer:7->Result:4\n",
      "Training for :Causer:7->Result:5\n",
      "Training for :Causer:7->Result:50\n",
      "Training for :Causer:7->Result:5b\n",
      "Training for :Empty\n"
     ]
    }
   ],
   "source": [
    "from wordtagginghelper import train_classifier_per_code, test_classifier_per_code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "fn_create_sent_cls  = lambda : LogisticRegression(dual=True)\n",
    "\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "for i in range(CV_FOLDS):\n",
    "    sent_td_xs = td_xs_by_fold[i]\n",
    "    sent_vd_xs = vd_xs_by_fold[i]\n",
    "    \n",
    "    sent_td_ys_bycode = td_ys_by_fold[i]\n",
    "    sent_vd_ys_bycode = vd_ys_by_fold[i]\n",
    "\n",
    "    tags = sent_td_ys_bycode.keys()\n",
    "    \n",
    "    tag2sent_classifier = train_classifier_per_code(sent_td_xs, sent_td_ys_bycode , fn_create_sent_cls, tags)\n",
    "    td_sent_predictions_by_code \\\n",
    "        = test_classifier_per_code(sent_td_xs, tag2sent_classifier, tags )\n",
    "\n",
    "    vd_sent_predictions_by_code \\\n",
    "        = test_classifier_per_code(sent_vd_xs, tag2sent_classifier, tags )\n",
    "        \n",
    "    merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "    merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_sent_predictions_by_code, cv_sent_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_sent_predictions_by_code, cv_sent_vd_predictions_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE\n",
      "\n",
      "TAG:       Causer:1->Result:11   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:1->Result:13   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9997588036661843    \n",
      "sentences:                       2                     \n",
      "\n",
      "TAG:       Causer:1->Result:14   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:1->Result:2    \n",
      "f1:        0.2828282828282828    0.2828282828282828    \n",
      "recall:    0.17073170731707318   0.17073170731707318   \n",
      "precision: 0.8235294117647058    0.8235294117647058    \n",
      "accuracy:  0.9914375301495417    0.9914375301495417    \n",
      "sentences:                       82                    \n",
      "\n",
      "TAG:       Causer:1->Result:3    \n",
      "f1:        0.6341463414634146    0.6014319809069213    \n",
      "recall:    0.5922222222222222    0.56                  \n",
      "precision: 0.6824583866837388    0.6494845360824743    \n",
      "accuracy:  0.981458031837916     0.9798601061263869    \n",
      "sentences:                       225                   \n",
      "\n",
      "TAG:       Causer:1->Result:4    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9992764109985528    0.9990352146647371    \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:1->Result:5    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9993970091654607    0.9993970091654607    \n",
      "sentences:                       5                     \n",
      "\n",
      "TAG:       Causer:1->Result:50   \n",
      "f1:        0.7857142857142856    0.7721179624664879    \n",
      "recall:    0.8242296918767507    0.8067226890756303    \n",
      "precision: 0.7506377551020408    0.7403598971722365    \n",
      "accuracy:  0.980643994211288     0.9794983116256633    \n",
      "sentences:                       357                   \n",
      "\n",
      "TAG:       Causer:1->Result:6    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:1->Result:7    \n",
      "f1:        0.07999999999999999   0.0                   \n",
      "recall:    0.041666666666666664  0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9993065605402798    0.9992764109985528    \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:11->Result:11  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:11->Result:12  \n",
      "f1:        0.9156626506024096    0.9156626506024096    \n",
      "recall:    0.9382716049382716    0.9382716049382716    \n",
      "precision: 0.8941176470588236    0.8941176470588236    \n",
      "accuracy:  0.9983116256632899    0.9983116256632899    \n",
      "sentences:                       81                    \n",
      "\n",
      "TAG:       Causer:11->Result:13  \n",
      "f1:        0.7243867243867245    0.6820809248554913    \n",
      "recall:    0.7050561797752809    0.6629213483146067    \n",
      "precision: 0.744807121661721     0.7023809523809523    \n",
      "accuracy:  0.9942414375301496    0.9933671008200675    \n",
      "sentences:                       89                    \n",
      "\n",
      "TAG:       Causer:11->Result:14  \n",
      "f1:        0.30985915492957744   0.2                   \n",
      "recall:    0.18333333333333332   0.13333333333333333   \n",
      "precision: 1.0                   0.4                   \n",
      "accuracy:  0.9985226724553786    0.9980704293294742    \n",
      "sentences:                       15                    \n",
      "\n",
      "TAG:       Causer:11->Result:3   \n",
      "f1:        0.2857142857142857    0.0                   \n",
      "recall:    0.16666666666666666   0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9993970091654607    0.9992764109985528    \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:11->Result:4   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:11->Result:50  \n",
      "f1:        0.765375854214123     0.6542056074766355    \n",
      "recall:    0.711864406779661     0.5932203389830508    \n",
      "precision: 0.8275862068965517    0.7291666666666666    \n",
      "accuracy:  0.9968945972021225    0.9955378678244091    \n",
      "sentences:                       59                    \n",
      "\n",
      "TAG:       Causer:11->Result:6   \n",
      "f1:        0.5                   0.0                   \n",
      "recall:    0.3333333333333333    0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:12->Result:11  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:12->Result:13  \n",
      "f1:        0.8925619834710744    0.847682119205298     \n",
      "recall:    0.9246575342465754    0.8767123287671232    \n",
      "precision: 0.8626198083067093    0.8205128205128205    \n",
      "accuracy:  0.9980402797877472    0.9972262421611191    \n",
      "sentences:                       73                    \n",
      "\n",
      "TAG:       Causer:12->Result:14  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:12->Result:50  \n",
      "f1:        0.45161290322580644   0.0                   \n",
      "recall:    0.2916666666666667    0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9994874577906416    0.999155812831645     \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:12->Result:5b  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:12->Result:7   \n",
      "f1:        0.5                   0.0                   \n",
      "recall:    0.3333333333333333    0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:13->Result:11  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:13->Result:12  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:13->Result:14  \n",
      "f1:        0.8531684698608965    0.8395061728395061    \n",
      "recall:    0.8414634146341463    0.8292682926829268    \n",
      "precision: 0.8652037617554859    0.85                  \n",
      "accuracy:  0.9971357935359383    0.9968644476603956    \n",
      "sentences:                       82                    \n",
      "\n",
      "TAG:       Causer:13->Result:4   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:13->Result:5   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:13->Result:50  \n",
      "f1:        0.6466666666666667    0.5517241379310345    \n",
      "recall:    0.5705882352941176    0.47058823529411764   \n",
      "precision: 0.7461538461538462    0.6666666666666666    \n",
      "accuracy:  0.9936082971538832    0.9921611191509889    \n",
      "sentences:                       85                    \n",
      "\n",
      "TAG:       Causer:13->Result:6   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9997588036661843    \n",
      "sentences:                       2                     \n",
      "\n",
      "TAG:       Causer:13->Result:7   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:14->Result:50  \n",
      "f1:        0.575                 0.4848484848484848    \n",
      "recall:    0.5424528301886793    0.4528301886792453    \n",
      "precision: 0.6117021276595744    0.5217391304347826    \n",
      "accuracy:  0.9948745779064159    0.993849493487699     \n",
      "sentences:                       53                    \n",
      "\n",
      "TAG:       Causer:14->Result:6   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:14->Result:7   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:2->Result:1    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:2->Result:3    \n",
      "f1:        0.661290322580645     0.6129032258064515    \n",
      "recall:    0.5694444444444444    0.5277777777777778    \n",
      "precision: 0.7884615384615384    0.7307692307692307    \n",
      "accuracy:  0.9974674384949349    0.9971056439942113    \n",
      "sentences:                       36                    \n",
      "\n",
      "TAG:       Causer:2->Result:50   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9995176073323685    0.9995176073323685    \n",
      "sentences:                       4                     \n",
      "\n",
      "TAG:       Causer:2->Result:6    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:3->Result:1    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.998191027496382     0.998191027496382     \n",
      "sentences:                       15                    \n",
      "\n",
      "TAG:       Causer:3->Result:13   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9995176073323685    0.9995176073323685    \n",
      "sentences:                       4                     \n",
      "\n",
      "TAG:       Causer:3->Result:14   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9992764109985528    0.9992764109985528    \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:3->Result:2    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:3->Result:4    \n",
      "f1:        0.8708815672306324    0.8398576512455516    \n",
      "recall:    0.8988970588235294    0.8676470588235294    \n",
      "precision: 0.844559585492228     0.8137931034482758    \n",
      "accuracy:  0.9956283164495899    0.9945730824891461    \n",
      "sentences:                       136                   \n",
      "\n",
      "TAG:       Causer:3->Result:5    \n",
      "f1:        0.3194444444444444    0.2933333333333334    \n",
      "recall:    0.20909090909090908   0.2                   \n",
      "precision: 0.6764705882352942    0.55                  \n",
      "accuracy:  0.9881813796430294    0.9872165943077665    \n",
      "sentences:                       110                   \n",
      "\n",
      "TAG:       Causer:3->Result:50   \n",
      "f1:        0.7493091196210028    0.7261146496815287    \n",
      "recall:    0.7322530864197531    0.7037037037037037    \n",
      "precision: 0.767178658043654     0.75                  \n",
      "accuracy:  0.9808550410033767    0.9792571152918476    \n",
      "sentences:                       324                   \n",
      "\n",
      "TAG:       Causer:3->Result:5b   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:3->Result:6    \n",
      "f1:        0.5866666666666667    0.0                   \n",
      "recall:    0.4230769230769231    0.0                   \n",
      "precision: 0.9565217391304348    0.0                   \n",
      "accuracy:  0.9990653642064641    0.9980704293294742    \n",
      "sentences:                       13                    \n",
      "\n",
      "TAG:       Causer:3->Result:7    \n",
      "f1:        0.6176470588235294    0.3448275862068965    \n",
      "recall:    0.4772727272727273    0.22727272727272727   \n",
      "precision: 0.875                 0.7142857142857143    \n",
      "accuracy:  0.9984322238301978    0.9977086348287506    \n",
      "sentences:                       22                    \n",
      "\n",
      "TAG:       Causer:4->Result:11   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:4->Result:13   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:4->Result:14   \n",
      "f1:        0.8774193548387097    0.8701298701298701    \n",
      "recall:    0.8395061728395061    0.8271604938271605    \n",
      "precision: 0.918918918918919     0.9178082191780822    \n",
      "accuracy:  0.9977086348287506    0.9975880366618427    \n",
      "sentences:                       81                    \n",
      "\n",
      "TAG:       Causer:4->Result:3    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9997588036661843    \n",
      "sentences:                       2                     \n",
      "\n",
      "TAG:       Causer:4->Result:5    \n",
      "f1:        0.14492753623188406   0.08955223880597014   \n",
      "recall:    0.078125              0.046875              \n",
      "precision: 1.0                   1.0                   \n",
      "accuracy:  0.9928847081524361    0.9926435118186203    \n",
      "sentences:                       64                    \n",
      "\n",
      "TAG:       Causer:4->Result:50   \n",
      "f1:        0.7089783281733746    0.6871165644171779    \n",
      "recall:    0.6291208791208791    0.6153846153846154    \n",
      "precision: 0.8120567375886525    0.7777777777777778    \n",
      "accuracy:  0.9943318861553304    0.993849493487699     \n",
      "sentences:                       91                    \n",
      "\n",
      "TAG:       Causer:4->Result:5b   \n",
      "f1:        0.4                   0.2608695652173913    \n",
      "recall:    0.25                  0.17647058823529413   \n",
      "precision: 1.0                   0.5                   \n",
      "accuracy:  0.9984623733719248    0.9979498311625663    \n",
      "sentences:                       17                    \n",
      "\n",
      "TAG:       Causer:4->Result:6    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:4->Result:7    \n",
      "f1:        0.5641025641025641    0.0                   \n",
      "recall:    0.39285714285714285   0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9994874577906416    0.999155812831645     \n",
      "sentences:                       7                     \n",
      "\n",
      "TAG:       Causer:5->Result:13   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:5->Result:14   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:5->Result:3    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9997588036661843    \n",
      "sentences:                       2                     \n",
      "\n",
      "TAG:       Causer:5->Result:4    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:5->Result:50   \n",
      "f1:        0.548611111111111     0.44776119402985076   \n",
      "recall:    0.4114583333333333    0.3125                \n",
      "precision: 0.8229166666666666    0.7894736842105263    \n",
      "accuracy:  0.9960805595754945    0.9955378678244091    \n",
      "sentences:                       48                    \n",
      "\n",
      "TAG:       Causer:5->Result:5b   \n",
      "f1:        0.029850746268656716  0.0                   \n",
      "recall:    0.015625              0.0                   \n",
      "precision: 0.3333333333333333    0.0                   \n",
      "accuracy:  0.9960805595754945    0.9957790641582248    \n",
      "sentences:                       32                    \n",
      "\n",
      "TAG:       Causer:5->Result:7    \n",
      "f1:        0.20689655172413793   0.13333333333333336   \n",
      "recall:    0.11538461538461539   0.07692307692307693   \n",
      "precision: 1.0                   0.5                   \n",
      "accuracy:  0.9986131210805596    0.9984322238301978    \n",
      "sentences:                       13                    \n",
      "\n",
      "TAG:       Causer:50->Result:1   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:50->Result:3   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:50->Result:50  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9996382054992764    0.9996382054992764    \n",
      "sentences:                       3                     \n",
      "\n",
      "TAG:       Causer:50->Result:7   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9992764109985528    0.9992764109985528    \n",
      "sentences:                       6                     \n",
      "\n",
      "TAG:       Causer:5b->Result:14  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9997588036661843    0.9997588036661843    \n",
      "sentences:                       2                     \n",
      "\n",
      "TAG:       Causer:5b->Result:5   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9995176073323685    0.9995176073323685    \n",
      "sentences:                       4                     \n",
      "\n",
      "TAG:       Causer:5b->Result:50  \n",
      "f1:        0.18309859154929578   0.0588235294117647    \n",
      "recall:    0.10483870967741936   0.03225806451612903   \n",
      "precision: 0.7222222222222222    0.3333333333333333    \n",
      "accuracy:  0.996502653159672     0.9961408586589484    \n",
      "sentences:                       31                    \n",
      "\n",
      "TAG:       Causer:5b->Result:7   \n",
      "f1:        0.05405405405405406   0.0                   \n",
      "recall:    0.027777777777777776  0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  0.9989447660395562    0.9989146164978292    \n",
      "sentences:                       9                     \n",
      "\n",
      "TAG:       Causer:6->Result:14   \n",
      "f1:        0.627027027027027     0.6067415730337079    \n",
      "recall:    0.6170212765957447    0.574468085106383     \n",
      "precision: 0.6373626373626373    0.6428571428571429    \n",
      "accuracy:  0.9958393632416788    0.9957790641582248    \n",
      "sentences:                       47                    \n",
      "\n",
      "TAG:       Causer:6->Result:50   \n",
      "f1:        0.6382022471910113    0.5636363636363637    \n",
      "recall:    0.5546875             0.484375              \n",
      "precision: 0.7513227513227513    0.6739130434782609    \n",
      "accuracy:  0.9951459237819585    0.9942112879884226    \n",
      "sentences:                       64                    \n",
      "\n",
      "TAG:       Causer:6->Result:5b   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:6->Result:7    \n",
      "f1:        0.8605817452357071    0.8455284552845529    \n",
      "recall:    0.825                 0.8                   \n",
      "precision: 0.89937106918239      0.896551724137931     \n",
      "accuracy:  0.9958092136999518    0.9954172696575012    \n",
      "sentences:                       130                   \n",
      "\n",
      "TAG:       Causer:7->Result:14   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9993970091654607    0.9993970091654607    \n",
      "sentences:                       5                     \n",
      "\n",
      "TAG:       Causer:7->Result:4    \n",
      "f1:        1.0                   0.0                   \n",
      "recall:    1.0                   0.0                   \n",
      "precision: 1.0                   0.0                   \n",
      "accuracy:  1.0                   0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:7->Result:5    \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9998794018330921    0.9998794018330921    \n",
      "sentences:                       1                     \n",
      "\n",
      "TAG:       Causer:7->Result:50   \n",
      "f1:        0.7550877192982457    0.7394957983193278    \n",
      "recall:    0.6897435897435897    0.676923076923077     \n",
      "precision: 0.834108527131783     0.8148148148148148    \n",
      "accuracy:  0.9789556198745779    0.9775687409551375    \n",
      "sentences:                       390                   \n",
      "\n",
      "TAG:       Causer:7->Result:5b   \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.9995176073323685    0.9995176073323685    \n",
      "sentences:                       4                     \n",
      "\n",
      "TAG:       Empty                 \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  1.0                   1.0                   \n",
      "sentences:                       0                     \n",
      "\n",
      "TAG:       MEAN                  \n",
      "f1:        0.24827438987048492   0.18014594284160992   \n",
      "recall:    0.21726167438230212   0.16438963055275727   \n",
      "precision: 0.37889904874862296   0.2313654881570026    \n",
      "accuracy:  0.9976323528557066    0.9973715411573937    \n",
      "sentences:                       83.0                  \n",
      "\n",
      "TAG:       MEAN_CONCEPT_CODES    \n",
      "f1:        0                     0                     \n",
      "recall:    0                     0                     \n",
      "precision: 0                     0                     \n",
      "accuracy:  0                     0                     \n",
      "sentences:                       0                     \n",
      "\n",
      "TAG:       WEIGHTED_MEAN         \n",
      "f1:        0.6567151177578167    0.6175916596583817    \n",
      "recall:    0.6210911510312708    0.5854956753160346    \n",
      "precision: 0.7636107745184784    0.7026163791976339    \n",
      "accuracy:  0.9890082052489327    0.9880797979535383    \n",
      "sentences:                       3006.0                \n",
      "\n",
      "TAG:       WEIGHTED_MEAN_CONCEPT_CODES  \n",
      "f1:        0.0                   0.0                   \n",
      "recall:    0.0                   0.0                   \n",
      "precision: 0.0                   0.0                   \n",
      "accuracy:  0.0                   0.0                   \n",
      "sentences:                       -1                    \n",
      "\n",
      "TAG:       MICRO_F1              \n",
      "f1:        0.6961871911997762    0.6605366860574217    \n",
      "recall:    0.6210911510312708    0.5854956753160346    \n",
      "precision: 0.7919406150583245    0.7576409814894532    \n",
      "accuracy:  0.9976323528557065    0.9973715411573937    \n",
      "sentences:                       3006.0                \n",
      "\n",
      "Macro F1:  0.0                   0.0                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_algo = str(fn_create_sent_cls())\n",
    "\n",
    "SUFFIX = \"\"\n",
    "CB_SENT_TD, CB_SENT_VD = \"CR_CB_STACKED_TD\" + SUFFIX, \"CR_CB_STACKED_VD\" + SUFFIX\n",
    "parameters = dict(config)\n",
    "parameters[\"extractors\"] = []\n",
    "parameters[\"loss\"] = \"bce\"\n",
    "parameters[\"use_pretrained_embedding\"] = use_pretrained_embedding\n",
    "parameters[\"bi-directional\"] = bi_directional\n",
    "parameters[\"hidden_size\"] = hidden_size\n",
    "parameters[\"merge_mode\"] = merge_mode\n",
    "parameters[\"num_rnns\"] = num_rnns\n",
    "\n",
    "#parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "\n",
    "sent_td_objectid = processor.persist_results(CB_SENT_TD, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, parameters, sent_algo)\n",
    "sent_vd_objectid = processor.persist_results(CB_SENT_VD, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, parameters, sent_algo)\n",
    "\n",
    "# This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\n",
    "print(processor.results_to_string(sent_td_objectid, CB_SENT_TD,     sent_vd_objectid,   CB_SENT_VD,     \"SENTENCE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
