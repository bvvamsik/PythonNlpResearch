{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-04-02 18:08:58.915382\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if (t[0].isdigit() or \"->\" in t) \n",
    "                     and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Causer:1->Result:11',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:11',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "    \n",
    "generator = idGen(seed=1) # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]  \n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "        \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays):\n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag = defaultdict(list)\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            unique_tags = set()\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)                \n",
    "                # remove unwanted tags\n",
    "                tags = vtags.intersection(tags)\n",
    "                unique_tags.update(tags)\n",
    "                # retain all tags for evaluation (not just most common)\n",
    "                # SKIP the START and END tags\n",
    "                if word != START and word != END:\n",
    "                    for t in (vtags - set([EMPTY_TAG])):\n",
    "                        if t in tags:\n",
    "                            ys_bytag[t].append(1)\n",
    "                        else:\n",
    "                            ys_bytag[t].append(0)\n",
    "\n",
    "                # encode ys with all valid tags\n",
    "                y_vector = []\n",
    "                for t in vtags:\n",
    "                    if t in tags:\n",
    "                        y_vector.append(1)\n",
    "                    else:\n",
    "                        y_vector.append(0)\n",
    "                y_seq.append(y_vector)\n",
    "            \n",
    "            for tag in vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_sent[tag].append(0)\n",
    "                \n",
    "            seq_lens.append(len(row)-2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "    \n",
    "    xs = sequence.pad_sequences(xs, maxlen=maxlen)\n",
    "    ys = sequence.pad_sequences(ys, maxlen=maxlen)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == maxlen, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag, ys_bytag_sent, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        for pred_tag in pred_ys:\n",
    "            pred_ys_by_tag[pred_tag].append(1)\n",
    "            # for all other tags, a 0\n",
    "            for tag in(vtags - set([EMPTY_TAG, pred_tag])):\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results_sentence_level(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = set(pred_ys[1:-1])\n",
    "        for tag in vtags:\n",
    "            if tag == EMPTY_TAG:\n",
    "                continue\n",
    "            if tag in pred_ys:\n",
    "                pred_ys_by_tag[tag].append(1)\n",
    "            else:\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split(lst, dev_split):\n",
    "    # random shuffle\n",
    "    shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 3.06 s, total: 35.8 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1641 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05,size=(max_features, embedding_dim))\n",
    "    elif init =='zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1,keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict_classes(xs, batch_size=batch_size, verbose=0)   \n",
    "    pred_ys_by_tag = collapse_results_sentence_level(seq_len, preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2017-04-04 22:52:16.655171', '20170404_225216_655909')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from datetime import datetime\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "\n",
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda : GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "         \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "\n",
    "    X_train, y_train, train_ys_by_tag, train_ys_by_tag_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev,   y_dev,   dev_ys_by_tag,   dev_ys_by_tag_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "    X_test,  y_test,  test_ys_by_tag,  test_ys_by_tag_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1 # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_by_tag_sent, seq_len_dev)\n",
    "\n",
    "        print(micro_metrics)\n",
    "        #print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag_sent, seq_len_train)\n",
    "    test_micro_metrics,  test_predictions_by_tag  = score_predictions(model, X_test,  test_ys_by_tag_sent,  seq_len_test)\n",
    "    return train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag_sent, test_ys_by_tag_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    \n",
    "    results = Parallel(n_jobs=1)(delayed(evaluate_fold)\\\n",
    "        (i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) for i in range(CV_FOLDS))\n",
    "\n",
    "    cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    for result in results:\n",
    "        td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "        merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "        merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "        merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "        merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "        \n",
    "    SUFFIX = \"_RNN\"\n",
    "    CB_TAGGING_TD, CB_TAGGING_VD = \"CR_CB_TAGGING_TD\" + SUFFIX, \"CR_CB_TAGGING_VD\" + SUFFIX\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = []\n",
    "    parameters[\"min_feat_freq\"] = 0\n",
    "\n",
    "    parameters[\"use_pretrained_embedding\"] = use_pretrained_embedding\n",
    "    parameters[\"bi-directional\"] = bi_directional\n",
    "    parameters[\"hidden_size\"] = hidden_size\n",
    "    parameters[\"merge_mode\"] = merge_mode\n",
    "    parameters[\"num_rnns\"] = num_rnns\n",
    "\n",
    "    wd_algo = \"RNN\"\n",
    "    wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "    wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "    avg_f1 = float(processor.get_metric(CB_TAGGING_VD, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Params 2017-04-07 22:10:05.604901 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=256\n",
      "2017-04-07 22:10:05.904720: Epoch=0\n",
      "Recall: 0.8434, Precision: 0.3858, F1: 0.5294, Accuracy: 0.9792, Codes:   881\n",
      "2017-04-07 22:14:19.900991: Epoch=1\n",
      "Recall: 0.8184, Precision: 0.3979, F1: 0.5355, Accuracy: 0.9803, Codes:   881\n",
      "2017-04-07 22:17:35.770606: Epoch=2\n",
      "Recall: 0.8354, Precision: 0.3993, F1: 0.5404, Accuracy: 0.9802, Codes:   881\n",
      "2017-04-07 22:20:37.902760: Epoch=3\n",
      "Recall: 0.8377, Precision: 0.4234, F1: 0.5625, Accuracy: 0.9819, Codes:   881\n",
      "2017-04-07 22:23:41.399058: Epoch=4\n",
      "Recall: 0.8195, Precision: 0.4252, F1: 0.5599, Accuracy: 0.9821, Codes:   881\n",
      "2017-04-07 22:26:46.383272: Epoch=5\n",
      "Recall: 0.8400, Precision: 0.4771, F1: 0.6086, Accuracy: 0.9850, Codes:   881\n",
      "2017-04-07 22:29:54.696350: Epoch=6\n",
      "Recall: 0.8331, Precision: 0.4619, F1: 0.5943, Accuracy: 0.9842, Codes:   881\n",
      "2017-04-07 22:33:09.222348: Epoch=7\n",
      "Recall: 0.7798, Precision: 0.4186, F1: 0.5448, Accuracy: 0.9819, Codes:   881\n",
      "2017-04-07 22:36:22.747959: Epoch=8\n",
      "Recall: 0.8116, Precision: 0.4352, F1: 0.5666, Accuracy: 0.9827, Codes:   881\n",
      "Too long since an improvement, stopping\n",
      "2017-04-07 22:40:42.302793: Epoch=0\n",
      "Recall: 0.8226, Precision: 0.3434, F1: 0.4846, Accuracy: 0.9771, Codes:   936\n",
      "2017-04-07 22:44:28.671611: Epoch=1\n",
      "Recall: 0.8162, Precision: 0.3693, F1: 0.5085, Accuracy: 0.9793, Codes:   936\n",
      "2017-04-07 22:47:47.258485: Epoch=2\n",
      "Recall: 0.8162, Precision: 0.3749, F1: 0.5138, Accuracy: 0.9798, Codes:   936\n",
      "2017-04-07 22:51:06.619899: Epoch=3\n",
      "Recall: 0.8376, Precision: 0.4113, F1: 0.5517, Accuracy: 0.9822, Codes:   936\n",
      "2017-04-07 22:54:25.006107: Epoch=4\n",
      "Recall: 0.7927, Precision: 0.4272, F1: 0.5552, Accuracy: 0.9834, Codes:   936\n",
      "2017-04-07 22:57:41.772946: Epoch=5\n",
      "Recall: 0.8269, Precision: 0.4093, F1: 0.5476, Accuracy: 0.9821, Codes:   936\n",
      "2017-04-07 23:00:56.690848: Epoch=6\n",
      "Recall: 0.7927, Precision: 0.4046, F1: 0.5357, Accuracy: 0.9820, Codes:   936\n",
      "2017-04-07 23:04:09.032790: Epoch=7\n",
      "Recall: 0.8472, Precision: 0.4435, F1: 0.5822, Accuracy: 0.9841, Codes:   936\n",
      "2017-04-07 23:07:20.612405: Epoch=8\n",
      "Recall: 0.8376, Precision: 0.4363, F1: 0.5737, Accuracy: 0.9837, Codes:   936\n",
      "2017-04-07 23:10:28.524437: Epoch=9\n",
      "Recall: 0.8632, Precision: 0.4337, F1: 0.5773, Accuracy: 0.9834, Codes:   936\n",
      "2017-04-07 23:13:34.828302: Epoch=10\n",
      "Recall: 0.8184, Precision: 0.4320, F1: 0.5655, Accuracy: 0.9835, Codes:   936\n",
      "Too long since an improvement, stopping\n",
      "2017-04-07 23:17:33.793725: Epoch=0\n",
      "Recall: 0.7821, Precision: 0.3796, F1: 0.5111, Accuracy: 0.9793, Codes:   881\n",
      "2017-04-07 23:21:11.806602: Epoch=1\n",
      "Recall: 0.7980, Precision: 0.3626, F1: 0.4986, Accuracy: 0.9778, Codes:   881\n",
      "2017-04-07 23:24:20.312236: Epoch=2\n",
      "Recall: 0.7764, Precision: 0.3715, F1: 0.5026, Accuracy: 0.9788, Codes:   881\n",
      "2017-04-07 23:27:27.950291: Epoch=3\n",
      "Recall: 0.8456, Precision: 0.4304, F1: 0.5704, Accuracy: 0.9824, Codes:   881\n",
      "2017-04-07 23:30:34.724815: Epoch=4\n",
      "Recall: 0.8002, Precision: 0.4042, F1: 0.5371, Accuracy: 0.9809, Codes:   881\n",
      "2017-04-07 23:33:40.826638: Epoch=5\n",
      "Recall: 0.8388, Precision: 0.4269, F1: 0.5658, Accuracy: 0.9822, Codes:   881\n",
      "2017-04-07 23:36:47.523659: Epoch=6\n",
      "Recall: 0.8138, Precision: 0.4301, F1: 0.5628, Accuracy: 0.9825, Codes:   881\n",
      "Too long since an improvement, stopping\n",
      "2017-04-07 23:40:47.608039: Epoch=0\n",
      "Recall: 0.8066, Precision: 0.3975, F1: 0.5325, Accuracy: 0.9802, Codes:   817\n",
      "2017-04-07 23:44:16.496106: Epoch=1\n",
      "Recall: 0.8091, Precision: 0.4184, F1: 0.5515, Accuracy: 0.9816, Codes:   817\n",
      "2017-04-07 23:47:17.693556: Epoch=2\n",
      "Recall: 0.7748, Precision: 0.4187, F1: 0.5436, Accuracy: 0.9818, Codes:   817\n",
      "2017-04-07 23:50:18.364091: Epoch=3\n",
      "Recall: 0.8066, Precision: 0.4480, F1: 0.5760, Accuracy: 0.9834, Codes:   817\n",
      "2017-04-07 23:53:19.386853: Epoch=4\n",
      "Recall: 0.7858, Precision: 0.4341, F1: 0.5592, Accuracy: 0.9827, Codes:   817\n",
      "2017-04-07 23:56:20.744566: Epoch=5\n",
      "Recall: 0.7870, Precision: 0.4475, F1: 0.5705, Accuracy: 0.9835, Codes:   817\n",
      "2017-04-07 23:59:20.927824: Epoch=6\n",
      "Recall: 0.8164, Precision: 0.4720, F1: 0.5982, Accuracy: 0.9847, Codes:   817\n",
      "2017-04-08 00:02:17.205787: Epoch=7\n",
      "Recall: 0.8286, Precision: 0.4584, F1: 0.5902, Accuracy: 0.9839, Codes:   817\n",
      "2017-04-08 00:05:04.102439: Epoch=8\n",
      "Recall: 0.8213, Precision: 0.4599, F1: 0.5896, Accuracy: 0.9840, Codes:   817\n",
      "2017-04-08 00:07:51.170212: Epoch=9\n",
      "Recall: 0.8311, Precision: 0.4651, F1: 0.5964, Accuracy: 0.9843, Codes:   817\n",
      "Too long since an improvement, stopping\n",
      "2017-04-08 00:11:26.482538: Epoch=0\n",
      "Recall: 0.7803, Precision: 0.4025, F1: 0.5311, Accuracy: 0.9785, Codes:   997\n",
      "2017-04-08 00:14:42.628646: Epoch=1\n",
      "Recall: 0.8064, Precision: 0.4589, F1: 0.5849, Accuracy: 0.9822, Codes:   997\n",
      "2017-04-08 00:17:30.345033: Epoch=2\n",
      "Recall: 0.8104, Precision: 0.4795, F1: 0.6025, Accuracy: 0.9834, Codes:   997\n",
      "2017-04-08 00:20:16.321219: Epoch=3\n",
      "Recall: 0.7994, Precision: 0.4413, F1: 0.5687, Accuracy: 0.9811, Codes:   997\n",
      "2017-04-08 00:23:02.420143: Epoch=4\n",
      "Recall: 0.8024, Precision: 0.4648, F1: 0.5887, Accuracy: 0.9825, Codes:   997\n",
      "2017-04-08 00:25:48.314958: Epoch=5\n",
      "Recall: 0.8185, Precision: 0.4725, F1: 0.5991, Accuracy: 0.9829, Codes:   997\n",
      "Too long since an improvement, stopping\n",
      "MicroF1=0.5816389005252474\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True]:\n",
    "    for bi_directional in [True]:\n",
    "        for num_rnns in [2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [256]:\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    micro_f1 = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True,False]:\n",
    "    for bi_directional in [True, False]:\n",
    "        for num_rnns in [1,2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [64,128,256]:\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    micro_f1 = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODO\n",
    "* Use early stopping criteria\n",
    "* Embeddings:\n",
    " * Don't remove low frequency words\n",
    " * Normalize all vector lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-05 21:41:12.188075: Epoch=0\n",
      "Recall: 0.5823, Precision: 0.2043, F1: 0.3025, Accuracy: 0.9627, Codes:   881\n",
      "2017-04-05 21:41:42.068373: Epoch=1\n",
      "Recall: 0.5936, Precision: 0.2666, F1: 0.3679, Accuracy: 0.9716, Codes:   881\n"
     ]
    }
   ],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "\n",
    "# merge_mode is Bi-Directional only\n",
    "fold_ix = 0\n",
    "use_pretrained_embedding=True\n",
    "bi_directional=True\n",
    "num_rnns=1\n",
    "merge_mode=\"sum\"\n",
    "hidden_size=16\n",
    "\n",
    "if use_pretrained_embedding:\n",
    "    embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "    embedding_layer = Embedding(max_features,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "else:\n",
    "    embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "if bi_directional:\n",
    "    rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "else:\n",
    "    rnn_layer_fact = lambda : GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "for i in range(num_rnns):\n",
    "    model.add(rnn_layer_fact())\n",
    "\n",
    "model.add(TimeDistributedDense(out_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "\n",
    "X_train, y_train, train_ys_by_tag, train_ys_by_tag_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "X_dev,   y_dev,   dev_ys_by_tag,   dev_ys_by_tag_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "X_test,  y_test,  test_ys_by_tag,  test_ys_by_tag_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "# init loop vars\n",
    "f1_scores = [-1]\n",
    "num_since_best_score = 0\n",
    "patience = 3\n",
    "best_weights = None\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "    epochs = 1 # epochs per training instance\n",
    "    results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "    micro_metrics,_ = score_predictions(model, X_dev, dev_ys_by_tag_sent, seq_len_dev)\n",
    "\n",
    "    print(micro_metrics)\n",
    "    #print()\n",
    "\n",
    "    f1_score = micro_metrics.f1_score\n",
    "    best_f1_score = max(f1_scores)\n",
    "    if f1_score <= best_f1_score:\n",
    "        num_since_best_score += 1\n",
    "    else: # score improved\n",
    "        num_since_best_score = 0\n",
    "        best_weights = model.get_weights()\n",
    "\n",
    "    f1_scores.append(f1_score)\n",
    "    if num_since_best_score >= patience:\n",
    "        print(\"Too long since an improvement, stopping\")\n",
    "        break\n",
    "\n",
    "# load best weights\n",
    "model.set_weights(best_weights)\n",
    "train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag_sent, seq_len_train)\n",
    "test_micro_metrics,  test_predictions_by_tag  = score_predictions(model, X_test,  test_ys_by_tag_sent,  seq_len_test)\n",
    "#return train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag, test_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6038 6038\n",
      "1587 1587\n",
      "1\n",
      "5879 5879\n",
      "1661 1661\n",
      "2\n",
      "6060 6060\n",
      "1561 1561\n",
      "3\n",
      "5909 5909\n",
      "1767 1767\n",
      "4\n",
      "5902 5902\n",
      "1716 1716\n"
     ]
    }
   ],
   "source": [
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "for fold_ix in range(5):\n",
    "    print(fold_ix)\n",
    "    X_train, y_train, train_ys_by_tag, train_ys_by_tag_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev,   y_dev,   dev_ys_by_tag,   dev_ys_by_tag_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "    X_test,  y_test,  test_ys_by_tag,  test_ys_by_tag_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "    \n",
    "    train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag_sent, seq_len_train)\n",
    "    test_micro_metrics,  test_predictions_by_tag  = score_predictions(model, X_test,  test_ys_by_tag_sent,  seq_len_test)\n",
    "    print(len(X_train), len(train_predictions_by_tag[\"50\"]))\n",
    "    print(len(X_test),  len(test_predictions_by_tag[\"50\"]))\n",
    "    \n",
    "    td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag_sent, test_ys_by_tag_sent\n",
    "    merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "    merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = dict(config)\n",
    "wd_algo = \"RNN\"\n",
    "\n",
    "wd_td_objectid = processor.persist_results(\"XXX1\", cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "wd_vd_objectid = processor.persist_results(\"XXX2\", cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
