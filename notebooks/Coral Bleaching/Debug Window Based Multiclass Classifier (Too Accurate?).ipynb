{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from Decorators import memoize_to_disk\n",
    "from load_data import load_process_essays, extract_features\n",
    "\n",
    "from featurevectorizer import FeatureVectorizer\n",
    "from featureextractionfunctions import *\n",
    "from CrossValidation import cross_validation\n",
    "from wordtagginghelper import *\n",
    "from IterableFP import flatten\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from tag_frequency import get_tag_freq, regular_tag\n",
    "from joblib import Parallel, delayed\n",
    "# END Classifiers\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create persister (mongo client) - fail fast if mongo service not initialized\n",
    "processor = ResultsProcessor()\n",
    "\n",
    "# not hashed as don't affect persistence of feature processing\n",
    "SPARSE_WD_FEATS     = True\n",
    "\n",
    "MIN_FEAT_FREQ       = 5        # 5 best so far\n",
    "CV_FOLDS            = 5\n",
    "\n",
    "MIN_TAG_FREQ        = 5\n",
    "LOOK_BACK           = 0     # how many sentences to look back when predicting tags\n",
    "# end not hashed\n",
    "\n",
    "# construct unique key using settings for pickling\n",
    "\n",
    "settings = Settings.Settings()\n",
    "folder =                            settings.data_directory + \"CoralBleaching/BrattData/EBA1415_Merged/\"\n",
    "processed_essay_filename_prefix =   settings.data_directory + \"CoralBleaching/BrattData/Pickled/essays_proc_pickled_\"\n",
    "features_filename_prefix =          settings.data_directory + \"CoralBleaching/BrattData/Pickled/feats_pickled_\"\n",
    "\n",
    "out_metrics_file     =              settings.data_directory + \"CoralBleaching/Results/metrics.txt\"\n",
    "\n",
    "config = get_config(folder)\n",
    "\n",
    "\"\"\" FEATURE EXTRACTION \"\"\"\n",
    "config[\"window_size\"] = 11\n",
    "offset = (config[\"window_size\"] - 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder': '/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/',\n",
       " 'include_normal': False,\n",
       " 'include_vague': True,\n",
       " 'lower_case': True,\n",
       " 'min_df': 2,\n",
       " 'min_sentence_length': 3,\n",
       " 'remove_infrequent': False,\n",
       " 'remove_punctuation': False,\n",
       " 'remove_stop_words': False,\n",
       " 'replace_nums': True,\n",
       " 'spelling_correct': True,\n",
       " 'stem': False,\n",
       " 'window_size': 11}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pickle Key:', 'folder_/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/_include_normal_False_include_vague_True_lower_case_True_min_df_2_min_sentence_length_3_remove_infrequent_False_remove_punctuation_False_remove_stop_words_False_replace_nums_True_spelling_correct_True_stem_False_window_size_11')\n",
      "('Pickle Key:', 'extractors_fn_bow_ngram_feat[ngram_size:1 offset:5]_fn_pos_wd_feats_stemmed[offset:5]_fn_pos_ngram_feat_stemmed[ngram_size:2 offset:5]_fn_pos_ngram_feat_stemmed[ngram_size:3 offset:5]_extract_brown_cluster_extract_dependency_relation_folder_/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/_include_normal_False_include_vague_True_lower_case_True_min_df_2_min_sentence_length_3_remove_infrequent_False_remove_punctuation_False_remove_stop_words_False_replace_nums_True_spelling_correct_True_stem_False_window_size_11')\n"
     ]
    }
   ],
   "source": [
    "unigram_bow_window = fact_extract_bow_ngram_features(offset, 1)\n",
    "\n",
    "unigram_window_stemmed = fact_extract_positional_word_features_stemmed(offset)\n",
    "biigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "trigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 3)\n",
    "\n",
    "extractors = [unigram_bow_window,\n",
    "              unigram_window_stemmed,\n",
    "              biigram_window_stemmed,\n",
    "              trigram_window_stemmed,\n",
    "              extract_brown_cluster,\n",
    "              extract_dependency_relation\n",
    "]\n",
    "\n",
    "feat_config = dict(config.items() + [(\"extractors\", extractors)])\n",
    "\n",
    "\"\"\" LOAD DATA \"\"\"\n",
    "mem_process_essays = memoize_to_disk(filename_prefix=processed_essay_filename_prefix)(load_process_essays)\n",
    "tagged_essays = mem_process_essays( **config )\n",
    "logger.info(\"Essays loaded\")\n",
    "# most params below exist ONLY for the purposes of the hashing to and from disk\n",
    "mem_extract_features = memoize_to_disk(filename_prefix=features_filename_prefix)(extract_features)\n",
    "essay_feats = mem_extract_features(tagged_essays, **feat_config)\n",
    "logger.info(\"Features loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" DEFINE TAGS \"\"\"\n",
    "\n",
    "_, lst_all_tags = flatten_to_wordlevel_feat_tags(essay_feats)\n",
    "regular_tags = list(set((t for t in flatten(lst_all_tags) if t[0].isdigit())))\n",
    "\n",
    "\"\"\" works best with all the pair-wise causal relation codes \"\"\"\n",
    "wd_train_tags = regular_tags\n",
    "wd_test_tags  = regular_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" CLASSIFIERS \"\"\"\n",
    "\"\"\" Log Reg + Log Reg is best!!! \"\"\"\n",
    "fn_create_wd_cls   = lambda: LogisticRegression() # C=1, dual = False seems optimal\n",
    "wd_algo   = str(fn_create_wd_cls())\n",
    "print \"Classifier:\", wd_algo\n",
    "\n",
    "folds = cross_validation(essay_feats, CV_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_f1_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3c656dcd3d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mavg_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCB_TAGGING_VD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_vd_objectid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__MICRO_F1__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mavg_f1_f1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_f1_f1' is not defined"
     ]
    }
   ],
   "source": [
    "def train_tagger(fold, essays_TD, essays_VD, wd_test_tags, wd_train_tags,\n",
    "                 dual, C, penalty, fit_intercept, multi_class):\n",
    "\n",
    "    # TD and VD are lists of Essay objects. The sentences are lists\n",
    "    # of featureextractortransformer.Word objects\n",
    "\n",
    "    \"\"\" Data Partitioning and Training \"\"\"\n",
    "    td_feats, td_tags = flatten_to_wordlevel_feat_tags(essays_TD)\n",
    "    vd_feats, vd_tags = flatten_to_wordlevel_feat_tags(essays_VD)\n",
    "    \n",
    "    feature_transformer = FeatureVectorizer(min_feature_freq=MIN_FEAT_FREQ, sparse=SPARSE_WD_FEATS)\n",
    "    td_X, vd_X = feature_transformer.fit_transform(td_feats), feature_transformer.transform(vd_feats)\n",
    "\n",
    "    wd_td_ys = get_wordlevel_powerset_ys(td_tags, wd_train_tags)\n",
    "    wd_vd_ys = get_wordlevel_powerset_ys(vd_tags, wd_train_tags)\n",
    "\n",
    "    wd_td_ys_by_code = get_by_code_from_powerset_predictions(wd_td_ys, wd_test_tags)\n",
    "    wd_vd_ys_by_code = get_by_code_from_powerset_predictions(wd_vd_ys, wd_test_tags)\n",
    "\n",
    "    \"\"\" TRAIN Tagger \"\"\"\n",
    "\n",
    "    solver = 'liblinear'\n",
    "    if multi_class == 'multinomial':\n",
    "        solver = \"lbfgs\"\n",
    "    model = LogisticRegression(dual=dual, C=C, penalty=penalty, fit_intercept=fit_intercept, multi_class=multi_class, solver=solver)\n",
    "    if fold == 0:\n",
    "        print(model)\n",
    "\n",
    "    model.fit(td_X, wd_td_ys)\n",
    "\n",
    "    wd_td_pred = model.predict(td_X)\n",
    "    wd_vd_pred = model.predict(vd_X)\n",
    "\n",
    "    \"\"\" TEST Tagger \"\"\"\n",
    "    td_wd_predictions_by_code = get_by_code_from_powerset_predictions(wd_td_pred, wd_test_tags)\n",
    "    vd_wd_predictions_by_code = get_by_code_from_powerset_predictions(wd_vd_pred, wd_test_tags)\n",
    "\n",
    "    return td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_by_code, wd_vd_ys_by_code\n",
    "\n",
    "multi_class = 'multinomial'\n",
    "dual = False\n",
    "penalty = 'l1'\n",
    "fit_intercept = True \n",
    "C = 100.0\n",
    "\n",
    "fold = 0\n",
    "\n",
    "hyper_opt_params = {}\n",
    "\n",
    "# Gather metrics per fold\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "\"\"\" This doesn't run in parallel ! \"\"\"\n",
    "essays_TD, essays_VD = folds[0]\n",
    "\n",
    "result = train_tagger(fold, essays_VD, essays_VD, wd_test_tags, wd_train_tags,\n",
    "                      dual=dual, C=C, penalty=penalty, fit_intercept=fit_intercept, multi_class=multi_class)\n",
    "\n",
    "td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "\n",
    "# print results for each code\n",
    "\"\"\" Persist Results to Mongo DB \"\"\"\n",
    "\n",
    "SUFFIX = \"_WINDOW_CLASSIFIER_LBL_POWERSET_MULTICLASS_HYPER_PARAM_TUNING\"\n",
    "CB_TAGGING_TD, CB_TAGGING_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX\n",
    "parameters = dict(config)\n",
    "parameters[\"extractors\"] = map(lambda fn: fn.func_name, extractors)\n",
    "parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "parameters.update(hyper_opt_params)\n",
    "\n",
    "wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "\n",
    "    # This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\n",
    "avg_f1 = float(processor.get_metric(CB_TAGGING_VD, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935685729184774"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11', '13', '12', '14', '50', '1', '3', '2', '5', '4', '7', '6', '5b']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag\n",
    "td_wd_predictions_by_code.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11', '13', '12', '14', '50', '1', '3', '2', '5', '4', '7', '6', '5b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd_wd_predictions_by_code.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len, td_wd_predictions_by_code.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545,\n",
       " 36545]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len, vd_wd_predictions_by_code.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080093035983034619"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(td_wd_predictions_by_code[\"50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080093035983034619"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vd_wd_predictions_by_code[\"50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_wd_predictions_by_code[\"50\"] == vd_wd_predictions_by_code[\"50\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
