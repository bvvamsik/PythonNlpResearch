{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/\n",
      "\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/All_Files/\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Training/\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Test/\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "\n",
    "PCT_TEST_SPLIT = 0.2\n",
    "\n",
    "#source (TO change)\n",
    "#essay_folder = \"CoralBleaching/BrattData/EBA1415_Merged/\"\n",
    "essay_folder = \"SkinCancer/EBA1415_Merged/\"\n",
    "data_dir = \"/Users/simon.hughes/Google Drive/Phd/Data/\"\n",
    "\n",
    "folder = data_dir + essay_folder\n",
    "\n",
    "root_folder = data_dir + essay_folder.split(\"/\")[0] + \"/\"\n",
    "print \"Root: \" + root_folder\n",
    "\n",
    "all_folder      = root_folder + \"All_Files/\"\n",
    "test_folder     = root_folder + \"Test/\"\n",
    "training_folder = root_folder + \"Training/\"\n",
    "\n",
    "print \"\"\n",
    "for f in [all_folder, training_folder, test_folder]:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map .ann And .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "def find_files(folder, regex, remove_empty = False):\n",
    "    \"\"\"\n",
    "    Find all files matching the [regex] pattern in [folder]\n",
    "\n",
    "    folder  :   string\n",
    "                    folder to search (not recursive)\n",
    "    regex   :   string (NOT regex object)\n",
    "                    pattern to match\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder)\n",
    "    matches = [os.path.abspath(os.path.join(folder, f))\n",
    "               for f in files\n",
    "               if re.search(regex, f, re.IGNORECASE)]\n",
    "\n",
    "    if remove_empty:\n",
    "        matches = [f for f in matches if os.path.getsize(f) > 0]\n",
    "    matches.sort()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107, 1114)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_files = find_files(folder, \"\\.ann$\", remove_empty=True)\n",
    "txt_files = find_files(folder, \"\\.txt$\", remove_empty=True)\n",
    "len(ann_files), len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107, 1114)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prefix(fname):\n",
    "    return fname[:-4]\n",
    "\n",
    "dct_ann = defaultdict(set)\n",
    "for ann in ann_files:\n",
    "    prefix = get_prefix(ann)\n",
    "    dct_ann[prefix].add(ann)\n",
    "\n",
    "dct_txt = defaultdict(set)\n",
    "for txt in txt_files:\n",
    "    prefix = get_prefix(txt)\n",
    "    dct_txt[prefix].add(txt)\n",
    "\n",
    "len(dct_ann), len(dct_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Represents a Mapping Between Prefixes and (.ann,.txt) Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = {}\n",
    "for prfx in dct_ann.keys():\n",
    "    if prfx in dct_txt:\n",
    "        joined[prfx] = (dct_ann[prfx], dct_txt[prfx])\n",
    "len(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Parse Actual The Essay Files (Removing Invalid and Empty Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "1107 files found\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_BGJD_1_SC_ES-05728.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_BGJD_1_SC_ES-5726_9.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_KYLS_6_SC_ES-05674.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_LRJE_7_SC_ES-05142.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_RDCS_1_SC_ES-04696.ann file as .txt file is no essay//'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SVJJ_2_SC_ES-05617.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SWAF_1_SC_ES-04832.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SWAF_1_SC_ES-04834.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SWSP_1_SC_ES-04853.ann file as .txt file is no essay  //'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SYMS_3_SC_ES-05900.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_SYMS_4_SC_ES-05980.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TFHC_1_SC_ES-05937.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TRDJ_11_SC_ES-05721.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TRKM_1_SC_ES-05026.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TTKP_4-5_SC_ES-04924.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TWDG_11_SC_ES-05463.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TWJB_7_SC_ES-05897.ann file as .txt file is no essay.'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_TWNB_2_SC_ES-04977.ann file as .txt file is no essay'\n",
      "Skipping /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/EBA1415_Merged/EBA1415_WSAL_2_SC_ES-05361.ann file as .txt file is no essay'\n",
      "1088 essays processed\n",
      "CPU times: user 5.44 s, sys: 185 ms, total: 5.63 s\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from BrattEssay import load_bratt_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "settings = Settings()\n",
    "essays = load_bratt_essays(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Remove Invalid Files from the Joined List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1088, 1088, 1107)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_essays = set(map(lambda essay: get_prefix(essay.full_path), essays))\n",
    "match = set((e for e in valid_essays if e in joined))\n",
    "len(valid_essays), len(match), len(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Essays deleted\n",
      "1088 1088 1088\n"
     ]
    }
   ],
   "source": [
    "deleted = 0\n",
    "for e in list(joined.keys()):\n",
    "    if e not in match:\n",
    "        del joined[e]\n",
    "        deleted+=1\n",
    "print deleted, \"Essays deleted\"\n",
    "print len(valid_essays), len(match), len(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate the Partition Sizes (in ints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 218 \tTrain: 870 \tTotal: 1088\n",
      "20.0368 %\n"
     ]
    }
   ],
   "source": [
    "num = len(joined.keys())\n",
    "n_test = int(round(PCT_TEST_SPLIT * len(joined.keys())))\n",
    "print \"Test:\", n_test, \"\\tTrain:\", num - n_test, \"\\tTotal:\", num\n",
    "print round(float(n_test) / num * 100,4), \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert num == len(essays), \"Total should match number of essays\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Partition, and Compute Code Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Counts Across Whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684521, 10670, 112)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm_wds = 0\n",
    "num_sentences = 0\n",
    "all_codes = set()\n",
    "for essay in essays:\n",
    "    for sentence in essay.tagged_sentences:\n",
    "        num_sentences+=1\n",
    "        for wd, tags in sentence:\n",
    "            num_wds +=1\n",
    "            all_codes.update(tags)\n",
    "num_wds, num_sentences, len(all_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Codes by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular ['1', '2', '3', '4', '5', '6', '11', '12', '50']\n",
      "causal Causer:1->Result:2\n",
      "Causer:1->Result:3\n",
      "Causer:1->Result:4\n",
      "Causer:1->Result:5\n",
      "Causer:1->Result:50\n",
      "Causer:1->Result:Anaphor\n",
      "Causer:1->Result:rhetorical\n",
      "Causer:11->Result:12\n",
      "Causer:11->Result:3\n",
      "Causer:11->Result:4\n",
      "Causer:11->Result:5\n",
      "Causer:11->Result:50\n",
      "Causer:11->Result:Anaphor\n",
      "Causer:12->Result:12\n",
      "Causer:12->Result:2\n",
      "Causer:12->Result:3\n",
      "Causer:12->Result:4\n",
      "Causer:12->Result:5\n",
      "Causer:12->Result:50\n",
      "Causer:12->Result:Anaphor\n",
      "Causer:2->Result:1\n",
      "Causer:2->Result:11\n",
      "Causer:2->Result:2\n",
      "Causer:2->Result:3\n",
      "Causer:2->Result:4\n",
      "Causer:2->Result:5\n",
      "Causer:2->Result:50\n",
      "Causer:2->Result:6\n",
      "Causer:2->Result:Anaphor\n",
      "Causer:2->Result:rhetorical\n",
      "Causer:3->Result:11\n",
      "Causer:3->Result:2\n",
      "Causer:3->Result:4\n",
      "Causer:3->Result:5\n",
      "Causer:3->Result:50\n",
      "Causer:3->Result:6\n",
      "Causer:3->Result:Anaphor\n",
      "Causer:3->Result:rhetorical\n",
      "Causer:4->Result:11\n",
      "Causer:4->Result:12\n",
      "Causer:4->Result:4\n",
      "Causer:4->Result:5\n",
      "Causer:4->Result:50\n",
      "Causer:4->Result:6\n",
      "Causer:4->Result:Anaphor\n",
      "Causer:5->Result:12\n",
      "Causer:5->Result:4\n",
      "Causer:5->Result:5\n",
      "Causer:5->Result:50\n",
      "Causer:5->Result:6\n",
      "Causer:5->Result:Anaphor\n",
      "Causer:5->Result:rhetorical\n",
      "Causer:50->Result:2\n",
      "Causer:50->Result:3\n",
      "Causer:50->Result:4\n",
      "Causer:50->Result:5\n",
      "Causer:50->Result:other\n",
      "Causer:6->Result:3\n",
      "Causer:6->Result:4\n",
      "Causer:6->Result:5\n",
      "Causer:6->Result:50\n",
      "Causer:6->Result:Anaphor\n",
      "Causer:Anaphor->Result:1\n",
      "Causer:Anaphor->Result:11\n",
      "Causer:Anaphor->Result:12\n",
      "Causer:Anaphor->Result:2\n",
      "Causer:Anaphor->Result:3\n",
      "Causer:Anaphor->Result:4\n",
      "Causer:Anaphor->Result:5\n",
      "Causer:Anaphor->Result:50\n",
      "Causer:Anaphor->Result:6\n",
      "Causer:Anaphor->Result:Anaphor\n",
      "Causer:rhetorical->Result:50\n"
     ]
    }
   ],
   "source": [
    "regular_codes = sorted([c for c in all_codes if c[0].isdigit()], key=lambda c: (len(c), c))\n",
    "causal_codes = sorted([c for c in all_codes if \"->\" in c])\n",
    "print \"regular\", regular_codes\n",
    "print \"causal\", \"\\n\".join(causal_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_tag_distributions(essays, codes=None):\n",
    "    if not codes:\n",
    "        codes = all_codes\n",
    "    else:\n",
    "        codes = set(codes)\n",
    "        \n",
    "    wd_tag_freq = defaultdict(int)\n",
    "    sent_tag_freq = defaultdict(int)\n",
    "    num_wds = 0\n",
    "    num_sentences = 0\n",
    "    for essay in essays:\n",
    "        for sentence in essay.tagged_sentences:\n",
    "            num_sentences+=1\n",
    "            # compute unique tags for sentence for computing sentence distributions\n",
    "            un_tags = set()\n",
    "            for wd, tags in sentence:\n",
    "                num_wds +=1\n",
    "                tags = tags.intersection(codes)\n",
    "                if not tags:\n",
    "                    continue\n",
    "        \n",
    "                un_tags.update(tags)\n",
    "                for tag in tags:\n",
    "                    wd_tag_freq[tag] += 1\n",
    "            for tag in un_tags:\n",
    "                sent_tag_freq[tag] += 1\n",
    "    \n",
    "    for tag in wd_tag_freq.keys():\n",
    "        wd_tag_freq[tag] = float(wd_tag_freq[tag])/ float(num_wds)\n",
    "        sent_tag_freq[tag] = float(sent_tag_freq[tag] / float(num_sentences))\n",
    "    \n",
    "    return wd_tag_freq, sent_tag_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def to_df(freq_dct):\n",
    "    return pd.DataFrame({\"codes\": freq_dct.keys(), \"freq\": freq_dct.values()}).sort_values(\"codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0.003311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.005218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.033306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.022969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.018607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.028226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.063649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>0.028756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  codes      freq\n",
       "3     1  0.033803\n",
       "0    11  0.003311\n",
       "1    12  0.005218\n",
       "5     2  0.033306\n",
       "4     3  0.022969\n",
       "7     4  0.018607\n",
       "6     5  0.028226\n",
       "2    50  0.063649\n",
       "8     6  0.028756"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_freq, sent_freq = compute_tag_distributions(essays, regular_codes)\n",
    "to_df(wd_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.134583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0.029803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.060825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.172165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.126898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.077132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.292877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>0.092034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  codes      freq\n",
       "3     1  0.134583\n",
       "0    11  0.029803\n",
       "1    12  0.060825\n",
       "5     2  0.172165\n",
       "4     3  0.126898\n",
       "7     4  0.077132\n",
       "6     5  0.219400\n",
       "2    50  0.292877\n",
       "8     6  0.092034"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df(sent_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.237845427559\n",
      "1.20571696345\n"
     ]
    }
   ],
   "source": [
    "print sum(wd_freq.values()) # A lot of words have 0 tags\n",
    "print sum(sent_freq.values()) # overlapping codes, a small number of sentences with no codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "def random_splt(essays, n_test):\n",
    "    np_essays = np.asarray(essays)\n",
    "    # inplace shuffle\n",
    "    random.shuffle(np_essays)\n",
    "    test, train = np_essays[:n_test], np_essays[n_test:]\n",
    "    assert len(test) == n_test, \"Test set size does not equal expected\"\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Function\n",
    "train, test = random_splt(essays, n_test)\n",
    "assert n_test == len(test), \"Test set not expected size\"\n",
    "assert len(essays) == (len(train) + len(test)), \"Train and Test set sizes should equal total essays size\"\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Kullback-Leibler Divergence to Find the Closest Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log is not defined if the denom is 0. if the denom is 0\n",
    "# this is a bad split, as we want all codes to be represented\n",
    "# so return max_val, as we are trying to minimize the divergence\n",
    "max_val = 99999999\n",
    "def kl(a,b):\n",
    "    join = zip(a,b)\n",
    "    kl_val_a = 0.0\n",
    "    kl_val_b = 0.0\n",
    "    for pa, pb in join:\n",
    "        if pa == 0.0 or pb == 0.0:\n",
    "            return max_val,max_val\n",
    "        kl_val_a += pa*np.log(pa/pb)\n",
    "        kl_val_b += pb*np.log(pb/pa)\n",
    "    return kl_val_a, kl_val_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_wd_tag_dist(test_essays, codes):\n",
    "    all_wd_freq, _ = compute_tag_distributions(essays, codes)\n",
    "    test_wd_freq,_ = compute_tag_distributions(test_essays, codes)\n",
    "    return pd.merge(to_df(all_wd_freq), to_df(test_wd_freq), on=\"codes\", suffixes=[\"all\", \"test\"])\n",
    "\n",
    "def compare_sent_tag_dist(test_essays, codes):\n",
    "    _, all_sent_freq  = compute_tag_distributions(essays, codes)\n",
    "    _, test_sent_freq = compute_tag_distributions(test_essays, codes)\n",
    "    return pd.merge(to_df(all_sent_freq), to_df(test_sent_freq), on=\"codes\", suffixes=[\"all\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Best Split Using Concept Codes Only"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "kl_vals = {}\n",
    "for i in range(1000):\n",
    "    train, test = random_splt(essays, n_test)\n",
    "    test_wd_freq, test_sent_freq = compute_tag_distributions(test, regular_codes)\n",
    "    vals = kl(wd_freq.values(), test_wd_freq.values())\n",
    "    # map to abs values\n",
    "    vals = list(map(abs, vals))\n",
    "    max_val = max(vals)\n",
    "    kl_vals[max_val] = (train,test)\n",
    "\n",
    "# Get best\n",
    "best_kl_code, best_code_split = min(kl_vals.items(), key=lambda (k,v): k)\n",
    "best_kl_code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_, best_testsplit_code = best_code_split\n",
    "best_kl_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Best Split Using Causal Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "kl_vals = {}\n",
    "for i in range(10000):\n",
    "    train, test = random_splt(essays, n_test)\n",
    "    test_wd_freq, test_sent_freq = compute_tag_distributions(test, causal_codes)\n",
    "    vals = kl(wd_tag_freq_causal.values(), test_wd_freq.values())\n",
    "    # map to abs values\n",
    "    vals = list(map(abs, vals))\n",
    "    max_val = max(vals)\n",
    "    kl_vals[max_val] = (train,test)\n",
    "\n",
    "# Get best\n",
    "best_kl_causal, best_split_causal = min(kl_vals.items(), key=lambda (k,v): k)\n",
    "best_kl_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, best_testsplit_causal = best_split_causal\n",
    "best_kl_causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute KL on Regular Codes for the Best Causal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wtfreq, _ = compute_tag_distributions(best_testsplit_causal, regular_codes)\n",
    "vals = kl(wd_freq.values(), wtfreq.values())\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect The Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = compare_wd_tag_dist(best_testsplit_causal, regular_codes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = compare_wd_tag_dist(best_testsplit_causal, causal_codes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot The Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(essays, codes):\n",
    "    df1 = compare_wd_tag_dist(essays, codes)\n",
    "    df2 = pd.DataFrame(df1[[\"freqtest\",\"freqall\"]])\n",
    "    df2.set_index(df1[\"codes\"].values, inplace=True)\n",
    "    df2.plot.bar(figsize=(15,8), color=['r', 'yellow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_distribution(best_testsplit_causal, regular_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_distribution(best_testsplit_causal, causal_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move The Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_file(from_file, to_dir):\n",
    "    to_file = os.path.join(to_dir, os.path.basename(from_file))\n",
    "    if os.path.exists(to_file):\n",
    "        print \"File already exists: %s deleting\" % to_file\n",
    "        os.remove(to_file)\n",
    "        shutil.copyfile(from_file, to_file)                    \n",
    "    else:\n",
    "        shutil.copyfile(from_file, to_file)\n",
    "\n",
    "def copy_files(essays, to_dir):\n",
    "    cnt = 0\n",
    "    \n",
    "    # Delete destination folder first\n",
    "    if os.path.exists(to_dir):\n",
    "        shutil.rmtree(to_dir, ignore_errors=True)\n",
    "        \n",
    "    os.mkdir(to_dir)\n",
    "    \n",
    "    ann_files = [e.full_path for e in essays]\n",
    "    pairs = [joined[get_prefix(f)] for f in ann_files]\n",
    "    \n",
    "    for ann, txt in pairs:\n",
    "        for a in ann:\n",
    "            copy_file(a, to_dir)\n",
    "        for t in txt:\n",
    "            copy_file(t, to_dir)\n",
    "        cnt += 2\n",
    "             \n",
    "    print \"Moved %s files\" % str(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_files(essays, all_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = best_split_causal\n",
    "copy_files(train, training_folder)\n",
    "copy_files(test,  test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "452/2, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:phd]",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
