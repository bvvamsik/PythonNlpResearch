{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Essay Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from BrattEssay import load_bratt_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "\n",
    "root_folder      = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder  = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words To Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "\n",
    "#pre-compute id sequences so we don't have to remap each time\n",
    "#xs\n",
    "wd_id_seqs  = []\n",
    "#ys\n",
    "tag_id_seqs = []\n",
    "# for computing accuracy\n",
    "tag_seqs    = []\n",
    "\n",
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        \n",
    "        wd_ids = []\n",
    "        tag_ids = []\n",
    "        tag_seq = []\n",
    "        \n",
    "        wd_id_seqs.append(wd_ids)\n",
    "        tag_id_seqs.append(tag_ids)\n",
    "        tag_seqs.append(tag_seq)\n",
    "        \n",
    "        for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "            wd_ids.append(word_to_ix[word])\n",
    "            \n",
    "            tags = vtags.intersection(tags)\n",
    "            if len(tags) > 1:\n",
    "                tag = max(tags, key=lambda t: tag_freq[t])                \n",
    "            else:\n",
    "                tag = EMPTY_TAG\n",
    "            if tag not in tag_to_ix:\n",
    "                tag_to_ix[tag] = len(tag_to_ix)\n",
    "            tag_ids.append(tag_to_ix[tag])\n",
    "            tag_seq.append(tag)\n",
    "            \n",
    "# invert the dictionary\n",
    "ix_to_tag  = dict(zip(tag_to_ix.values(),  tag_to_ix.keys()))\n",
    "ix_to_word = dict(zip(word_to_ix.values(), word_to_ix.keys()))\n",
    "\n",
    "len(tag_id_seqs), len(wd_id_seqs)\n",
    "wd_seq_lens  = np.asarray(list(map(len, wd_id_seqs)))\n",
    "tag_seq_lens = np.asarray(list(map(len, tag_id_seqs)))\n",
    "assert set(wd_seq_lens == tag_seq_lens) == set([True]), \"Sequence lengths must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\\Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6633, 1659, 6633, 1659)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20% split\n",
    "test_split = 0.2\n",
    "train_size = int((1-test_split) * len(wd_id_seqs))\n",
    "\n",
    "train_xs,   test_xs   = wd_id_seqs[:train_size],  wd_id_seqs[train_size:]\n",
    "train_tags, test_tags = tag_id_seqs[:train_size], tag_id_seqs[train_size:]\n",
    "len(train_xs), len(test_xs), len(train_tags), len(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.Tensor(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.Tensor(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To and From Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_id_sequence(id_seq):\n",
    "    tensor = torch.LongTensor(id_seq)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w], seq))\n",
    "    return prepare_id_sequence(idxs, to_ix)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a list of python ints\n",
    "    _, ixs = torch.max(vec, 1)\n",
    "    return ixs.view(-1).data.tolist()\n",
    "\n",
    "def argmin(vec):\n",
    "    # return the argmax as a list of python ints\n",
    "    _, ixs = torch.min(vec, 1)\n",
    "    return ixs.view(-1).data.tolist()\n",
    "\n",
    "def map_to(vals, to_other_val):\n",
    "    return list(map(lambda v: to_other_val[v], vals))\n",
    "\n",
    "def to_tags(scores, to_tag):\n",
    "    ids = argmax(scores)\n",
    "    return map_to(ids, to_tag)\n",
    "\n",
    "def to_tags_min(scores, to_tag):\n",
    "    ids = argmin(scores)\n",
    "    return map_to(ids, to_tag)\n",
    "\n",
    "to_tags(tag_scores, ix_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5794 -1.5555 -1.6675 -1.6013 -1.6479\n",
      "-1.5386 -1.5978 -1.7189 -1.6167 -1.5839\n",
      "-1.5793 -1.5543 -1.7603 -1.6261 -1.5425\n",
      "-1.5910 -1.6069 -1.6120 -1.7206 -1.5263\n",
      "-1.5610 -1.6532 -1.5819 -1.7082 -1.5519\n",
      "-1.5518 -1.6337 -1.6723 -1.6566 -1.5403\n",
      "-1.6097 -1.5640 -1.7140 -1.5439 -1.6242\n",
      "-1.6030 -1.6098 -1.5694 -1.6509 -1.6159\n",
      "-1.6333 -1.5828 -1.5434 -1.6430 -1.6488\n",
      "-1.5716 -1.6698 -1.5252 -1.7354 -1.5602\n",
      "-1.5349 -1.6556 -1.4689 -1.7087 -1.7023\n",
      "-1.5445 -1.6405 -1.4832 -1.7611 -1.6402\n",
      "-1.5610 -1.6171 -1.5855 -1.6137 -1.6735\n",
      "[torch.FloatTensor of size 13x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "#inputs = prepare_sequence(train_xs[0], word_to_ix)\n",
    "inputs = prepare_id_sequence(train_xs[0])\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Epoch=1\n",
      "Epoch=2\n",
      "Epoch=3\n",
      "Epoch=4\n",
      "Epoch=5\n",
      "Epoch=6\n",
      "Epoch=7\n",
      "Epoch=8\n",
      "Epoch=9\n",
      "Epoch=10\n",
      "Epoch=11\n",
      "Epoch=12\n",
      "Epoch=13\n",
      "Epoch=14\n",
      "Epoch=15\n",
      "Epoch=16\n",
      "Epoch=17\n",
      "Epoch=18\n",
      "Epoch=19\n",
      "Epoch=20\n",
      "Epoch=21\n",
      "Epoch=22\n",
      "Epoch=23\n",
      "Epoch=24\n",
      "Epoch=25\n",
      "Epoch=26\n",
      "Epoch=27\n",
      "Epoch=28\n",
      "Epoch=29\n",
      "Epoch=30\n",
      "Epoch=31\n",
      "Epoch=32\n",
      "Epoch=33\n",
      "Epoch=34\n",
      "Epoch=35\n",
      "Epoch=36\n",
      "Epoch=37\n",
      "Epoch=38\n",
      "Epoch=39\n",
      "Epoch=40\n",
      "Epoch=41\n",
      "Epoch=42\n",
      "Epoch=43\n",
      "Epoch=44\n",
      "Epoch=45\n",
      "Epoch=46\n",
      "Epoch=47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-71819c9a72b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from numpy.random import shuffle\\n#TODO - shuffle on each epoch\\n#TODO - prepare the sequences once only\\nfor epoch in range(100): # again, normally you would NOT do 300 epochs, it is toy data\\n    print(\"Epoch={epoch}\".format(epoch=epoch))\\n    ixs = list(range(len(train_xs)))\\n    # shuffle order of sequences\\n    shuffle(ixs)\\n    for ix in ixs:\\n        sentence, tags = train_xs[ix], train_tags[ix]\\n        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\\n        # before each instance\\n        model.zero_grad()\\n    \\n        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\\n        # of word indices.\\n        sentence_in = prepare_id_sequence(sentence)\\n        targets = prepare_id_sequence(tags)\\n    \\n        # Step 3. Run our forward pass.\\n        tag_scores = model(sentence_in)\\n    \\n        # Step 4. Compute the loss, gradients, and update the parameters by calling\\n        # optimizer.step()\\n        loss = loss_function(tag_scores, targets)\\n        loss.backward()\\n        optimizer.step()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/simon.hughes/anaconda3/envs/pytorch_src/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/simon.hughes/anaconda3/envs/pytorch_src/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/simon.hughes/anaconda3/envs/pytorch_src/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/simon.hughes/anaconda3/envs/pytorch_src/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    144\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/simon.hughes/anaconda3/envs/pytorch_src/lib/python3.5/site-packages/torch/nn/_functions/linear.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.random import shuffle\n",
    "#TODO - shuffle on each epoch\n",
    "#TODO - prepare the sequences once only\n",
    "for epoch in range(100): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print(\"Epoch={epoch}\".format(epoch=epoch))\n",
    "    ixs = list(range(len(train_xs)))\n",
    "    # shuffle order of sequences\n",
    "    shuffle(ixs)\n",
    "    for ix in ixs:\n",
    "        sentence, tags = train_xs[ix], train_tags[ix]\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n",
    "        # of word indices.\n",
    "        sentence_in = prepare_id_sequence(sentence)\n",
    "        targets = prepare_id_sequence(tags)\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "   0.0000 -302.7565 -247.2489 -305.4602 -265.1942\n",
      "   0.0000 -272.5114  -62.5185 -151.4068 -129.3555\n",
      "   0.0000 -280.8776 -254.0998 -262.9916 -302.4913\n",
      "   0.0000 -330.3940 -196.0099 -304.3195 -303.8049\n",
      "   0.0000 -286.1771 -198.9735 -358.3427 -216.7727\n",
      "   0.0000 -354.5726 -357.4479 -344.7640 -284.7492\n",
      "   0.0000 -337.5768 -265.1843 -329.7178 -299.0603\n",
      "   0.0000 -301.0872 -262.6500 -335.9150 -237.4075\n",
      "   0.0000 -298.4237 -140.0931 -201.3633 -189.1251\n",
      "   0.0000 -163.0376 -204.5023 -185.8618 -193.1536\n",
      "   0.0000 -204.5024 -239.4676 -163.8020 -231.5441\n",
      "   0.0000 -290.3929 -375.9790 -320.7814 -331.4705\n",
      "   0.0000 -236.4848 -310.6095 -187.0103 -354.3151\n",
      "[torch.FloatTensor of size 13x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "inputs = prepare_id_sequence(train_xs[0])\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', '7'),\n",
       " ('what', '50'),\n",
       " ('leads', '4'),\n",
       " ('to', '50'),\n",
       " ('different', '3'),\n",
       " ('in', '50'),\n",
       " ('the', '7'),\n",
       " ('rates', '7'),\n",
       " ('of', '50'),\n",
       " ('coral', '4'),\n",
       " ('bleaching', '4'),\n",
       " ('is', '50'),\n",
       " ('that', '50'),\n",
       " ('INFREQUENT', '4'),\n",
       " ('different', '3'),\n",
       " ('types', '7'),\n",
       " ('of', '50'),\n",
       " ('INFREQUENT', '4'),\n",
       " ('living', '50'),\n",
       " ('within', '50'),\n",
       " ('the', '7'),\n",
       " ('coral', '3'),\n",
       " ('polyps', '4'),\n",
       " ('gives', '3'),\n",
       " ('the', '4'),\n",
       " ('coral', '3'),\n",
       " ('their', '3'),\n",
       " ('varying', '4'),\n",
       " ('colors', '3'),\n",
       " ('.', '3'),\n",
       " ('<end>', '4')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 10\n",
    "inputs = prepare_id_sequence(train_xs[ix])\n",
    "tag_scores = model(inputs)\n",
    "\n",
    "list(zip(map_to(train_xs[ix],ix_to_word),to_tags_min(tag_scores, ix_to_tag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def to_ys_by_code(seq_tags):\n",
    "    ys_by_tag = defaultdict(list)\n",
    "    for seq in seq_tags:\n",
    "        for tag in seq:\n",
    "            ys_by_tag[tag].append(1)\n",
    "            for vtag in vtags:\n",
    "                if tag != vtag:\n",
    "                    ys_by_tag.append(0)\n",
    "    return ys_by_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - prepare the sequences as Tensors once only\n",
    "## TODO - use a better optimizer - ADAM maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
