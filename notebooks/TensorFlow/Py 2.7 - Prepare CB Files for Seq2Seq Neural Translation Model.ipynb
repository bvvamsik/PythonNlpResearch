{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">NOTE: Needs Python 2.7</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_folder = \"Training\"\n",
    "#target_folder = \"Test\"\n",
    "\n",
    "# From (essay text) and To Files for the seq2seq model\n",
    "from_file           = \"/Users/simon.hughes/data/tensorflow/translate/cb/%s/inputs.txt\" % target_folder\n",
    "to_file             = \"/Users/simon.hughes/data/tensorflow/translate/cb/%s/output.txt\" % target_folder\n",
    "to_file_most_freq   = \"/Users/simon.hughes/data/tensorflow/translate/cb/%s/output_most_freq.txt\"  % target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "902 files found\n",
      "902 essays processed\n"
     ]
    }
   ],
   "source": [
    "from BrattEssay import load_bratt_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + target_folder + \"/\"\n",
    "\n",
    "essays = load_bratt_essays(training_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flatten Essays, Separate Out Words From Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sentences, sent_tags = [], []\n",
    "wd_freq, tag_freq = defaultdict(int), defaultdict(int)\n",
    "for essay_ix, essay in enumerate(essays):\n",
    "    for sent_ix, taggged_sentence in enumerate(essay.tagged_sentences):\n",
    "        t_sentence = []\n",
    "        sentences.append(t_sentence)\n",
    "        t_tag_seq = []\n",
    "        sent_tags.append(t_tag_seq)\n",
    "        for word_ix, (wd, t) in enumerate(taggged_sentence):\n",
    "            t_sentence.append(wd)\n",
    "            t_tag_seq.append(t)\n",
    "            wd_freq[wd]+=1\n",
    "            for tag in t:\n",
    "                tag_freq[tag] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regular_tags = set((t for t in tag_freq.keys() if t[0].isdigit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  Vocabulary: 4286\n",
      "Output Vocabulary: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTraining:\\nInput  Vocabulary: 4286\\nOutput Vocabulary: 13\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Input  Vocabulary: \" + str(len(wd_freq)))\n",
    "print(\"Output Vocabulary: \" + str(len(regular_tags)))\n",
    "\"\"\"\n",
    "Training:\n",
    "Input  Vocabulary: 4286\n",
    "Output Vocabulary: 13\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collapse Tags to Most Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regular_tag_seq = []\n",
    "for tag_seq in sent_tags:\n",
    "    r_tag_seq = []\n",
    "    regular_tag_seq.append(r_tag_seq)\n",
    "    # for each set of tags in sent\n",
    "    for tag_set in tag_seq:\n",
    "        tag_set = set((t for t in tag_set if t in regular_tags))\n",
    "        r_tag_seq.append(tag_set)\n",
    "del tag_set\n",
    "del r_tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_freq_tags = []\n",
    "None_Tag = \"None\"\n",
    "# for each sentence\n",
    "for tag_seq in regular_tag_seq:\n",
    "    most_freq = []\n",
    "    most_freq_tags.append(most_freq)\n",
    "    # for each set of tags in sent\n",
    "    for tag_set in tag_seq:\n",
    "        tag_set = set((t for t in tag_set if t in regular_tags))\n",
    "        if len(tag_set) == 0:\n",
    "            most_freq.append(None_Tag)\n",
    "        else:\n",
    "            tag = max(tag_set, key = lambda t:tag_freq[t])\n",
    "            most_freq.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate All of the Lengths Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 1918, 1918)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(most_freq_tags), len(regular_tag_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ix, (sent, tags, mfreq_tags) in enumerate(zip(sentences, most_freq_tags, regular_tag_seq)):\n",
    "    assert len(sent) == len(tags) == len(mfreq_tags), \"Lengths differ at %i\" % ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output For the Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(fname, sentences, mapper = lambda s:s):\n",
    "    with open(fname, \"w+\") as f:\n",
    "        for sent in sentences:\n",
    "            mapped = map(mapper, sent)\n",
    "            str_sent = \" \".join(mapped).strip()\n",
    "            f.write(str_sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_file(from_file,           sentences=sentences)\n",
    "write_to_file(to_file,             sentences=regular_tag_seq, mapper = lambda st: None_Tag if not st else \",\".join(sorted(st)))\n",
    "write_to_file(to_file_most_freq,   sentences=most_freq_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
