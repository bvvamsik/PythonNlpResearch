{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-07-17 20:54:07.758742\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAUSER = \"Causer\"\n",
    "RESULT = \"Result\"\n",
    "EXPLICIT = \"explicit\"\n",
    "CAUSER_EXPLICIT = \"Causer_Explicit\"\n",
    "EXPLICIT_RESULT = \"Explicit_Result\"\n",
    "CAUSER_EXPLICIT_RESULT = \"Causer_Explicit_Result\"\n",
    "CAUSER_RESULT = \"Causer_Result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "#TODO - don't ignore Anaphor, other and rhetoricals here\n",
    "cr_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "#vtags.add(EMPTY_TAG)\n",
    "#cr_tags = vtags\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50', '5', '4', '11', '13', '3', '7', '1', '6', '14', '2', '12', '5b']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:11',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:12->Result:11']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Anaphor',\n",
       " 'COMPILED',\n",
       " 'Causer',\n",
       " 'Causer:1',\n",
       " 'Causer:11',\n",
       " 'Causer:12',\n",
       " 'Causer:13',\n",
       " 'Causer:14',\n",
       " 'Causer:2',\n",
       " 'Causer:3',\n",
       " 'Causer:4',\n",
       " 'Causer:5',\n",
       " 'Causer:50',\n",
       " 'Causer:5b',\n",
       " 'Causer:6',\n",
       " 'Causer:7',\n",
       " 'Causer:Anaphor',\n",
       " 'Causer:other',\n",
       " 'Causer:rhetorical',\n",
       " 'Result',\n",
       " 'Result:1',\n",
       " 'Result:11',\n",
       " 'Result:12',\n",
       " 'Result:13',\n",
       " 'Result:14',\n",
       " 'Result:2',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:5',\n",
       " 'Result:50',\n",
       " 'Result:5b',\n",
       " 'Result:6',\n",
       " 'Result:7',\n",
       " 'Result:Anaphor',\n",
       " 'Result:other',\n",
       " 'Result:rhetorical',\n",
       " 'explicit',\n",
       " 'other',\n",
       " 'rhetorical']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in sorted(tag_freq.keys()) if \"->\" not in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "config[\"window_size\"] = WINDOW_SIZE\n",
    "offset = int((config[\"window_size\"] - 1) / 2)\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attach_function_identifier(fn, d):\n",
    "    try:\n",
    "        s = fn.func_name + \"[\"\n",
    "    except:\n",
    "        s = fn.__name__ + \"[\"\n",
    "        fn.func_name = fn.__name__\n",
    "\n",
    "    for k, v in sorted(d.items(), key = lambda tpl: tpl[0]):\n",
    "        if k == fn.func_name:\n",
    "            continue\n",
    "        if type(v) == dict:\n",
    "            continue\n",
    "        s +=   \"%s:%s \" % (str(k), str(v))\n",
    "    fn.func_name = s.strip() + \"]\"\n",
    "    return fn\n",
    "\n",
    "def fact_extract_positional_word_features_stemmed(offset):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    lcls = locals()\n",
    "    # curry offset\n",
    "    def fn_pos_wd_feats_stemmed(input, val=1):\n",
    "        return extract_positional_word_features_stemmed(offset, input, val)\n",
    "    return attach_function_identifier(fn_pos_wd_feats_stemmed, lcls) # recently renamed for mongodob logging\n",
    "\n",
    "def extract_positional_word_features_stemmed(offset, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    start = input.wordix - offset\n",
    "    stop  = input.wordix + offset\n",
    "\n",
    "    end = len(input.sentence) - 1\n",
    "    for i in range(start, stop+1):\n",
    "        relative_offset = str(i - input.wordix)\n",
    "        if i < 0:\n",
    "            feats[\"WD\" +__START__ + \":\" + relative_offset] = val\n",
    "        elif i > end:\n",
    "            feats[\"WD\" +__END__ + \":\" + relative_offset] = val\n",
    "        else:\n",
    "            offset_word = stem(input.sentence[i])\n",
    "            feats[\"WD:\" + relative_offset + \"->\" + offset_word] = val\n",
    "    return feats\n",
    "\n",
    "def fact_extract_ngram_features_stemmed(offset, ngram_size):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    lcls = locals()\n",
    "    # curry offset and ngram size\n",
    "    def fn_pos_ngram_feat_stemmed(input, val=1):\n",
    "        return extract_ngram_features_stemmed(offset, ngram_size, input, val)\n",
    "    return attach_function_identifier(fn_pos_ngram_feat_stemmed, lcls)\n",
    "\n",
    "def extract_ngram_features_stemmed(offset, ngram_size, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    end = len(input.sentence) - 1\n",
    "\n",
    "    # fix to within bounds only\n",
    "    start = max(0, input.wordix - offset)\n",
    "    stop  = min(end, input.wordix + offset)\n",
    "\n",
    "    window = list(input.sentence[start:stop+1])\n",
    "    window = list(map(stem, window))\n",
    "    if input.wordix < offset:\n",
    "        diff = offset - input.wordix\n",
    "        for i in range(diff):\n",
    "            window.insert(0,__START__)\n",
    "    if input.wordix + offset > end:\n",
    "        diff = input.wordix + offset - end\n",
    "        for i in range(diff):\n",
    "            window.append(__END__)\n",
    "\n",
    "    ngrams = compute_ngrams(window, ngram_size, ngram_size)\n",
    "    str_num_ngrams = str(ngram_size)\n",
    "\n",
    "    for i, offset_ngram in enumerate(ngrams):\n",
    "        relative_offset = str(i - offset)\n",
    "        str_ngram = \",\".join(offset_ngram)\n",
    "        feats[\"POS_\" + str_num_ngrams + \"GRAMS:\" + relative_offset + \"->\" + str_ngram] = val\n",
    "\n",
    "    return feats\n",
    "\n",
    "def fact_extract_bow_ngram_features(offset, ngram_size):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    # curry offset and ngram size\n",
    "    lcls = locals()\n",
    "    def fn_bow_ngram_feat(input, val=1):\n",
    "        return extract_bow_ngram_features(offset, ngram_size, input, val)\n",
    "    return attach_function_identifier(fn_bow_ngram_feat, lcls)\n",
    "\n",
    "def extract_bow_ngram_features(offset, ngram_size, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    end = len(input.sentence) - 1\n",
    "\n",
    "    # fix to within bounds only\n",
    "    start = max(0, input.wordix - offset)\n",
    "    stop  = min(end, input.wordix + offset)\n",
    "\n",
    "    window = list(input.sentence[start:stop+1])\n",
    "    if input.wordix < offset:\n",
    "        diff = offset - input.wordix\n",
    "        for i in range(diff):\n",
    "            window.insert(0,__START__)\n",
    "    if input.wordix + offset > end:\n",
    "        diff = input.wordix + offset - end\n",
    "        for i in range(diff):\n",
    "            window.append(__END__)\n",
    "\n",
    "    ngrams = compute_ngrams(window, ngram_size, ngram_size)\n",
    "    str_num_ngrams = str(ngram_size)\n",
    "\n",
    "    for i, offset_ngram in enumerate(ngrams):\n",
    "        str_ngram = \",\".join(offset_ngram)\n",
    "        feats[\"POS_\" + str_num_ngrams + \"GRAMS:BOW\" + \"->\" + str_ngram] = val\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__START__ = \"<START>\"\n",
    "__END__   = \"<END>\"\n",
    "\n",
    "#from featureextractionfunctions \\\n",
    "    #import fact_extract_positional_word_features_stemmed, fact_extract_ngram_features_stemmed,\\\n",
    "    #fact_extract_bow_ngram_features, extract_brown_cluster    \n",
    "    #extract_dependency_relation\n",
    "\n",
    "unigram_window_stemmed  = fact_extract_positional_word_features_stemmed(offset)\n",
    "biigram_window_stemmed  = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "triigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 3)\n",
    "unigram_bow_window      = fact_extract_bow_ngram_features(offset, 1)\n",
    "\n",
    "#TODO - add in full feature set, but keep simple for now\n",
    "#optimal CB feature set\n",
    "extractors = [\n",
    "    unigram_window_stemmed,\n",
    "    biigram_window_stemmed,\n",
    "    #triigram_window_stemmed,\n",
    "    #unigram_bow_window,\n",
    "    #extract_dependency_relation,\n",
    "    #extract_brown_cluster\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Decorators import memoize\n",
    "from nltk import PorterStemmer\n",
    "from NgramGenerator import compute_ngrams\n",
    "from featureextractortransformer import FeatureExtractorTransformer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "@memoize\n",
    "def stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "feature_extractor = FeatureExtractorTransformer(extractors)\n",
    "essay_feats = feature_extractor.transform(tagged_essays)\n",
    "len(tagged_essays), len(essay_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data - Essays Tagged with Codes By a Bi-Directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "fname = predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays = dill.load(f)\n",
    "\n",
    "len(tagged_essays), len(pred_tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays):\n",
    "    essay2pred = {}\n",
    "    for essay in pred_tagged_essays:\n",
    "        essay2pred[essay.name] = essay\n",
    "\n",
    "    for essay in essay_feats:\n",
    "        pred_essay = essay2pred[essay.name]\n",
    "        for six, sent in enumerate(essay.sentences):\n",
    "            pred_sent = pred_essay.pred_tagged_sentences[six]\n",
    "            assert len(pred_sent) == len(sent), \"Miss-match on sentences\"\n",
    "            for wix, wd in enumerate(sent):\n",
    "                wd.predicted_tag = pred_sent[wix]\n",
    "    \n",
    "assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stack(object):\n",
    "    def __init__(self, verbose=False):    \n",
    "        self.stack = []\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def tos(self):\n",
    "        if self.len() == 0:\n",
    "            return None\n",
    "        #assert self.len() > 0, \"Can't peek when stack is empty\"\n",
    "        return self.stack[-1]\n",
    "    \n",
    "    def pop(self):\n",
    "        assert self.len() > 0, \"Can't pop when stack is empty\"\n",
    "        item = self.stack.pop()\n",
    "        if self.verbose:\n",
    "            print(\"POPPING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "        return item\n",
    "    \n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "        if self.verbose:\n",
    "            print(\"PUSHING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "    def contains(self, item):\n",
    "        return item in self.stack\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"|\".join(self.stack)\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Stack(self.verbose)\n",
    "        clone.stack = list(self.stack)\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT = \"root\"\n",
    "\n",
    "def norm_arc(arc):\n",
    "    return tuple(sorted(arc))\n",
    "\n",
    "def norm_arcs(arcs):\n",
    "    return set(map(norm_arc, arcs))\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, stack):\n",
    "        self.stack = stack\n",
    "        self.arcs = []\n",
    "        self.normed_arcs = set()\n",
    "        # nodes with heads\n",
    "        self.children = set()\n",
    "        self.actions = []\n",
    "        \n",
    "    def get_dependencies(self):\n",
    "        return [(l,r) for (l,r) in self.arcs if r != ROOT and l != ROOT]\n",
    "        \n",
    "    def left_arc(self, buffer):\n",
    "        tos = self.stack.pop()\n",
    "        #Pre-condition\n",
    "        #assert self.has_head(tos) == False\n",
    "        arc = (tos,buffer)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(arc)\n",
    "        self.children.add(tos)\n",
    "        self.actions.append(\"L ARC   : \" + tos + \"->\" + buffer)\n",
    "        \n",
    "    def right_arc(self, buffer):\n",
    "        tos = self.stack.tos()\n",
    "        #normalize arc\n",
    "        arc = (buffer,tos)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(n_arc)\n",
    "        self.actions.append(\"R ARC   : \" + tos + \"<-\" + buffer)\n",
    "        self.children.add(buffer)\n",
    "        self.stack.push(buffer)\n",
    "        \n",
    "    def reduce(self):\n",
    "        tos = self.stack.pop()\n",
    "        #assert self.has_head(tos) == True\n",
    "        self.actions.append(\"REDUCE  : Pop  %s\" % tos)\n",
    "        \n",
    "    def shift(self, buffer):\n",
    "        self.stack.push(buffer)\n",
    "        self.actions.append(\"SHIFT   : Push %s\" % buffer)\n",
    "    \n",
    "    def skip(self, buffer):\n",
    "        self.actions.append(\"SKIP    : item %s\" % buffer)\n",
    "    \n",
    "    def has_head(self, item):\n",
    "        return item in self.children\n",
    "    \n",
    "    def in_stack(self, item):\n",
    "        return self.stack.contains(item)\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Parser(self.stack.clone())\n",
    "        clone.arcs = list(self.arcs)\n",
    "        clone.normed_arcs = set(self.normed_arcs)\n",
    "        # nodes with heads\n",
    "        clone.children = set(self.children)\n",
    "        clone.actions = list(self.actions)\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "SHIFT = \"Shift\"\n",
    "REDUCE = \"Reduce\"\n",
    "LARC = \"LArc\"\n",
    "RARC = \"Rarc\"\n",
    "SKIP = \"Skip\"\n",
    "\n",
    "class Oracle(object):\n",
    "    \n",
    "    def __init__(self, crels, parser):\n",
    "        self.parser = parser\n",
    "        self.crels = norm_arcs(crels) # type: Set[Tuple[str,str]]\n",
    "        self.mapping = self.build_mappings(crels)\n",
    "    \n",
    "    def build_mappings(self, pairs):\n",
    "        mapping = defaultdict(set)\n",
    "        for c,res in pairs:\n",
    "            mapping[c].add(res)\n",
    "            mapping[res].add(c)\n",
    "        return mapping\n",
    "\n",
    "    def should_continue(self, action):\n",
    "        # continue parsing if REDUCE or LARC\n",
    "        return action in (REDUCE,LARC)\n",
    "    \n",
    "    def remove_relation(self, a,b):\n",
    "        self.mapping[a].remove(b)\n",
    "        if len(self.mapping[a]) == 0:\n",
    "            del self.mapping[a]\n",
    "        self.mapping[b].remove(a)\n",
    "        if len(self.mapping[b]) == 0:\n",
    "            del self.mapping[b]\n",
    "    \n",
    "    def consult(self, tos, buffer):\n",
    "        \"\"\"\n",
    "        Performs optimal decision for parser\n",
    "        If true, continue processing, else Consume Buffer\n",
    "        \"\"\"\n",
    "        parser = self.parser\n",
    "        a,b = norm_arc((tos, buffer))\n",
    "        if (a,b) in self.crels:\n",
    "            # TOS has arcs remaining? If so, we need RARC, else LARC\n",
    "            if len(self.mapping[tos]) == 1:\n",
    "                return LARC\n",
    "            else:\n",
    "                return RARC\n",
    "        else:\n",
    "            if buffer not in self.mapping:\n",
    "                return SKIP\n",
    "            # If the buffer has relations further down in the stack, we need to POP the TOS\n",
    "            for item in self.mapping[buffer]:\n",
    "                if item == tos:\n",
    "                    continue\n",
    "                if parser.in_stack(item):\n",
    "                    return REDUCE\n",
    "            #end for\n",
    "            #ELSE\n",
    "            return SHIFT\n",
    "        \n",
    "    def execute(self, action, tos, buffer):\n",
    "        \"\"\"\n",
    "        Performs optimal decision for parser\n",
    "        If true, continue processing, else Consume Buffer\n",
    "        \"\"\"\n",
    "        parser = self.parser\n",
    "        if action == LARC:\n",
    "            parser.left_arc(buffer)\n",
    "            self.remove_relation(tos, buffer)\n",
    "        elif action == RARC:\n",
    "            parser.right_arc(buffer)\n",
    "            self.remove_relation(tos, buffer)\n",
    "        elif action == REDUCE:\n",
    "            parser.reduce()\n",
    "        elif action == SHIFT:\n",
    "            parser.shift(buffer)\n",
    "        elif action == SKIP:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Unknown parsing action %s\" % action)\n",
    "        return self.should_continue(action)\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Oracle(set(self.crels), self.parser.clone())\n",
    "        clone.mapping = dict(self.mapping.items())\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    def __init__(self, extractors):\n",
    "        self.extractors = extractors\n",
    "    \n",
    "    def extract(self, predicted_tag, word_seq, positive_val=1):\n",
    "        \"\"\"\n",
    "        word: str\n",
    "        word_seq: List[Word]\n",
    "        \n",
    "        returns: List[Dict{str,float}]\n",
    "        \"\"\"\n",
    "        feats = []\n",
    "        for seq in word_seq:\n",
    "            fts = {}\n",
    "            feats.append(fts)\n",
    "            for ext in self.extractors:\n",
    "                new_feats = ext(predicted_tag, seq, positive_val)\n",
    "                fts.update(new_feats)\n",
    "        return feats\n",
    "    \n",
    "def bag_of_word_extractor(predicted_tag, word_seq, positive_val):\n",
    "    feats = {}\n",
    "    for word in word_seq:\n",
    "        feats[\"bow_\" + word.word] = positive_val\n",
    "    return feats\n",
    "\n",
    "def bag_of_word_plus_tag_extractor(predicted_tag, word_seq, positive_val):\n",
    "    feats = {}\n",
    "    for word in word_seq:\n",
    "        feats[\"bow_\" + word.word + \"_tag_\" + predicted_tag] = positive_val\n",
    "    return feats\n",
    "\n",
    "feat_extractor = FeatureExtractor([\n",
    "    bag_of_word_extractor,\n",
    "    bag_of_word_plus_tag_extractor,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('05 ', '50 ')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(code):\n",
    "    if not code.endswith(\"b\"):\n",
    "        code += \" \"\n",
    "    return code.rjust(3, \"0\")\n",
    "\n",
    "def extract_lr(cr):\n",
    "    return cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "\n",
    "def normalize_cr(cr):\n",
    "    pair = extract_lr(cr)\n",
    "    return (normalize(pair[0]),normalize(pair[1]))\n",
    "\n",
    "normalize_cr(\"Causer:5->Result:50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearnModel(object):\n",
    "\n",
    "    CAUSAL = set([CAUSER, EXPLICIT, RESULT])\n",
    "    \n",
    "    def __init__(self, feat_extractor, tags, cr_tags, base_learner_fact, beta, positive_val=1):\n",
    "        # init checks\n",
    "        #assert CAUSER in tags, \"%s must be in tags\" % CAUSER\n",
    "        #assert RESULT in tags, \"%s must be in tags\" % RESULT\n",
    "        #assert EXPLICIT in tags, \"%s must be in tags\" % EXPLICIT\n",
    "\n",
    "        self.feat_extractor = feature_extractor    # feature extractor (for use later)\n",
    "        self.positive_val = positive_val\n",
    "        self.base_learner_fact = base_learner_fact # Sklearn classifier\n",
    "        self.tags = set(tags)                      # tags for basic tagging\n",
    "        \n",
    "        self.cr_tags = set(cr_tags)                # causal relation tags\n",
    "            \n",
    "        self.actions = set([SearnModel.Shift, SearnModel.Reduce, SearnModel.Left_ARC, SearnModel.Right_ARC, SearnModel.Skip])\n",
    "        self.epoch = -1\n",
    "        self.initial_beta = beta\n",
    "        self.beta = 1.0 # probability of using oracle for each parsing decision, initialize to 1 as we don't use until second epoch\n",
    "        self.stack = []\n",
    "        self.models = []\n",
    "        self.training_datasets = {}\n",
    "        self.current_model = None\n",
    "        \n",
    "    def train(self, essay_feats, epochs):\n",
    "        #essay_feats = self.feat_extractor.transform(tagged_essays)        \n",
    "        for i in range(0, epochs):\n",
    "            self.epoch +=1\n",
    "            examples_with_loss = dict() # dict of tag\\decision to examples (with labels)\n",
    "            new_model = self.base_learner_fact()\n",
    "            \n",
    "            for essay_ix, essay in enumerate(essay_feats):\n",
    "                for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "                    exs, parse = self.taggged_sentence(taggged_sentence)\n",
    "                    examples.extend(exs)\n",
    "            \n",
    "            #TODO, dictionary vectorize examples, train a weighted binary classifier for each separate parsing action\n",
    "            self.training_datasets.append(examples_with_loss)\n",
    "            self.models.append(new_model)\n",
    "            \n",
    "            # Decay beta\n",
    "            self.beta = self.beta * self.initial_beta\n",
    "    \n",
    "    def parse_sentence(self, taggged_sentence):\n",
    "                \n",
    "        #Pseudo Code\n",
    "        \n",
    "        # Get predicted tags\n",
    "        # collapse word features for predicted tags by tag seq (gather the sequence of words assigned to that tag, they may not be contiguous)\n",
    "        # compute features over these word sequences uni-grams, bi-grams, tri-grams, etc\n",
    "        # include features about the buffer and the stack - tos, tos-1, buffer, buffer+1, all previous tags, all subsequent tags\n",
    "        # compute features between the TOS code's word sequence and the buffer's word sequence \n",
    "            #(dep relations, pairs of words, the tag pair)\n",
    "        \n",
    "        # for while items in buffer:\n",
    "          # read item from buffer\n",
    "          # get oracle decision, if p < beta, add it to prediction_history, use as label for training data\n",
    "          # get parser decision, if p >= beta add it to the prediction_history (for cond history feat extraction)\n",
    "          # create new examples using features and oracle decision\n",
    "          # act on decision chosen by beta threshold\n",
    "       \n",
    "        action_history = []\n",
    "        action_tag_pair_history = []\n",
    "        #x, y, weight\n",
    "        examples = []\n",
    "        \n",
    "        arcs = set()\n",
    "\n",
    "        all_tags = set()\n",
    "        all_predicted_tags = set()\n",
    "        \n",
    "        min_ixs, max_ixs = defaultdict(lambda : len(taggged_sentence)+1 ), defaultdict(lambda : -1)\n",
    "        ptag_seq = []\n",
    "        for i, (wd) in enumerate(taggged_sentence):\n",
    "            all_tags.update(wd.tags)\n",
    "            ptag = wd.predicted_tag\n",
    "            if ptag == EMPTY_TAG:\n",
    "                continue\n",
    "            if not ptag in all_predicted_tags:\n",
    "                ptag_seq.append(ptag)\n",
    "            all_predicted_tags.add(ptag)\n",
    "            # determine span of each predicted tag\n",
    "            min_ixs[ptag] = min(min_ixs[ptag], i)\n",
    "            max_ixs[ptag] = max(max_ixs[ptag], i)\n",
    "            \n",
    "        ground_truth = all_tags.intersection(self.cr_tags)\n",
    "        ground_truth = set([normalize_cr(crel) for crel in ground_truth])\n",
    "        # Filter to only those crels that have support in the predicted tags\n",
    "        supported_crels = set()\n",
    "        for crel in ground_truth:\n",
    "            l,r = crel\n",
    "            if l in all_predicted_tags and r in all_predicted_tags:\n",
    "                supported_crels.add(crel)\n",
    "        ground_truth = supported_crels\n",
    "        \n",
    "        # Initialize stack, basic parser and oracle\n",
    "        stack = Stack(False)        \n",
    "        stack.push(ROOT)\n",
    "        parser = Parser(stack)\n",
    "        oracle = Oracle(ground_truth, parser)\n",
    "        \n",
    "        # Oracle parsing logic\n",
    "        for tag_ix, ptag in enumerate(ptag_seq):\n",
    "            buffer = ptag\n",
    "            word_seq = taggged_sentence[min_ixs[ptag]:max_ixs[ptag]+1]\n",
    "            base_feats = self.feat_extractor.extract(ptag, word_sequence, self.positive_val)\n",
    "            while True:\n",
    "                # make sure to include the current tag\n",
    "                #TODO Get features for TOS element.\n",
    "                #TODO Get interaction features between buffer feats and tos feats\n",
    "                feats = self.get_conditional_feats(word_seq, action_history, action_tag_pair_history, ptag, ptag_seq[:tag_ix], ptag_seq[tag_ix+1:]) \n",
    "                feats.update(base_feats)\n",
    "                \n",
    "                tos = stack.tos()\n",
    "                gold_action = oracle.consult(tos, buffer)\n",
    "                \n",
    "                # Consult Oracle or Model based on coin toss\n",
    "                rand_float = np.random.random_sample() # between [0,1) (half-open interval, includes 0 but not 1)\n",
    "                # If no trained models, always use Oracle\n",
    "                if rand_float >= self.beta and len(self.models) > 0:\n",
    "                    action = self.models[-1].predict(feats)\n",
    "                else:\n",
    "                    action = gold_action\n",
    "                \n",
    "                action_history.append(action)\n",
    "                action_tag_pair_history.append((action, ptag))\n",
    "                \n",
    "                #TODO compute weight for each example\n",
    "                #TODO - if errors are made, are there more optimal decisions?\n",
    "                #TODO - if an error was made on this action, how many crels would be affected?\n",
    "                    # Can we clone the current oracle and parser state, and determine the max errors if the golden action is not taken\n",
    "                    # As some wrong decisions can have higher losses (e.g. a Reduce could break 2 relations where a shift may break one or be-recoverable)\n",
    "                    # Then we are better training multiple binary classifiers so each decision can be weighed independently\n",
    "                weight = 1\n",
    "                examples.append(feats, gold_action, weight)\n",
    "                \n",
    "                if not oracle.execute(action, tos, buffer):\n",
    "                    break\n",
    "                if stack.len() == 0:\n",
    "                    break\n",
    "        \n",
    "        return examples, action_history\n",
    "            \n",
    "    def get_conditional_feats(self, word_seq, action_history, action_tag_pair_history, current_tag, previous_tags, subsequent_tags):\n",
    "        feats = {}\n",
    "        feats[\"current_tag-\" + current_tag]\n",
    "        for i, tag in enumerate(previous_tags[::-1]):\n",
    "            feats[\"prev_tag-{i}:{tag}\".format(i=i, tag=tag)] = self.positive_val\n",
    "            feats[\"prev_tag:{tag}\".format(tag=tag)] = self.positive_val\n",
    "            \n",
    "        if len(previous_tags) > 0:\n",
    "            bigrams = compute_ngrams(previous_tags + [current_tag], 2, 2)\n",
    "            for i, bigram in enumerate(bigrams[::-1]):\n",
    "                feats[\"prev_bigram-tag-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"prev_bigram-tag:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "\n",
    "        for i, tag in enumerate(subsequent_tags):\n",
    "            feats[\"subseq_tag-{i}:{tag}\".format(i=i, tag=tag)] = self.positive_val\n",
    "        if len(subsequent_tags) > 0:\n",
    "            bigrams = compute_ngrams([current_tag] + subsequent_tags, 2, 2)\n",
    "            for i, bigram in enumerate(bigrams):\n",
    "                feats[\"subseq_bigram-tag-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"subseq_bigram-tag:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "\n",
    "        action_tally = defaultdict(int)\n",
    "        for i, action in enumerate(action_history[::-1]):\n",
    "            feats[\"action-{i}:{action}\".format(i=i, action=action)] = self.positive_val\n",
    "            action_tally[action] +=1\n",
    "        \n",
    "        for action, count in action_tally.items():                           \n",
    "            feats[\"action-tally:{action}_{count}\".format(action=action, count=count)] = self.positive_val\n",
    "                                       \n",
    "        if len(action_history) > 0:\n",
    "            bigrams = compute_ngrams(action_history + [current_tag], 2, 2)\n",
    "            for i, bigram in enumerate(bigrams[::-1]):\n",
    "                feats[\"prev_bigram_action-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"prev_bigram_action:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "        \n",
    "        for i, (action,tag) in enumerate(action_tag_pair_history[::-1]):\n",
    "            feats[\"actiontag-{i}:{action}_{tag}\".format(i=i, action=action, tag=tag)] = self.positive_val\n",
    "            feats[\"actiontag:{action}_{tag}\".format(action=action, tag=tag)] = self.positive_val\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note You Can Set Per Observation Weights in XGBoost\n",
    "* http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "  * see weight parameter in dtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO\n",
    "* Need to make sure the tagger tags EXCPLICT tags. These can then be skipped by the parser, but will be included in the features used to train the parser and taggger. Do we want to train a separate tagger that determines if a tagged word is a cause, explict or result. That will then resolve the direction of the relation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
