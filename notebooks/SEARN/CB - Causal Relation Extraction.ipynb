{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(training_pickled, \"rb+\") as f:\n",
    "#    tagged_essays = pickle.load(f)\n",
    "#len(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data - Essays Tagged with Codes By a Bi-Directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "fname = predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays = dill.load(f)\n",
    "\n",
    "#len(tagged_essays), \n",
    "len(pred_tagged_essays) # should be 902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-07-29 13:14:44.078527\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAUSER = \"Causer\"\n",
    "RESULT = \"Result\"\n",
    "EXPLICIT = \"explicit\"\n",
    "CAUSER_EXPLICIT = \"Causer_Explicit\"\n",
    "EXPLICIT_RESULT = \"Explicit_Result\"\n",
    "CAUSER_EXPLICIT_RESULT = \"Causer_Explicit_Result\"\n",
    "CAUSER_RESULT = \"Causer_Result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in pred_tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "#TODO - don't ignore Anaphor, other and rhetoricals here\n",
    "cr_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "#vtags.add(EMPTY_TAG)\n",
    "#cr_tags = vtags\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50', '5', '4', '11', '13', '3', '7', '1', '6', '14', '2', '12', '5b']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:11',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:12->Result:11']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Anaphor',\n",
       " 'COMPILED',\n",
       " 'Causer',\n",
       " 'Causer:1',\n",
       " 'Causer:11',\n",
       " 'Causer:12',\n",
       " 'Causer:13',\n",
       " 'Causer:14',\n",
       " 'Causer:2',\n",
       " 'Causer:3',\n",
       " 'Causer:4',\n",
       " 'Causer:5',\n",
       " 'Causer:50',\n",
       " 'Causer:5b',\n",
       " 'Causer:6',\n",
       " 'Causer:7',\n",
       " 'Causer:Anaphor',\n",
       " 'Causer:other',\n",
       " 'Causer:rhetorical',\n",
       " 'Result',\n",
       " 'Result:1',\n",
       " 'Result:11',\n",
       " 'Result:12',\n",
       " 'Result:13',\n",
       " 'Result:14',\n",
       " 'Result:2',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:5',\n",
       " 'Result:50',\n",
       " 'Result:5b',\n",
       " 'Result:6',\n",
       " 'Result:7',\n",
       " 'Result:Anaphor',\n",
       " 'Result:other',\n",
       " 'Result:rhetorical',\n",
       " 'explicit',\n",
       " 'other',\n",
       " 'rhetorical']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in sorted(tag_freq.keys()) if \"->\" not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Decorators import memoize\n",
    "from nltk import PorterStemmer\n",
    "from NgramGenerator import compute_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays):\n",
    "    essay2pred = {}\n",
    "    for essay in pred_tagged_essays:\n",
    "        essay2pred[essay.name] = essay\n",
    "\n",
    "    for essay in essay_feats:\n",
    "        pred_essay = essay2pred[essay.name]\n",
    "        for six, sent in enumerate(essay.sentences):\n",
    "            pred_sent = pred_essay.pred_tagged_sentences[six]\n",
    "            assert len(pred_sent) == len(sent), \"Miss-match on sentences\"\n",
    "            for wix, wd in enumerate(sent):\n",
    "                wd.predicted_tag = pred_sent[wix]\n",
    "    \n",
    "#assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stack(object):\n",
    "    def __init__(self, verbose=False):    \n",
    "        self.stack = []\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def tos(self):\n",
    "        if self.len() == 0:\n",
    "            return None\n",
    "        #assert self.len() > 0, \"Can't peek when stack is empty\"\n",
    "        return self.stack[-1]\n",
    "    \n",
    "    def pop(self):\n",
    "        assert self.len() > 0, \"Can't pop when stack is empty\"\n",
    "        item = self.stack.pop()\n",
    "        if self.verbose:\n",
    "            print(\"POPPING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "        return item\n",
    "    \n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "        if self.verbose:\n",
    "            print(\"PUSHING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "    def contains(self, item):\n",
    "        return item in self.stack\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"|\".join(self.stack)\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Stack(self.verbose)\n",
    "        clone.stack = list(self.stack)\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT = \"root\"\n",
    "\n",
    "def norm_arc(arc):\n",
    "    return tuple(sorted(arc))\n",
    "\n",
    "def norm_arcs(arcs):\n",
    "    return set(map(norm_arc, arcs))\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, stack):\n",
    "        self.stack = stack\n",
    "        self.arcs = []\n",
    "        self.normed_arcs = set()\n",
    "        # nodes with heads\n",
    "        self.children = set()\n",
    "        self.actions = []\n",
    "        \n",
    "    def get_dependencies(self):\n",
    "        return [(l,r) for (l,r) in self.arcs if r != ROOT and l != ROOT]\n",
    "        \n",
    "    def left_arc(self, buffer):\n",
    "        tos = self.stack.pop()\n",
    "        #Pre-condition\n",
    "        #assert self.has_head(tos) == False\n",
    "        arc = (tos,buffer)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(arc)\n",
    "        self.children.add(tos)\n",
    "        self.actions.append(\"L ARC   : \" + tos + \"->\" + buffer)\n",
    "        \n",
    "    def right_arc(self, buffer):\n",
    "        tos = self.stack.tos()\n",
    "        #normalize arc\n",
    "        arc = (buffer,tos)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(n_arc)\n",
    "        self.actions.append(\"R ARC   : \" + tos + \"<-\" + buffer)\n",
    "        self.children.add(buffer)\n",
    "        self.stack.push(buffer)\n",
    "        \n",
    "    def reduce(self):\n",
    "        tos = self.stack.pop()\n",
    "        #assert self.has_head(tos) == True\n",
    "        self.actions.append(\"REDUCE  : Pop  %s\" % tos)\n",
    "        \n",
    "    def shift(self, buffer):\n",
    "        self.stack.push(buffer)\n",
    "        self.actions.append(\"SHIFT   : Push %s\" % buffer)\n",
    "    \n",
    "    def skip(self, buffer):\n",
    "        self.actions.append(\"SKIP    : item %s\" % buffer)\n",
    "    \n",
    "    def has_head(self, item):\n",
    "        return item in self.children\n",
    "    \n",
    "    def in_stack(self, item):\n",
    "        return self.stack.contains(item)\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Parser(self.stack.clone())\n",
    "        clone.arcs = list(self.arcs)\n",
    "        clone.normed_arcs = set(self.normed_arcs)\n",
    "        # nodes with heads\n",
    "        clone.children = set(self.children)\n",
    "        clone.actions = list(self.actions)\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "SHIFT = \"Shift\"\n",
    "REDUCE = \"Reduce\"\n",
    "LARC = \"LArc\"\n",
    "RARC = \"Rarc\"\n",
    "SKIP = \"Skip\"\n",
    "\n",
    "class Oracle(object):\n",
    "    \n",
    "    def __init__(self, crels, parser):\n",
    "        self.parser = parser\n",
    "        self.raw_crels = crels\n",
    "        self.crels = norm_arcs(crels) # type: Set[Tuple[str,str]]\n",
    "        self.mapping = self.build_mappings(crels)\n",
    "    \n",
    "    def build_mappings(self, pairs):\n",
    "        mapping = defaultdict(set)\n",
    "        for c,res in pairs:\n",
    "            mapping[c].add(res)\n",
    "            mapping[res].add(c)\n",
    "        return mapping\n",
    "\n",
    "    def should_continue(self, action):\n",
    "        # continue parsing if REDUCE or LARC\n",
    "        return action in (REDUCE,LARC)\n",
    "    \n",
    "    def remove_relation(self, a,b):\n",
    "        self.mapping[a].remove(b)\n",
    "        if len(self.mapping[a]) == 0:\n",
    "            del self.mapping[a]\n",
    "        self.mapping[b].remove(a)\n",
    "        if len(self.mapping[b]) == 0:\n",
    "            del self.mapping[b]\n",
    "    \n",
    "    def consult(self, tos, buffer):\n",
    "        \"\"\"\n",
    "        Performs optimal decision for parser\n",
    "        If true, continue processing, else Consume Buffer\n",
    "        \"\"\"\n",
    "        parser = self.parser\n",
    "        a,b = norm_arc((tos, buffer))\n",
    "        if (a,b) in self.crels:\n",
    "            # TOS has arcs remaining? If so, we need RARC, else LARC\n",
    "            if len(self.mapping[tos]) == 1:\n",
    "                return LARC\n",
    "            else:\n",
    "                return RARC\n",
    "        else:\n",
    "            if buffer not in self.mapping:\n",
    "                return SKIP\n",
    "            # If the buffer has relations further down in the stack, we need to POP the TOS\n",
    "            for item in self.mapping[buffer]:\n",
    "                if item == tos:\n",
    "                    continue\n",
    "                if parser.in_stack(item):\n",
    "                    return REDUCE\n",
    "            #end for\n",
    "            #ELSE\n",
    "            return SHIFT\n",
    "        \n",
    "    def execute(self, action, tos, buffer):\n",
    "        \"\"\"\n",
    "        Performs optimal decision for parser\n",
    "        If true, continue processing, else Consume Buffer\n",
    "        \"\"\"\n",
    "        parser = self.parser\n",
    "        if action == LARC:\n",
    "            parser.left_arc(buffer)\n",
    "            self.remove_relation(tos, buffer)\n",
    "        elif action == RARC:\n",
    "            parser.right_arc(buffer)\n",
    "            self.remove_relation(tos, buffer)\n",
    "        elif action == REDUCE:\n",
    "            parser.reduce()\n",
    "        elif action == SHIFT:\n",
    "            parser.shift(buffer)\n",
    "        elif action == SKIP:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Unknown parsing action %s\" % action)\n",
    "        return self.should_continue(action)\n",
    "    \n",
    "    def tos(self):\n",
    "        return self.parser.stack.tos()\n",
    "    \n",
    "    def is_stack_empty(self):\n",
    "        return self.parser.stack.len() == 0\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = Oracle(set(self.raw_crels), self.parser.clone())\n",
    "        clone.mapping = dict(self.mapping.items())\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    def __init__(self, extractors):\n",
    "        self.extractors = extractors\n",
    "    \n",
    "    def extract(self, predicted_tag, word_seq, positive_val=1):\n",
    "        \"\"\"\n",
    "        word: str\n",
    "        word_seq: List[Word]\n",
    "        \n",
    "        returns: List[Dict{str,float}]\n",
    "        \"\"\"\n",
    "        fts = dict()\n",
    "        for ext in self.extractors:\n",
    "            new_feats = ext(predicted_tag, word_seq, positive_val)\n",
    "            fts.update(new_feats)\n",
    "        return fts\n",
    "    \n",
    "def bag_of_word_extractor(predicted_tag, word_seq, positive_val):\n",
    "    feats = {}\n",
    "    for word in word_seq:\n",
    "        feats[\"bow_\" + word] = positive_val\n",
    "    return feats\n",
    "\n",
    "def bag_of_word_plus_tag_extractor(predicted_tag, word_seq, positive_val):\n",
    "    feats = {}\n",
    "    for word in word_seq:\n",
    "        feats[\"bow_\" + word + \"_tag_\" + predicted_tag] = positive_val\n",
    "    return feats\n",
    "\n",
    "feat_extractor = FeatureExtractor([\n",
    "    bag_of_word_extractor,\n",
    "    bag_of_word_plus_tag_extractor,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5', '50')\n",
      "Causer:5->Result:50\n"
     ]
    }
   ],
   "source": [
    "#def normalize(code):\n",
    "#    if not code.endswith(\"b\"):\n",
    "#        code += \" \"\n",
    "#   return code.rjust(3, \"0\")\n",
    "\n",
    "def extract_lr(cr):\n",
    "    return cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "\n",
    "def normalize_cr(cr):\n",
    "    pair = tuple(extract_lr(cr))\n",
    "    return pair\n",
    "\n",
    "def denormalize_cr(crel):\n",
    "    l,r = crel\n",
    "    return \"Causer:{l}->Result:{r}\".format(l=l,r=r)\n",
    "\n",
    "crel = normalize_cr(\"Causer:5->Result:50\")\n",
    "print(crel)\n",
    "cr = denormalize_cr(crel)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightedExamples(object):\n",
    "    def __init__(self, labels, positive_value):\n",
    "        self.xs = []\n",
    "        self.labels = labels\n",
    "        self.positive_value = positive_value\n",
    "        self.labels  = defaultdict(list) # list of ints\n",
    "        self.weights = defaultdict(list) # list of floats\n",
    "    \n",
    "    def add(self, x, actual_lbl, weights=None):\n",
    "        self.xs.append(x)\n",
    "        for lbl in self.labels:\n",
    "            val = self.positive_value if lbl == actual_lbl else -1\n",
    "            self.labels[lbl].append(val)\n",
    "            weight = 1\n",
    "            if weights:\n",
    "                weight = weights[lbl]\n",
    "            self.weights[lbl].append(weight)\n",
    "    \n",
    "    def get_labels_for(self, lbl):\n",
    "        return self.labels[lbl]\n",
    "    \n",
    "    def get_weights_for(self, lbl):\n",
    "        return self.weights[lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "#xgboost.DMatrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "PARSE_ACTIONS = [\n",
    "    SHIFT,\n",
    "    REDUCE,\n",
    "    LARC,\n",
    "    RARC,\n",
    "    SKIP\n",
    "]\n",
    "\n",
    "CAUSE_EFFECT = \"CAUSE_EFFECT\"\n",
    "EFFECT_CAUSE = \"EFFECT_CAUSE\"\n",
    "REJECT       = \"REJECT\" # Not a CREL\n",
    "\n",
    "CREL_ACTIONS = [\n",
    "    CAUSE_EFFECT,\n",
    "    EFFECT_CAUSE,\n",
    "    REJECT\n",
    "]\n",
    "    \n",
    "class SearnModel(object):\n",
    "    def __init__(self, feature_extractor, cr_tags, base_learner_fact, beta_decay_fn = lambda b: b-0.1, positive_val=1):\n",
    "        # init checks\n",
    "        #assert CAUSER in tags, \"%s must be in tags\" % CAUSER\n",
    "        #assert RESULT in tags, \"%s must be in tags\" % RESULT\n",
    "        #assert EXPLICIT in tags, \"%s must be in tags\" % EXPLICIT\n",
    "\n",
    "        self.feat_extractor = feature_extractor    # feature extractor (for use later)\n",
    "        self.positive_val = positive_val\n",
    "        self.base_learner_fact = base_learner_fact # Sklearn classifier\n",
    "        \n",
    "        self.cr_tags = set(cr_tags)                # causal relation tags\n",
    "        self.epoch = -1\n",
    "        self.beta = 1.0 # probability of using oracle for each parsing decision, initialize to 1 as we don't use until second epoch\n",
    "        self.beta_decay_fn = beta_decay_fn\n",
    "        self.stack = []\n",
    "        \n",
    "        self.parser_models = []\n",
    "        self.current_parser_models = None\n",
    "        self.current_parser_dict_vectorizer = None\n",
    "        self.crel_models = []\n",
    "        self.current_crel_model   = None\n",
    "        self.current_crel_dict_vectorizer = None\n",
    "        \n",
    "        self.training_datasets_parsing = {}\n",
    "        self.training_datasets_crel = {}\n",
    "        self.current_model = None\n",
    "        \n",
    "    def train(self, tagged_essays, max_epochs):\n",
    "        \n",
    "        trained_with_beta0 = False\n",
    "        for i in range(0, max_epochs):\n",
    "            if self.beta < 0:\n",
    "                trained_with_beta0 = True\n",
    "                \n",
    "            self.epoch +=1\n",
    "            print(\"Epoch: {epoch}\".format(epoch=self.epoch))\n",
    "            print(\"Beta:  {beta}\".format(beta=self.beta))\n",
    "            \n",
    "            #TODO - provide option for different model types here?\n",
    "            parse_examples = WeightedExamples(PARSE_ACTIONS, self.positive_val)\n",
    "            crel_examples  = WeightedExamples(CREL_ACTIONS,  self.positive_val)\n",
    "            \n",
    "            for essay_ix, essay in enumerate(tagged_essays):\n",
    "                for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "                    predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "                    relations = self.parse_sentence(taggged_sentence, predicted_tags, parse_examples, crel_examples)\n",
    "            \n",
    "            #TODO, dictionary vectorize examples, train a weighted binary classifier for each separate parsing action\n",
    "            self.train_parse_models(parse_examples)\n",
    "            self.train_crel_models(crel_examples)\n",
    "            \n",
    "            self.training_datasets_parsing[self.epoch] = parse_examples\n",
    "            self.training_datasets_crel[self.epoch]    = crel_examples\n",
    "            \n",
    "            # Decay beta\n",
    "            self.beta = self.beta_decay_fn(beta)\n",
    "            if self.beta < 0 and trained_with_beta0:\n",
    "                print(\"beta decayed below 0 - beta:'{beta}', stopping\".format(beta=self.beta))\n",
    "                break\n",
    "        #end [for each epoch]\n",
    "        if not trained_with_beta0:\n",
    "            print(\"Algorithm hit max epochs without training with beta <= 0 - final_beta:{beta}\".format(beta=self.beta))\n",
    "    \n",
    "    def train_parse_models(self, examples):\n",
    "        models = {}\n",
    "        self.current_parser_dict_vectorizer = DictVectorizer(sparse=True)\n",
    "        xs = self.current_parser_dict_vectorizer.fit_transform(examples.xs)\n",
    "        \n",
    "        for action in PARSE_ACTIONS:\n",
    "            mdl = self.base_learner_fact()\n",
    "            ys = examples.get_labels_for(action)\n",
    "            weights = examples.get_weights_for(action)\n",
    "            #TODO - train cost sensitive classifier\n",
    "            mdl.fit(xs,ys)\n",
    "            models[action] = mdl\n",
    "        self.parser_models.append(models)\n",
    "            \n",
    "    def train_crel_models(self, examples):\n",
    "        \n",
    "        self.current_crel_dict_vectorizer = DictVectorizer(sparse=True)\n",
    "        \n",
    "        model = self.base_learner_fact()\n",
    "        xs = self.current_crel_dict_vectorizer.fit_transform(examples.xs)\n",
    "        ys = examples.get_labels_for(action)\n",
    "        model.fit(xs,ys)\n",
    "        \n",
    "        self.crel_models.append(model)\n",
    "    \n",
    "    def add_relation(self, action, tos, buffer, ground_truth, relations):\n",
    "        if action in [LARC, RARC]:\n",
    "            if (tos, buffer) in ground_truth:\n",
    "                relations.add((tos,buffer))\n",
    "            elif (buffer,tos) in ground_truth:\n",
    "                relations.add((buffer,tos))\n",
    "    \n",
    "    def relations_for_action(self, forced_action, ground_truth, remaining_buffer, oracle):\n",
    "        oracle = oracle.clone()\n",
    "        relns = set()\n",
    "        first_action = True\n",
    "        for buffer in remaining_buffer:\n",
    "            while True:\n",
    "                tos = oracle.tos()\n",
    "                if not first_action: # need to force first action\n",
    "                    action = oracle.consult(tos, buffer)\n",
    "                else:\n",
    "                    action = forced_action\n",
    "                    first_action = False\n",
    "                self.add_relation(action, tos, buffer, ground_truth, relns)                        \n",
    "                if not oracle.execute(action, tos, buffer):\n",
    "                    break\n",
    "                if oracle.is_stack_empty():\n",
    "                    break\n",
    "        return relns\n",
    "        \n",
    "    def compute_cost(self, ground_truth, remaining_buffer, oracle):\n",
    "        \n",
    "        tos = oracle.tos()\n",
    "        gold_action = oracle.consult(tos, remaining_buffer[0])\n",
    "        gold_parse = self.relations_for_action(gold_action, ground_truth, remaining_buffer, oracle)\n",
    "        \n",
    "        action_costs = {}\n",
    "        for action in PARSE_ACTIONS:\n",
    "            if action == gold_action:\n",
    "                continue\n",
    "            parse = self.relations_for_action(action, ground_truth, remaining_buffer, oracle)\n",
    "            num_matches = len(gold_parse.intersection(parse))\n",
    "            # recall\n",
    "            false_negatives = len(gold_parse) - num_matches\n",
    "            # precision\n",
    "            false_positives = len(parse) - num_matches      \n",
    "            # Cost is the total of the false positives + false negatives\n",
    "            cost = false_positives + false_negatives\n",
    "            action_costs[action] = cost\n",
    "        \n",
    "        # Cost of the gold action is the mean of all of the wrong choices\n",
    "        action_costs[gold_action] = np.mean(list(action_costs.values()))\n",
    "        return action_costs\n",
    "        \n",
    "    def predict_parse_action(self, feats):\n",
    "        xs = self.current_parser_dict_vectorizer.transform(feats)\n",
    "        prob_by_label = {}\n",
    "        for action in PARSE_ACTIONS:\n",
    "            prob_by_label[action] = self.current_parser_models[action].predict_proba(xs)[0][-1]\n",
    "        \n",
    "        max_act, max_prob = max(prob_by_label.items(), key = lambda tpl: tpl[1])\n",
    "        return max_act\n",
    "    \n",
    "    def predict_crel_action(self, feats):\n",
    "        xs = self.current_crel_dict_vectorizer.transform(feats)\n",
    "        return self.current_crel_model.predict(xs)\n",
    "    \n",
    "    def parse_sentence(self, taggged_sentence, predicted_tags, parse_examples, crel_examples):\n",
    "   \n",
    "        action_history = []\n",
    "        action_tag_pair_history = []\n",
    "\n",
    "        all_tags = set()\n",
    "        all_predicted_tags = set()\n",
    "        \n",
    "        min_ixs, max_ixs = defaultdict(lambda : len(taggged_sentence)+1 ), defaultdict(lambda : -1)\n",
    "        ptag_seq = []\n",
    "        words = []\n",
    "        for i, (wd, tags) in enumerate(taggged_sentence):\n",
    "            words.append(wd)\n",
    "            all_tags.update(tags)\n",
    "            ptag = predicted_tags[i]\n",
    "            if ptag == EMPTY_TAG:\n",
    "                continue\n",
    "            if not ptag in all_predicted_tags:\n",
    "                ptag_seq.append(ptag)\n",
    "            all_predicted_tags.add(ptag)\n",
    "            # determine span of each predicted tag\n",
    "            min_ixs[ptag] = min(min_ixs[ptag], i)\n",
    "            max_ixs[ptag] = max(max_ixs[ptag], i)\n",
    "            \n",
    "        ground_truth = all_tags.intersection(self.cr_tags)\n",
    "        ground_truth = set([normalize_cr(crel) for crel in ground_truth])\n",
    "        # Filter to only those crels that have support in the predicted tags\n",
    "        supported_crels = set()\n",
    "        for crel in ground_truth:\n",
    "            l,r = crel\n",
    "            if l in all_predicted_tags and r in all_predicted_tags:\n",
    "                supported_crels.add(crel)\n",
    "        ground_truth = supported_crels\n",
    "        \n",
    "        # Initialize stack, basic parser and oracle\n",
    "        stack = Stack(False)        \n",
    "        stack.push(ROOT)\n",
    "        parser = Parser(stack)\n",
    "        oracle = Oracle(ground_truth, parser)\n",
    "        \n",
    "        predicted_relations = []\n",
    "        \n",
    "        # Oracle parsing logic\n",
    "        for tag_ix, buffer in enumerate(ptag_seq):\n",
    "            buffer = buffer\n",
    "            word_seq = words[min_ixs[buffer]:max_ixs[buffer]+1]\n",
    "            buffer_feats = self.feat_extractor.extract(buffer, word_seq, self.positive_val)\n",
    "            while True:\n",
    "                tos = oracle.tos()\n",
    "                tos_word_seq = words[min_ixs[tos]:max_ixs[tos]+1]\n",
    "                tos_feats = self.feat_extractor.extract(tos, tos_word_seq, self.positive_val)\n",
    "                \n",
    "                feats = self.get_conditional_feats(action_history, action_tag_pair_history, tos, buffer, ptag_seq[:tag_ix], ptag_seq[tag_ix+1:])\n",
    "                interaction_feats = self.get_interaction_feats(tos_feats, buffer_feats)\n",
    "                feats.update(buffer_feats)\n",
    "                feats.update(tos_feats)\n",
    "                feats.update(interaction_feats)\n",
    "                \n",
    "                gold_action = oracle.consult(tos, buffer)\n",
    "                \n",
    "                # Consult Oracle or Model based on coin toss\n",
    "                rand_float = np.random.random_sample() # between [0,1) (half-open interval, includes 0 but not 1)\n",
    "                # If no trained models, always use Oracle\n",
    "                if rand_float >= self.beta and len(self.self.parser_models) > 0:\n",
    "                    action = self.predict_parse_action(feats)\n",
    "                else:\n",
    "                    action = gold_action\n",
    "                \n",
    "                action_history.append(action)\n",
    "                action_tag_pair_history.append((action, tos, buffer))\n",
    "                \n",
    "                #TODO compute weight for each example\n",
    "                #TODO - if errors are made, are there more optimal decisions?\n",
    "                #TODO - if an error was made on this action, how many crels would be affected?\n",
    "                    # Can we clone the current oracle and parser state, and determine the max errors if the golden action is not taken\n",
    "                    # As some wrong decisions can have higher losses (e.g. a Reduce could break 2 relations where a shift may break one or be-recoverable)\n",
    "                    # Then we are better training multiple binary classifiers so each decision can be weighed independently\n",
    "                \n",
    "                cost_per_action = self.compute_cost(ground_truth, ptag_seq[tag_ix:], oracle)\n",
    "                # make a copy as changing later\n",
    "                parse_examples.add(dict(feats), gold_action, cost_per_action)\n",
    "\n",
    "                # Decide the direction of the causal relation\n",
    "                if action in [LARC, RARC]:\n",
    "                    if (tos, buffer) in ground_truth:\n",
    "                        gold_lr_action = CAUSE_EFFECT\n",
    "                    elif (buffer,tos) in ground_truth:\n",
    "                        gold_lr_action = EFFECT_CAUSE\n",
    "                    else:\n",
    "                        gold_lr_action = REJECT\n",
    "                    \n",
    "                    # Add arc to features\n",
    "                    feats[\"ARC:\" + action] = self.positive_val                    \n",
    "                    rand_float = np.random.random_sample()\n",
    "                    if rand_float >= self.beta and len(self.crel_models) > 0:\n",
    "                        #TODO - we need separate models for different decisions\n",
    "                        lr_action = self.predict_crel_action(feats)\n",
    "                    else:\n",
    "                        lr_action = gold_lr_action\n",
    "                    \n",
    "                    if lr_action == CAUSE_EFFECT:\n",
    "                        predicted_relations.append((tos,buffer))\n",
    "                    elif lr_action == EFFECT_CAUSE:\n",
    "                        predicted_relations.append((buffer,tos))\n",
    "                                            \n",
    "                    # cost is always 1 for this action (cost of 1 for getting it wrong)\n",
    "                    #  because getting the wrong direction won't screw up the parse as it doesn't modify the stack\n",
    "\n",
    "                    crel_examples.append(dict(feats), gold_lr_action)                    \n",
    "                    # Not sure we want to condition on the actions of this crel model\n",
    "                    #action_history.append(lr_action)\n",
    "                    #action_tag_pair_history.append((lr_action, tos, buffer))\n",
    "                    \n",
    "                #end if action in [LARC,RARC]\n",
    "                if not oracle.execute(action, tos, buffer):\n",
    "                    break\n",
    "                if oracle.is_stack_empty():\n",
    "                    break\n",
    "                    \n",
    "        pred_relns = [denormalize_cr(cr) for cr in predicted_relations]\n",
    "        return pred_relns\n",
    "    \n",
    "    def get_interaction_feats(self, fts1, fts2):\n",
    "        interactions = {}\n",
    "        for fta, vala in fts1.items():\n",
    "            for ftb, valb in fts2.items():\n",
    "                if vala > 0 and valb > 0:\n",
    "                    interactions[\"inter: \" + fta + \"|\" + ftb] = self.positive_val\n",
    "        return interactions\n",
    "            \n",
    "    def get_conditional_feats(self, action_history, action_tag_pair_history, tos, buffer, previous_tags, subsequent_tags):\n",
    "        feats = {}\n",
    "        feats[\"tos:\"        + tos]    = self.positive_val\n",
    "        feats[\"buffer:\"     + buffer] = self.positive_val\n",
    "        feats[\"tos_buffer:\" + tos + \"|\" + buffer] = self.positive_val\n",
    "        \n",
    "        ### PREVIOUS TAGS\n",
    "        for i, tag in enumerate(previous_tags[::-1]):\n",
    "            feats[\"prev_tag-{i}:{tag}\".format(i=i, tag=tag)] = self.positive_val\n",
    "            feats[\"prev_tag:{tag}\".format(tag=tag)] = self.positive_val\n",
    "            \n",
    "        if len(previous_tags) > 0:\n",
    "            feats[\"prev-tag-tos-buffer:{tag}_{tos}_{buffer}\".format(tag=previous_tags[-1], tos=tos, buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev-tag-buffer:{tag}_{buffer}\".format(tag=previous_tags[-1], buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev-tag-tos:{tag}_{tos}\".format(tag=previous_tags[-1], tos=tos)] = self.positive_val\n",
    "            bigrams = compute_ngrams(previous_tags, 2, 2)\n",
    "            for i, bigram in enumerate(bigrams[::-1]):\n",
    "                feats[\"prev_bigram-tag-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"prev_bigram-tag:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "\n",
    "        ### REMAINING TAGS\n",
    "        for i, tag in enumerate(subsequent_tags):\n",
    "            feats[\"subseq_tag-{i}:{tag}\".format(i=i, tag=tag)] = self.positive_val\n",
    "            feats[\"subseq_tag:{tag}\".format(i=i, tag=tag)] = self.positive_val\n",
    "\n",
    "        if len(subsequent_tags) > 0:\n",
    "            feats[\"subseq-tag-tos-buffer:{tag}_{buffer}\".format(tag=subsequent_tags[0], tos=tos, buffer=buffer)] = self.positive_val\n",
    "            feats[\"subseq-tag-buffer:{tag}_{buffer}\".format(tag=subsequent_tags[0], buffer=buffer)] = self.positive_val\n",
    "            feats[\"subseq-tag-tos:{tag}_{tos}\".format(tag=subsequent_tags[0], tos=tos)] = self.positive_val            \n",
    "            bigrams = compute_ngrams(subsequent_tags, 2, 2)\n",
    "            for i, bigram in enumerate(bigrams):\n",
    "                feats[\"subseq_bigram-tag-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"subseq_bigram-tag:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "\n",
    "        # features for each previous action\n",
    "        action_tally = defaultdict(int)\n",
    "        for i, action in enumerate(action_history[::-1]):\n",
    "            feats[\"action-{i}:{action}\".format(i=i, action=action)] = self.positive_val\n",
    "            feats[\"action:{action}\".format(action=action)] = self.positive_val\n",
    "            action_tally[action] +=1        \n",
    "        \n",
    "        # Features for the number of times each action has been performed\n",
    "        for action, count in action_tally.items():                           \n",
    "            feats[\"action-tally:{action}_{count}\".format(action=action, count=count)] = self.positive_val\n",
    "                                       \n",
    "        if len(action_history) > 0:\n",
    "            feats[\"prev_action-tos-buffer:{action}_{tos}_{buffer}\".format(action=action_history[-1], tos=tos, buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev_action-buffer:{action}_{buffer}\".format(action=action_history[-1], buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev_action-tos:{action}_{tos}\".format(action=action_history[-1], tos=tos)] = self.positive_val\n",
    "            bigrams = compute_ngrams(action_history, 2, 2)\n",
    "            for i, bigram in enumerate(bigrams[::-1]):\n",
    "                feats[\"prev_bigram_action-{i}:{tag}\".format(i=i, tag=str(bigram))] = self.positive_val\n",
    "                feats[\"prev_bigram_action:{tag}\".format(tag=str(bigram))] = self.positive_val\n",
    "        \n",
    "        for i, (action, prev_tos, prev_buffer) in enumerate(action_tag_pair_history[::-1]):\n",
    "            feats[\"actiontag-{i}:{action}_{tos}_{buffer}\".format(i=i, action=action, tos= prev_tos, buffer=prev_buffer)] = self.positive_val\n",
    "            feats[\"actiontag:{action}_{tos}_{buffer}\".format(action=action, tos= prev_tos, buffer=prev_buffer)] = self.positive_val\n",
    "\n",
    "            feats[\"actiontos-{i}:{action}_{tos}\".format(i=i, action=action, tos=prev_tos)] = self.positive_val\n",
    "            feats[\"actiontos:{action}_{tos}\".format(action=action, tos=prev_tos)] = self.positive_val\n",
    "            \n",
    "            feats[\"actionbuffer-{i}:{action}_{buffer}\".format(i=i, action=action, buffer=prev_buffer)] = self.positive_val\n",
    "            feats[\"actionbuffer:{action}_{buffer}\".format(action=action, buffer=prev_buffer)] = self.positive_val\n",
    "        \n",
    "        if len(action_tag_pair_history) > 0:\n",
    "            action, prev_tos, prev_buffer = action_tag_pair_history[-1]            \n",
    "            feats[\"prev_actiontag_tos_buffer_currnet_tos_current_buffer:{action}_{prev_tos}_{prev_buffer}_{tos}_{buffer}\".format(action=action, prev_tos=prev_tos, prev_buffer=prev_buffer, tos=tos, buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev_actiontag_tos_buffer_current_buffer:{action}_{prev_tos}_{prev_buffer}_{buffer}\".format(action=action, prev_tos=prev_tos, prev_buffer=prev_buffer, buffer=buffer)] = self.positive_val\n",
    "            feats[\"prev_actiontag_tos_buffer_current_tos:{action}_{prev_tos}_{prev_buffer}_{tos}\".format(action=action, prev_tos=prev_tos, prev_buffer=prev_buffer, tos=tos)] = self.positive_val\n",
    "            \n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Beta:  1.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-eb4a8204cf44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mparse_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSearnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcr_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_learner_fact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mparse_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tagged_essays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-9f9e9c0913ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tagged_essays, max_epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msent_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaggged_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mpredicted_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0messay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_tagged_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggged_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrel_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#TODO, dictionary vectorize examples, train a weighted binary classifier for each separate parsing action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-9f9e9c0913ce>\u001b[0m in \u001b[0;36mparse_sentence\u001b[0;34m(self, taggged_sentence, predicted_tags, parse_examples, crel_examples)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;31m# Then we are better training multiple binary classifiers so each decision can be weighed independently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0mcost_per_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptag_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# make a copy as changing later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mparse_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_per_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-9f9e9c0913ce>\u001b[0m in \u001b[0;36mcompute_cost\u001b[0;34m(self, ground_truth, remaining_buffer, oracle)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgold_action\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelations_for_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mnum_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-9f9e9c0913ce>\u001b[0m in \u001b[0;36mrelations_for_action\u001b[0;34m(self, forced_action, ground_truth, remaining_buffer, oracle)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mfirst_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stack_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-08e494a64a0b>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, action, tos, buffer)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLARC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_arc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRARC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_arc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-08e494a64a0b>\u001b[0m in \u001b[0;36mremove_relation\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'root'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#class SearnModel(object):\n",
    "#    def __init__(self, feat_extractor, cr_tags, base_learner_fact, beta, beta_decay_fn = lambda b: b-0.1, positive_val=1):\n",
    "    \n",
    "parse_model = SearnModel(feat_extractor, cr_tags, base_learner_fact=LogisticRegression)\n",
    "parse_model.train(pred_tagged_essays, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Sensitive Learning\n",
    "* Cost Proportionate Example weighting\n",
    "  * https://www.researchgate.net/publication/4047552_Cost-Sensitive_Learning_by_Cost-Proportionate_Example_Weighting\n",
    "* You Can Set Per Observation Weights in XGBoost\n",
    "  * http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "  * see weight parameter in dtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO\n",
    "* Need to make sure the tagger tags EXCPLICT tags. These can then be skipped by the parser, but will be included in the features used to train the parser and taggger. Do we want to train a separate tagger that determines if a tagged word is a cause, explict or result. That will then resolve the direction of the relation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
