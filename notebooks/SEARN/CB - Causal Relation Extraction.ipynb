{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/training.pl'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-07-15 15:31:15.709932\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CAUSER = \"Causer\"\n",
    "RESULT = \"Result\"\n",
    "EXPLICIT = \"explicit\"\n",
    "CAUSER_EXPLICIT = \"Causer_Explicit\"\n",
    "EXPLICIT_RESULT = \"Explicit_Result\"\n",
    "CAUSER_EXPLICIT_RESULT = \"Causer_Explicit_Result\"\n",
    "CAUSER_RESULT = \"Causer_Result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "#TODO - don't ignore Anaphor, other and rhetoricals here\n",
    "cr_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "#vtags.add(EMPTY_TAG)\n",
    "#cr_tags = vtags\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13', '5', '11', '5b', '7', '2', '1', '4', '12', '50', '6', '3', '14']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:3->Result:4',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:1->Result:11',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:12->Result:11']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Anaphor',\n",
       " 'COMPILED',\n",
       " 'Causer',\n",
       " 'Causer:1',\n",
       " 'Causer:11',\n",
       " 'Causer:12',\n",
       " 'Causer:13',\n",
       " 'Causer:14',\n",
       " 'Causer:2',\n",
       " 'Causer:3',\n",
       " 'Causer:4',\n",
       " 'Causer:5',\n",
       " 'Causer:50',\n",
       " 'Causer:5b',\n",
       " 'Causer:6',\n",
       " 'Causer:7',\n",
       " 'Causer:Anaphor',\n",
       " 'Causer:other',\n",
       " 'Causer:rhetorical',\n",
       " 'Result',\n",
       " 'Result:1',\n",
       " 'Result:11',\n",
       " 'Result:12',\n",
       " 'Result:13',\n",
       " 'Result:14',\n",
       " 'Result:2',\n",
       " 'Result:3',\n",
       " 'Result:4',\n",
       " 'Result:5',\n",
       " 'Result:50',\n",
       " 'Result:5b',\n",
       " 'Result:6',\n",
       " 'Result:7',\n",
       " 'Result:Anaphor',\n",
       " 'Result:other',\n",
       " 'Result:rhetorical',\n",
       " 'explicit',\n",
       " 'other',\n",
       " 'rhetorical']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in sorted(tag_freq.keys()) if \"->\" not in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">How to Solve for Sentences with Same Cause, Multiple Effects?</span>\n",
    "* Dependency tree requires parent child relationship\n",
    "* One cause, multiple effects means 1 child with multiple parents\n",
    "* **Solution** - maybe link effects to effects in this scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "config[\"window_size\"] = WINDOW_SIZE\n",
    "offset = int((config[\"window_size\"] - 1) / 2)\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def attach_function_identifier(fn, d):\n",
    "    try:\n",
    "        s = fn.func_name + \"[\"\n",
    "    except:\n",
    "        s = fn.__name__ + \"[\"\n",
    "        fn.func_name = fn.__name__\n",
    "\n",
    "    for k, v in sorted(d.items(), key = lambda tpl: tpl[0]):\n",
    "        if k == fn.func_name:\n",
    "            continue\n",
    "        if type(v) == dict:\n",
    "            continue\n",
    "        s +=   \"%s:%s \" % (str(k), str(v))\n",
    "    fn.func_name = s.strip() + \"]\"\n",
    "    return fn\n",
    "\n",
    "def fact_extract_positional_word_features_stemmed(offset):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    lcls = locals()\n",
    "    # curry offset\n",
    "    def fn_pos_wd_feats_stemmed(input, val=1):\n",
    "        return extract_positional_word_features_stemmed(offset, input, val)\n",
    "    return attach_function_identifier(fn_pos_wd_feats_stemmed, lcls) # recently renamed for mongodob logging\n",
    "\n",
    "def extract_positional_word_features_stemmed(offset, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    start = input.wordix - offset\n",
    "    stop  = input.wordix + offset\n",
    "\n",
    "    end = len(input.sentence) - 1\n",
    "    for i in range(start, stop+1):\n",
    "        relative_offset = str(i - input.wordix)\n",
    "        if i < 0:\n",
    "            feats[\"WD\" +__START__ + \":\" + relative_offset] = val\n",
    "        elif i > end:\n",
    "            feats[\"WD\" +__END__ + \":\" + relative_offset] = val\n",
    "        else:\n",
    "            offset_word = stem(input.sentence[i])\n",
    "            feats[\"WD:\" + relative_offset + \"->\" + offset_word] = val\n",
    "    return feats\n",
    "\n",
    "def fact_extract_ngram_features_stemmed(offset, ngram_size):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    lcls = locals()\n",
    "    # curry offset and ngram size\n",
    "    def fn_pos_ngram_feat_stemmed(input, val=1):\n",
    "        return extract_ngram_features_stemmed(offset, ngram_size, input, val)\n",
    "    return attach_function_identifier(fn_pos_ngram_feat_stemmed, lcls)\n",
    "\n",
    "def extract_ngram_features_stemmed(offset, ngram_size, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    end = len(input.sentence) - 1\n",
    "\n",
    "    # fix to within bounds only\n",
    "    start = max(0, input.wordix - offset)\n",
    "    stop  = min(end, input.wordix + offset)\n",
    "\n",
    "    window = list(input.sentence[start:stop+1])\n",
    "    window = list(map(stem, window))\n",
    "    if input.wordix < offset:\n",
    "        diff = offset - input.wordix\n",
    "        for i in range(diff):\n",
    "            window.insert(0,__START__)\n",
    "    if input.wordix + offset > end:\n",
    "        diff = input.wordix + offset - end\n",
    "        for i in range(diff):\n",
    "            window.append(__END__)\n",
    "\n",
    "    ngrams = compute_ngrams(window, ngram_size, ngram_size)\n",
    "    str_num_ngrams = str(ngram_size)\n",
    "\n",
    "    for i, offset_ngram in enumerate(ngrams):\n",
    "        relative_offset = str(i - offset)\n",
    "        str_ngram = \",\".join(offset_ngram)\n",
    "        feats[\"POS_\" + str_num_ngrams + \"GRAMS:\" + relative_offset + \"->\" + str_ngram] = val\n",
    "\n",
    "    return feats\n",
    "\n",
    "def fact_extract_bow_ngram_features(offset, ngram_size):\n",
    "    \"\"\" offset      :   int\n",
    "                            the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        returns     :   fn\n",
    "                            feature extractor function: FeatureExtactorInput -> dict\n",
    "    \"\"\"\n",
    "    # curry offset and ngram size\n",
    "    lcls = locals()\n",
    "    def fn_bow_ngram_feat(input, val=1):\n",
    "        return extract_bow_ngram_features(offset, ngram_size, input, val)\n",
    "    return attach_function_identifier(fn_bow_ngram_feat, lcls)\n",
    "\n",
    "def extract_bow_ngram_features(offset, ngram_size, input, val = 1):\n",
    "    \"\"\" offset      :   int\n",
    "                           the number of words either side of the input to extract features from\n",
    "        ngram_size  :   int\n",
    "                            the size of the ngrams\n",
    "        input      :    FeatureExtactorInput\n",
    "                            input to feature extractor\n",
    "        returns     :   dict\n",
    "                            dictionary of features\n",
    "    \"\"\"\n",
    "\n",
    "    feats = {}\n",
    "    end = len(input.sentence) - 1\n",
    "\n",
    "    # fix to within bounds only\n",
    "    start = max(0, input.wordix - offset)\n",
    "    stop  = min(end, input.wordix + offset)\n",
    "\n",
    "    window = list(input.sentence[start:stop+1])\n",
    "    if input.wordix < offset:\n",
    "        diff = offset - input.wordix\n",
    "        for i in range(diff):\n",
    "            window.insert(0,__START__)\n",
    "    if input.wordix + offset > end:\n",
    "        diff = input.wordix + offset - end\n",
    "        for i in range(diff):\n",
    "            window.append(__END__)\n",
    "\n",
    "    ngrams = compute_ngrams(window, ngram_size, ngram_size)\n",
    "    str_num_ngrams = str(ngram_size)\n",
    "\n",
    "    for i, offset_ngram in enumerate(ngrams):\n",
    "        str_ngram = \",\".join(offset_ngram)\n",
    "        feats[\"POS_\" + str_num_ngrams + \"GRAMS:BOW\" + \"->\" + str_ngram] = val\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__START__ = \"<START>\"\n",
    "__END__   = \"<END>\"\n",
    "\n",
    "#from featureextractionfunctions \\\n",
    "    #import fact_extract_positional_word_features_stemmed, fact_extract_ngram_features_stemmed,\\\n",
    "    #fact_extract_bow_ngram_features, extract_brown_cluster    \n",
    "    #extract_dependency_relation\n",
    "\n",
    "unigram_window_stemmed  = fact_extract_positional_word_features_stemmed(offset)\n",
    "biigram_window_stemmed  = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "triigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 3)\n",
    "unigram_bow_window      = fact_extract_bow_ngram_features(offset, 1)\n",
    "\n",
    "#TODO - add in full feature set, but keep simple for now\n",
    "#optimal CB feature set\n",
    "extractors = [\n",
    "    unigram_window_stemmed,\n",
    "    biigram_window_stemmed,\n",
    "    #triigram_window_stemmed,\n",
    "    #unigram_bow_window,\n",
    "    #extract_dependency_relation,\n",
    "    #extract_brown_cluster\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Decorators import memoize\n",
    "from nltk import PorterStemmer\n",
    "from NgramGenerator import compute_ngrams\n",
    "from featureextractortransformer import FeatureExtractorTransformer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "@memoize\n",
    "def stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "feature_extractor = FeatureExtractorTransformer(extractors)\n",
    "essay_feats = feature_extractor.transform(tagged_essays)\n",
    "len(tagged_essays), len(essay_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data - Essays Tagged with Codes By a Bi-Directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "fname = predictions_folder + \"essays_train_bi_directional-False_hidden_size-32_merge_mode-sum_num_rnns-1_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays = dill.load(f)\n",
    "\n",
    "len(tagged_essays), len(pred_tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays):\n",
    "    essay2pred = {}\n",
    "    for essay in pred_tagged_essays:\n",
    "        essay2pred[essay.name] = essay\n",
    "\n",
    "    for essay in essay_feats:\n",
    "        pred_essay = essay2pred[essay.name]\n",
    "        for six, sent in enumerate(essay.sentences):\n",
    "            pred_sent = pred_essay.pred_tagged_sentences[six]\n",
    "            assert len(pred_sent) == len(sent), \"Miss-match on sentences\"\n",
    "            for wix, wd in enumerate(sent):\n",
    "                wd.predicted_tag = pred_sent[wix]\n",
    "    \n",
    "assign_predicted_tags_to_essay_feats(essay_feats, pred_tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Stack(object):\n",
    "    def __init__(self, verbose=False):    \n",
    "        self.stack = []\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def tos(self):\n",
    "        if self.len() == 0:\n",
    "            return None\n",
    "        #assert self.len() > 0, \"Can't peek when stack is empty\"\n",
    "        return self.stack[-1]\n",
    "    \n",
    "    def pop(self):\n",
    "        assert self.len() > 0, \"Can't pop when stack is empty\"\n",
    "        item = self.stack.pop()\n",
    "        if self.verbose:\n",
    "            print(\"POPPING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "        return item\n",
    "    \n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "        if self.verbose:\n",
    "            print(\"PUSHING: %s\" % item)\n",
    "            print(\"LEN:     %i\" % len(self.stack))\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "    def contains(self, item):\n",
    "        return item in self.stack\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"|\".join(self.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ROOT = \"root\"\n",
    "\n",
    "def norm_arc(arc):\n",
    "    return tuple(sorted(arc))\n",
    "\n",
    "def norm_arcs(arcs):\n",
    "    return set(map(norm_arc, arcs))\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, stack):\n",
    "        self.stack = stack\n",
    "        self.arcs = []\n",
    "        self.normed_arcs = set()\n",
    "        # nodes with heads\n",
    "        self.children = set()\n",
    "        self.actions = []\n",
    "        \n",
    "    def get_dependencies(self):\n",
    "        return [(l,r) for (l,r) in self.arcs if r != ROOT and l != ROOT]\n",
    "        \n",
    "    def left_arc(self, buffer):\n",
    "        tos = self.stack.pop()\n",
    "        #Pre-condition\n",
    "        #assert self.has_head(tos) == False\n",
    "        arc = (tos,buffer)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(arc)\n",
    "        self.children.add(tos)\n",
    "        self.actions.append(\"L ARC   : \" + tos + \"->\" + buffer)\n",
    "        \n",
    "    def right_arc(self, buffer):\n",
    "        tos = self.stack.tos()\n",
    "        #normalize arc\n",
    "        arc = (buffer,tos)\n",
    "        n_arc = norm_arc(arc)\n",
    "        assert n_arc not in self.normed_arcs, \"Arc already processed %s\" % (n_arc)\n",
    "        self.arcs.append(arc)\n",
    "        self.normed_arcs.add(n_arc)\n",
    "        self.actions.append(\"R ARC   : \" + tos + \"<-\" + buffer)\n",
    "        self.children.add(buffer)\n",
    "        self.stack.push(buffer)\n",
    "        \n",
    "    def reduce(self):\n",
    "        tos = self.stack.pop()\n",
    "        #assert self.has_head(tos) == True\n",
    "        self.actions.append(\"REDUCE  : Pop  %s\" % tos)\n",
    "        \n",
    "    def shift(self, buffer):\n",
    "        self.stack.push(buffer)\n",
    "        self.actions.append(\"SHIFT   : Push %s\" % buffer)\n",
    "    \n",
    "    def skip(self, buffer):\n",
    "        self.actions.append(\"SKIP    : item %s\" % buffer)\n",
    "    \n",
    "    def has_head(self, item):\n",
    "        return item in self.children\n",
    "    \n",
    "    def in_stack(self, item):\n",
    "        return self.stack.contains(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "SHIFT = \"Shift\"\n",
    "REDUCE = \"Reduce\"\n",
    "LARC = \"LArc\"\n",
    "RARC = \"Rarc\"\n",
    "SKIP = \"Skip\"\n",
    "\n",
    "class Oracle(object):\n",
    "    \n",
    "    def __init__(self, crels, parser):\n",
    "        self.parser = parser\n",
    "        self.crels = norm_arcs(crels)\n",
    "        self.mapping = self.build_mappings(crels)\n",
    "    \n",
    "    def build_mappings(self, pairs):\n",
    "        mapping = defaultdict(set)\n",
    "        for c,res in pairs:\n",
    "            mapping[c].add(res)\n",
    "            mapping[res].add(c)\n",
    "        return mapping\n",
    "\n",
    "    def cont(self, action):\n",
    "        # continue parsing if REDUCE or LARC\n",
    "        return action in (REDUCE,LARC)\n",
    "    \n",
    "    def remove_relation(self, a,b):\n",
    "        self.mapping[a].remove(b)\n",
    "        if len(self.mapping[a]) == 0:\n",
    "            del self.mapping[a]\n",
    "        self.mapping[b].remove(a)\n",
    "        if len(self.mapping[b]) == 0:\n",
    "            del self.mapping[b]\n",
    "    \n",
    "    def consult(self, tos, buffer):\n",
    "        \"\"\"\n",
    "        Performs optimal decision for parser\n",
    "        If true, continue processing, else Consume Buffer\n",
    "        \"\"\"\n",
    "        parser = self.parser\n",
    "        a,b = norm_arc((tos, buffer))\n",
    "        if (a,b) in self.crels:\n",
    "            # TOS has arcs remaining? If so, we need RARC, else LARC\n",
    "            if len(self.mapping[tos]) == 1:\n",
    "                parser.left_arc(buffer)\n",
    "                self.remove_relation(tos, buffer)\n",
    "                return self.cont(LARC)\n",
    "            else:\n",
    "                parser.right_arc(buffer)\n",
    "                self.remove_relation(tos, buffer)\n",
    "                return self.cont(RARC)\n",
    "        else:\n",
    "            if buffer not in self.mapping:\n",
    "                parser.skip(buffer)\n",
    "                return self.cont(SKIP)\n",
    "            # If the buffer has relations further down in the stack, we need to POP the TOS\n",
    "            for item in self.mapping[buffer]:\n",
    "                if item == tos:\n",
    "                    continue\n",
    "                if parser.in_stack(item):\n",
    "                    parser.reduce()\n",
    "                    return self.cont(REDUCE)\n",
    "            #end for\n",
    "            #ELSE\n",
    "            parser.shift(buffer)\n",
    "            return self.cont(SHIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SearnModel(object):\n",
    "    Shift = \"Shift\"\n",
    "    Reduce = \"Reduce\"\n",
    "    Left_ARC = \"LeftArc\"\n",
    "    Right_ARC = \"RightArc\"\n",
    "    Root = \"Root\"\n",
    "\n",
    "    CAUSAL = set([CAUSER, EXPLICIT, RESULT])\n",
    "    EMPTY_TAG = \"Empty\"\n",
    "    \n",
    "    def __init__(self, feat_extractor, tags, cr_tags, base_learner_fact, beta):\n",
    "        # init checks\n",
    "        #assert CAUSER in tags, \"%s must be in tags\" % CAUSER\n",
    "        #assert RESULT in tags, \"%s must be in tags\" % RESULT\n",
    "        #assert EXPLICIT in tags, \"%s must be in tags\" % EXPLICIT\n",
    "\n",
    "        self.feat_extractor = feature_extractor    # feature extractor (for use later)\n",
    "        self.base_learner_fact = base_learner_fact # Sklearn classifier\n",
    "        self.tags = set(tags)                      # tags for basic tagging\n",
    "        \n",
    "        self.cr_tags = set(cr_tags)                # causal relation tags\n",
    "            \n",
    "        self.actions = set([SearnModel.Shift, SearnModel.Reduce, SearnModel.Left_ARC, SearnModel.Right_ARC, SearnModel.Skip])\n",
    "        self.epoch = -1\n",
    "        self.beta = beta\n",
    "        self.stack = []\n",
    "        self.tagging_models = {}\n",
    "        self.parser_model = None\n",
    "        \n",
    "    def train(self, essay_feats, epochs):\n",
    "        #essay_feats = self.feat_extractor.transform(tagged_essays)        \n",
    "        for i in range(0, epochs):\n",
    "            self.epoch +=1\n",
    "            tagging_models = {}\n",
    "            examples_with_loss = dict() # dict of tag\\decision to examples (with labels)\n",
    "            \n",
    "            for essay_ix, essay in enumerate(essay_feats):\n",
    "                for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "                    tags_exs, parse_exs = self.taggged_sentence(taggged_sentence)                    \n",
    "    \n",
    "    def tag_sentence(self, taggged_sentence):\n",
    "        prediction_history = []\n",
    "        ground_truth_history = []\n",
    "        previous_words = []\n",
    "        self.stack = [Root]\n",
    "        \n",
    "        tagging_examples = []\n",
    "        parsing_examples = []\n",
    "        arcs = set()\n",
    "\n",
    "        gt_cr_tags = set()\n",
    "        for i, (wd) in enumerate(taggged_sentence):\n",
    "            gt_cr_tags.update(wd.tags)\n",
    "        gt_cr_tags = gt_cr_tags.intersection(self.cr_tags)\n",
    "        \n",
    "        for i, (wd) in enumerate(taggged_sentence):\n",
    "            #TODO - decay beta to determine when to return GT and when not\n",
    "            return_ground_truth = True\n",
    "            \n",
    "            gt_tags = wd.tags.intersection(self.tags)\n",
    "            gt_cr_tags = wd.tags.intersection(self.cr_tags)\n",
    "            \n",
    "            current_word = wd.word\n",
    "            # Get tagging features            \n",
    "            window_feats = dict(wd.features.items()) # - Don't mutate the feat dictionary\n",
    "            tag_hist_feats = self.get_conditional_feats(current_word, previous_words, ground_truth_history)\n",
    "            tag_feats = tag_hist_feats.update(window_feats)\n",
    "            # Tag Word\n",
    "            predicted_tag = self.tag_word(self, gt_tags, tag_feats, return_ground_truth)\n",
    "            for gt_tag in gt_tags:\n",
    "                #TODO - fill in the loss here\n",
    "                tagging_examples.append((tag_feats, gt_tag, 1.0)) #xs, y, LOSS\n",
    "                \n",
    "            if self.__is_new_tag__(predicted_tag):\n",
    "                parse_feats = dict(tag_feats.items())\n",
    "                parse_feats[\"WORD_CONCEPT_TAG:\" + str(predicted_tag)] = 1\n",
    "                \n",
    "                # has to come before parse_word, as that modifies the stack\n",
    "                oracle_decision = self.parse_word_oracle(predicted_tag, gt_cr_tags)\n",
    "                #TODO - fill in the loss here\n",
    "                parsing_examples.append((parse_feats, oracle_decision, 1.0))\n",
    "                #Modifies the stack\n",
    "                parse_decision = self.parse_word(predicted_tag, parse_feats, oracle_decision, return_ground_truth, arcs)\n",
    "                parse_decisions = []\n",
    "                # Left_Arc and Reduce don't advance the buffer\n",
    "                while parse_decision in [Left_ARC, Reduce]:\n",
    "                    #TODO  What ground truth history do we add here?\n",
    "                    parse_feats = self.get_conditional_feats(current_word, previous_words, ground_truth_history)\n",
    "                    parse_feats.update(window_feats)\n",
    "                    parse_feats[\"WORD_CONCEPT_TAG:\" + str(predicted_tag)] = 1\n",
    "                    \n",
    "                    pass\n",
    "                pass\n",
    "                \n",
    "            previous_words.append(wd.word)\n",
    "            ground_truth_history.append(gt_tags.union(gt_cr_tags))            \n",
    "            \n",
    "            ## parse decisions\n",
    "            new_tags = set([predicted_tag])\n",
    "            #TODO - get parser decisions\n",
    "            parser_tags = set()\n",
    "            new_tags.update(parser_tags)\n",
    "            prediction_history.append(new_tags)\n",
    "            \n",
    "        return tagging_examples, parsing_examples\n",
    "    \n",
    "    def get_conditional_feats(self, current_word, prev_words, history):\n",
    "        feats = {}\n",
    "        if len(history) > 0:\n",
    "            #TODO add feats for tags not predicted and actions not taken\n",
    "            prev_tags = history[-1]\n",
    "            for t in prev_tags:\n",
    "                feats[\"tag-1:\" + t] = 1\n",
    "                feats[\"tag-1:\" + t + \" wd:\"  + current_word] = 1\n",
    "                feats[\"tag-1:\" + t + \" wd-1:\"+ prev_words[-1]] = 1\n",
    "                \n",
    "            if len(history) > 1:\n",
    "                prev_prev_tags = history[-2]\n",
    "                for t in prev_prev_tags:\n",
    "                    # tag bigrams\n",
    "                    for prev_tag in prev_tags:\n",
    "                        feats[\"tag-1:\" + prev_tag + \" tag-2:\" + t] = 1\n",
    "                    feats[\"tag-2:\" + t] = 1\n",
    "                    feats[\"tag-2:\" + t + \" wd:\"   + current_word] = 1\n",
    "                    feats[\"tag-2:\" + t + \" wd-2:\" + prev_words[-2]] = 1\n",
    "        \n",
    "        for i in range(4):\n",
    "            offset = -(i+1)\n",
    "            if len(self.stack) < abs(offset):\n",
    "                break\n",
    "            s1_key = \"s\" + str(offset) + \":\" + self.stack[offset]\n",
    "            feats[s1_key] = 1\n",
    "            # Stack Bigram feats\n",
    "            if len(self.stack) >= abs(offset) + 1:\n",
    "                next_offset = offset-1\n",
    "                s2_key = \"s\" + str(next_offset) + \":\" + self.stack[next_offset]\n",
    "                feats[s1_key + \" \" + s2_key] = 1\n",
    "        return feats\n",
    "    \n",
    "    def tag_word(self, ground_truth_tags, tag_feats, return_ground_truth):\n",
    "        tags = None\n",
    "        if return_ground_truth:\n",
    "            tags = self.tags.intersection(ground_truth_tags)            \n",
    "        else:\n",
    "            #TODO - implement word tagging model\n",
    "            raise Exception(\"#TODO - implement word tagger\")\n",
    "        if not tags:\n",
    "            return EMPTY_TAG\n",
    "        most_freq = max(tags, key = lambda tag: self.tag_freq[tag])\n",
    "        return most_freq\n",
    "    \n",
    "    def __is_new_tag__(self, predicted_tag, history):\n",
    "        if predicted_tag == EMPTY_TAG:\n",
    "            return False\n",
    "        if len(history) == 0 or predicted_tag not in history[-1]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def parse_word(self, predicted_tag, parse_feats, oracle_decision, return_ground_truth, arcs):\n",
    "        parse_decision = None\n",
    "        if return_ground_truth:\n",
    "            parse_decision = oracle_decision\n",
    "        else:\n",
    "            #TODO - implement ML parser\n",
    "            raise Exception(\"#TODO - implement parser\")\n",
    "        if parse_decision == Shift:\n",
    "            self.stack.append(predicted_tag)\n",
    "        elif parse_decision == Reduce:\n",
    "            self.stack.pop()\n",
    "        elif parse_decision == Left_ARC:\n",
    "            tos = self.stack.pop()\n",
    "            arc = [tos,predicted_tag] # Cause -> Effect  \n",
    "            arcs.add(arc)\n",
    "        elif parse_decision == Right_ARC:\n",
    "            arc = [predicted_tag, self.stack[-1]] # Effect <- Cause so [\"Cause Code\",\"Effect Code\"]\n",
    "            arcs.add(arc)\n",
    "            self.stack.append(predicted_tag)\n",
    "        return parse_decision\n",
    "        \n",
    "    def parse_word_oracle(self, predicted_tag, golden_tags, prediction_history):\n",
    "        csl_tags = self.cr_tags.intersection(golden_tags)\n",
    "        tos = self.stack[-1]\n",
    "        #TODO - about 5% of causal relations lack a constituent cause, effect or both - remove these from training?\n",
    "        #TODO - if no causal in the sentence, shift, reduce, shift, reduce, etc\n",
    "        #TODO - if causal in sentence, but this code not in causal, shift, reduce\n",
    "        #TODO - or do we implement a second classifier - Causal, explicit, Result\n",
    "        if len(prediction_history) == 0 or len(csl_tags) == 0:\n",
    "            return Shift\n",
    "        pairs = []\n",
    "        for crel in csl_tags:\n",
    "            left, right = crel.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "            pairs.append((left,right))\n",
    "        \n",
    "        if len(pairs) == 1:\n",
    "            left, right = pairs[0]\n",
    "            if tos == left and predicted_tag == right:\n",
    "                return Left_ARC\n",
    "            elif tos == right and predicted_tag == left:\n",
    "                return Right_ARC\n",
    "        \n",
    "        #TODO - get optimal parse decision given current state\n",
    "        #TODO - enforce valid decisions\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('05 ', '50 ')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(code):\n",
    "    if not code.endswith(\"b\"):\n",
    "        code += \" \"\n",
    "    return code.rjust(3, \"0\")\n",
    "\n",
    "def extract_lr(cr):\n",
    "    return cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "\n",
    "def normalize_cr(cr):\n",
    "    pair = extract_lr(cr)\n",
    "    return (normalize(pair[0]),normalize(pair[1]))\n",
    "\n",
    "normalize_cr(\"Causer:5->Result:50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note You Can Set Per Observation Weights in XGBoost\n",
    "* http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "  * see weight parameter in dtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TODO\n",
    "* Need to make sure the tagger tags EXCPLICT tags. These can then be skipped by the parser, but will be included in the features used to train the parser and taggger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
