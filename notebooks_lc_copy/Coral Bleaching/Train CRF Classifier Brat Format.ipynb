{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Sequential Based Classier on the Coral Bleaching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Decorators import memoize_to_disk\n",
    "from sent_feats_for_stacking import *\n",
    "from load_data import load_process_essays, extract_features\n",
    "\n",
    "from featurevectorizer import FeatureVectorizer\n",
    "from featureextractionfunctions import *\n",
    "from CrossValidation import cross_validation\n",
    "from wordtagginghelper import *\n",
    "from IterableFP import flatten\n",
    "from predictions_to_file import predictions_to_file\n",
    "from results_procesor import ResultsProcessor\n",
    "# Classifiers\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from tag_frequency import get_tag_freq, regular_tag\n",
    "# END Classifiers\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Essays\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Create persister (mongo client) - fail fast if mongo service not initialized\n",
    "processor = ResultsProcessor()\n",
    "\n",
    "# not hashed as don't affect persistence of feature processing\n",
    "SPARSE_WD_FEATS     = True\n",
    "\n",
    "MIN_FEAT_FREQ       = 5        # 5 best so far\n",
    "CV_FOLDS            = 5\n",
    "\n",
    "MIN_TAG_FREQ        = 5\n",
    "LOOK_BACK           = 0     # how many sentences to look back when predicting tags\n",
    "# end not hashed\n",
    "\n",
    "# construct unique key using settings for pickling\n",
    "\n",
    "settings = Settings.Settings()\n",
    "folder =                            settings.data_directory + \"CoralBleaching/BrattData/EBA1415_Merged/\"\n",
    "processed_essay_filename_prefix =   settings.data_directory + \"CoralBleaching/BrattData/Pickled/essays_proc_pickled_\"\n",
    "features_filename_prefix =          settings.data_directory + \"CoralBleaching/BrattData/Pickled/feats_pickled_\"\n",
    "models_folder =                     settings.data_directory + \"CoralBleaching/models/CRF\"\n",
    "out_metrics_file     =              settings.data_directory + \"CoralBleaching/Results/metrics.txt\"\n",
    "\n",
    "config = get_config(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder': '/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/',\n",
       " 'include_normal': False,\n",
       " 'include_vague': True,\n",
       " 'lower_case': True,\n",
       " 'min_df': 2,\n",
       " 'min_sentence_length': 3,\n",
       " 'remove_infrequent': False,\n",
       " 'remove_punctuation': False,\n",
       " 'remove_stop_words': False,\n",
       " 'replace_nums': True,\n",
       " 'spelling_correct': True,\n",
       " 'stem': False,\n",
       " 'window_size': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pickle Key:', 'folder_/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/EBA1415_Merged/_include_normal_False_include_vague_True_lower_case_True_min_df_2_min_sentence_length_3_remove_infrequent_False_remove_punctuation_False_remove_stop_words_False_replace_nums_True_spelling_correct_True_stem_False_window_size_7')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_process_essays = memoize_to_disk(filename_prefix=processed_essay_filename_prefix)(load_process_essays)\n",
    "tagged_essays = mem_process_essays( **config )\n",
    "logger.info(\"Essays loaded\")\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Corpus in CRF Format (list of list of tuples(word,tag))\n",
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', set()),\n",
       " ('leads', set()),\n",
       " ('to', set()),\n",
       " ('differences', set()),\n",
       " ('in', set()),\n",
       " ('the', set()),\n",
       " ('rates', set()),\n",
       " ('of', set()),\n",
       " ('coral', {'50'}),\n",
       " ('bleaching', {'50'}),\n",
       " ('.', set())]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_essays[0].sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "\n",
    "def tally_code_frequencies(tagged_essays):\n",
    "    freq = defaultdict(int)\n",
    "    all_codes = set()\n",
    "    for essay in tagged_essays:\n",
    "        for i, sentence in enumerate(essay.sentences):\n",
    "            words, tags = zip(*sentence)\n",
    "            utags = set(flatten(tags))\n",
    "            all_codes.update(utags)\n",
    "            for t in utags:\n",
    "                freq[t] += 1\n",
    "    return freq\n",
    "\n",
    "code_freq = tally_code_frequencies(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '5b', '6', '7', '11', '12', '13', '14', '50']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tags = list((t for t in all_codes if t[0].isdigit()))\n",
    "sorted(regular_tags, key = lambda s: (len(s.replace(\"b\",\"\")), s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INSIDE = \"I\"\n",
    "OUTSIDE = \"O\"\n",
    "\n",
    "def to_tagged_sentences_by_code(essays, codes):\n",
    "    code2sents = defaultdict(list)\n",
    "    for essay in essays:\n",
    "        for i, sentence in enumerate(essay.sentences):\n",
    "            for code in codes:\n",
    "                sent = []\n",
    "                for wd, tags in sentence:\n",
    "                    if code in tags:\n",
    "                        sent.append((unicode(wd), INSIDE))\n",
    "                    else:\n",
    "                        sent.append((wd, OUTSIDE))\n",
    "                code2sents[code].append(sent)\n",
    "    return code2sents\n",
    "\n",
    "def to_most_common_code_tagged_sentences(essays, codes, code_freq):\n",
    "    codes = set(codes)\n",
    "    tagged = []\n",
    "    for essay in essays:\n",
    "        for i, sentence in enumerate(essay.sentences):\n",
    "            sent = []\n",
    "            for wd, tags in sentence:\n",
    "                # filter to target codes only\n",
    "                tags = codes.intersection(tags)\n",
    "                if len(tags) > 0:\n",
    "                    most_common = max(tags, key = lambda tag: code_freq[tag])\n",
    "                    sent.append((wd, most_common))\n",
    "                else:\n",
    "                    sent.append((wd, OUTSIDE))\n",
    "            tagged.append(sent)\n",
    "    return tagged\n",
    "\n",
    "def to_label_powerset_tagged_sentences(essays, codes):\n",
    "    codes = set(codes)\n",
    "    tagged = []\n",
    "    for essay in essays:\n",
    "        for i, sentence in enumerate(essay.sentences):\n",
    "            sent = []\n",
    "            for wd, tags in sentence:\n",
    "                # filter to target codes only\n",
    "                isect_tags = \",\".join(sorted(codes.intersection(tags)))\n",
    "                if len(isect_tags) > 0:\n",
    "                    # append as powerset label\n",
    "                    sent.append((wd, isect_tags))\n",
    "                else:\n",
    "                    sent.append((wd, OUTSIDE))\n",
    "            tagged.append(sent)\n",
    "    return tagged\n",
    "\n",
    "def to_sentences(tagged_sentences):\n",
    "    sents = []\n",
    "    for sentence in tagged_sentences:\n",
    "        words, tags = zip(*sentence)\n",
    "        sents.append(words)\n",
    "    return sents\n",
    "\n",
    "# flattens list of sentences to a flattened list of binary tags\n",
    "def to_flattened_binary_tags(tagged_sentences):\n",
    "    tags = []\n",
    "    for sentence in tagged_sentences:\n",
    "        words, lbls = zip(*sentence)\n",
    "        tags.extend((1 if t == INSIDE else 0 for t in lbls))\n",
    "    return tags\n",
    "\n",
    "def to_flattened_binary_tags_by_code(tagged_sentences, codes):\n",
    "    code2sents = defaultdict(list)\n",
    "    for sentence in tagged_sentences:\n",
    "        words, lbls = zip(*sentence)\n",
    "        # for each word's tag (expects a single tag)\n",
    "        for t in lbls:\n",
    "            if type(t) != set:\n",
    "                t = set(t.split(\",\"))\n",
    "            for code in codes:\n",
    "                code2sents[code].append(1 if code in t else 0)\n",
    "    return code2sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features(tokens, idx):\n",
    "    \"\"\"\n",
    "    Extract basic features about this word\n",
    "    :return : a list which contains the features\n",
    "    :rtype : list(str)\n",
    "    \"\"\" \n",
    "    token = tokens[idx]\n",
    "\n",
    "    feature_list = []\n",
    "    feature_list.append('WORD_' + token )\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Binary Relevance Tagger on Single Word Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Training code: 4\n",
      "Fold 0 Training code: 5\n",
      "Fold 0 Training code: 5b\n",
      "Fold 0 Training code: 50\n",
      "Fold 0 Training code: 3\n",
      "Fold 0 Training code: 7\n",
      "Fold 0 Training code: 2\n",
      "Fold 0 Training code: 6\n",
      "Fold 0 Training code: 11\n",
      "Fold 0 Training code: 13\n",
      "Fold 0 Training code: 14\n",
      "Fold 0 Training code: 1\n",
      "Fold 0 Training code: 12\n",
      "Fold 1 Training code: 4\n",
      "Fold 1 Training code: 5\n",
      "Fold 1 Training code: 5b\n",
      "Fold 1 Training code: 50\n",
      "Fold 1 Training code: 3\n",
      "Fold 1 Training code: 7\n",
      "Fold 1 Training code: 2\n",
      "Fold 1 Training code: 6\n",
      "Fold 1 Training code: 11\n",
      "Fold 1 Training code: 13\n",
      "Fold 1 Training code: 14\n",
      "Fold 1 Training code: 1\n",
      "Fold 1 Training code: 12\n",
      "Fold 2 Training code: 4\n",
      "Fold 2 Training code: 5\n",
      "Fold 2 Training code: 5b\n",
      "Fold 2 Training code: 50\n",
      "Fold 2 Training code: 3\n",
      "Fold 2 Training code: 7\n",
      "Fold 2 Training code: 2\n",
      "Fold 2 Training code: 6\n",
      "Fold 2 Training code: 11\n",
      "Fold 2 Training code: 13\n",
      "Fold 2 Training code: 14\n",
      "Fold 2 Training code: 1\n",
      "Fold 2 Training code: 12\n",
      "Fold 3 Training code: 4\n",
      "Fold 3 Training code: 5\n",
      "Fold 3 Training code: 5b\n",
      "Fold 3 Training code: 50\n",
      "Fold 3 Training code: 3\n",
      "Fold 3 Training code: 7\n",
      "Fold 3 Training code: 2\n",
      "Fold 3 Training code: 6\n",
      "Fold 3 Training code: 11\n",
      "Fold 3 Training code: 13\n",
      "Fold 3 Training code: 14\n",
      "Fold 3 Training code: 1\n",
      "Fold 3 Training code: 12\n",
      "Fold 4 Training code: 4\n",
      "Fold 4 Training code: 5\n",
      "Fold 4 Training code: 5b\n",
      "Fold 4 Training code: 50\n",
      "Fold 4 Training code: 3\n",
      "Fold 4 Training code: 7\n",
      "Fold 4 Training code: 2\n",
      "Fold 4 Training code: 6\n",
      "Fold 4 Training code: 11\n",
      "Fold 4 Training code: 13\n",
      "Fold 4 Training code: 14\n",
      "Fold 4 Training code: 1\n",
      "Fold 4 Training code: 12\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from numpy.random import randint\n",
    "import os\n",
    "\n",
    "fold_models = []\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "folds = cross_validation(tagged_essays, CV_FOLDS)\n",
    "for fold,(essays_TD, essays_VD) in enumerate(folds):\n",
    "    td_sents_by_code = to_tagged_sentences_by_code(essays_TD, regular_tags)\n",
    "    vd_sents_by_code = to_tagged_sentences_by_code(essays_VD, regular_tags)\n",
    "\n",
    "    code2model = dict()\n",
    "    fold_models.append(code2model)\n",
    "    \n",
    "    wd_td_ys_bytag = dict()\n",
    "    wd_vd_ys_bytag = dict()\n",
    "    td_wd_predictions_by_code = dict()\n",
    "    vd_wd_predictions_by_code = dict()\n",
    "    \n",
    "    for code in sorted(regular_tags):\n",
    "        print(\"Fold %i Training code: %s\" % (fold, code))\n",
    "        td, vd = td_sents_by_code[code], vd_sents_by_code[code]\n",
    "        \n",
    "        model_filename = models_folder + \"/\" + \"%i_%s__%s\" % (fold, \"power_set\", str(randint(0, 9999999)))\n",
    "\n",
    "        model = CRFTagger(feature_func = get_word_features, verbose=False)\n",
    "        model.train(td, model_filename)\n",
    "        code2model[code] = model\n",
    "        \n",
    "        os.remove(model_filename)\n",
    "            \n",
    "        #TODO - non binary\n",
    "        wd_td_ys_bytag[code] = to_flattened_binary_tags(td)\n",
    "        wd_vd_ys_bytag[code] = to_flattened_binary_tags(vd)\n",
    "        \n",
    "        td_predictions = model.tag_sents(to_sentences(td))\n",
    "        vd_predictions = model.tag_sents(to_sentences(vd))\n",
    "\n",
    "        td_wd_predictions_by_code[code] = to_flattened_binary_tags(td_predictions)\n",
    "        vd_wd_predictions_by_code[code] = to_flattened_binary_tags(vd_predictions)\n",
    "        \n",
    "    merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "    merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671460 167865\n",
      "671460 167865\n"
     ]
    }
   ],
   "source": [
    "print len(cv_wd_td_ys_by_tag[\"50\"]), len(cv_wd_vd_ys_by_tag[\"50\"])\n",
    "print len(cv_wd_td_predictions_by_tag[\"50\"]), len(cv_wd_vd_predictions_by_tag[\"50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGGING\n",
      "\n",
      "TAG:       1                     \n",
      "f1:        0.750773816348        0.727482678984        \n",
      "recall:    0.67738932561         0.651634257344        \n",
      "precision: 0.841990228851        0.823314166231        \n",
      "accuracy:  0.98704911685         0.985941083609        \n",
      "sentences:                       4834                  \n",
      "\n",
      "TAG:       2                     \n",
      "f1:        0.637118193891        0.591245791246        \n",
      "recall:    0.499739583333        0.457291666667        \n",
      "precision: 0.878663003663        0.83619047619         \n",
      "accuracy:  0.996744407709        0.996383999047        \n",
      "sentences:                       960                   \n",
      "\n",
      "TAG:       3                     \n",
      "f1:        0.729700828132        0.707602924972        \n",
      "recall:    0.643013674614        0.619214586255        \n",
      "precision: 0.843403276804        0.825426501519        \n",
      "accuracy:  0.983812885354        0.982611026718        \n",
      "sentences:                       5704                  \n",
      "\n",
      "TAG:       4                     \n",
      "f1:        0.740174975698        0.709334823924        \n",
      "recall:    0.640317155214        0.609802979337        \n",
      "precision: 0.876933201711        0.847695390782        \n",
      "accuracy:  0.994427069371        0.993804545319        \n",
      "sentences:                       2081                  \n",
      "\n",
      "TAG:       5                     \n",
      "f1:        0.10964332893         0.0786369593709       \n",
      "recall:    0.0607613469985       0.0439238653001       \n",
      "precision: 0.560810810811        0.375                 \n",
      "accuracy:  0.995984868793        0.995812110922        \n",
      "sentences:                       683                   \n",
      "\n",
      "TAG:       6                     \n",
      "f1:        0.799549126432        0.768306421329        \n",
      "recall:    0.725784447476        0.697817189632        \n",
      "precision: 0.89000418235         0.854636591479        \n",
      "accuracy:  0.996821850892        0.996324427367        \n",
      "sentences:                       1466                  \n",
      "\n",
      "TAG:       7                     \n",
      "f1:        0.717179902755        0.70176575409         \n",
      "recall:    0.593163538874        0.580697050938        \n",
      "precision: 0.906762295082        0.886614817847        \n",
      "accuracy:  0.989604741906        0.989032853781        \n",
      "sentences:                       3730                  \n",
      "\n",
      "TAG:       11                    \n",
      "f1:        0.875781948168        0.867383512545        \n",
      "recall:    0.822147651007        0.812080536913        \n",
      "precision: 0.93690248566         0.930769230769        \n",
      "accuracy:  0.998550918893        0.998457093498        \n",
      "sentences:                       1043                  \n",
      "\n",
      "TAG:       12                    \n",
      "f1:        0.863813229572        0.836870026525        \n",
      "recall:    0.799519807923        0.7575030012          \n",
      "precision: 0.939351198872        0.934814814815        \n",
      "accuracy:  0.998748994728        0.998534536681        \n",
      "sentences:                       833                   \n",
      "\n",
      "TAG:       13                    \n",
      "f1:        0.638244606435        0.607906295754        \n",
      "recall:    0.527250608273        0.505109489051        \n",
      "precision: 0.808431262824        0.763235294118        \n",
      "accuracy:  0.99268310845         0.992023352098        \n",
      "sentences:                       2055                  \n",
      "\n",
      "TAG:       14                    \n",
      "f1:        0.722670444048        0.678260869565        \n",
      "recall:    0.605844692276        0.563816604709        \n",
      "precision: 0.895315122844        0.850997506234        \n",
      "accuracy:  0.993293718166        0.992285467489        \n",
      "sentences:                       2421                  \n",
      "\n",
      "TAG:       50                    \n",
      "f1:        0.878131373379        0.868588630311        \n",
      "recall:    0.839464390818        0.827913478517        \n",
      "precision: 0.920532472771        0.913467002192        \n",
      "accuracy:  0.981133649063        0.979715843088        \n",
      "sentences:                       13592                 \n",
      "\n",
      "TAG:       5b                    \n",
      "f1:        0.0100035727045       0.0135501355014       \n",
      "recall:    0.00539291217257      0.00770416024653      \n",
      "precision: 0.0689655172414       0.0561797752809       \n",
      "accuracy:  0.995873171894        0.995663181723        \n",
      "sentences:                       649                   \n",
      "\n",
      "TAG:       MEAN                  \n",
      "f1:        0.651752718961        0.627456524932        \n",
      "recall:    0.572291471891        0.548808374316        \n",
      "precision: 0.797543466114        0.761410889804        \n",
      "accuracy:  0.992671423236        0.992045347796        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       MEAN_CONCEPT_CODES    \n",
      "f1:        0.651752718961        0.627456524932        \n",
      "recall:    0.572291471891        0.548808374316        \n",
      "precision: 0.797543466114        0.761410889804        \n",
      "accuracy:  0.992671423236        0.992045347796        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       WEIGHTED_MEAN         \n",
      "f1:        0.76157216903         0.741602807468        \n",
      "recall:    0.688116401588        0.667848493171        \n",
      "precision: 0.868007285673        0.846459180336        \n",
      "accuracy:  0.987296954507        0.986290391912        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "TAG:       WEIGHTED_MEAN_CONCEPT_CODES  \n",
      "f1:        0.76157216903         0.741602807468        \n",
      "recall:    0.688116401588        0.667848493171        \n",
      "precision: 0.868007285673        0.846459180336        \n",
      "accuracy:  0.987296954507        0.986290391912        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "Macro F1:  0.666397551203        0.637860675786        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training completed\")\n",
    "\n",
    "\"\"\" Persist Results to Mongo DB \"\"\"\n",
    "\n",
    "wd_algo = \"CRF\"\n",
    "SUFFIX = \"_CRF\"\n",
    "CB_TAGGING_TD, CB_TAGGING_VD, CB_SENT_TD, CB_SENT_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX, \"CB_SENT_TD\" + SUFFIX, \"CB_SENT_VD\" + SUFFIX\n",
    "parameters = dict(config)\n",
    "#parameters[\"extractors\"] = map(lambda fn: fn.func_name, extractors)\n",
    "parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "\n",
    "wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "\n",
    "# This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\n",
    "\n",
    "print processor.results_to_string(wd_td_objectid,   CB_TAGGING_TD,  wd_vd_objectid,     CB_TAGGING_VD,  \"TAGGING\")\n",
    "logger.info(\"Results Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Label Powerset Tagger On Single Word Features Only (Slightly Better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Training code\n",
      "Fold 1 Training code\n",
      "Fold 2 Training code\n",
      "Fold 3 Training code\n",
      "Fold 4 Training code\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from numpy.random import randint\n",
    "import os\n",
    "\n",
    "fold_models = []\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "folds = cross_validation(tagged_essays, CV_FOLDS)\n",
    "for fold,(essays_TD, essays_VD) in enumerate(folds):\n",
    "        \n",
    "    # For training\n",
    "    td_sents = to_label_powerset_tagged_sentences(essays_TD, regular_tags)\n",
    "    vd_sents = to_label_powerset_tagged_sentences(essays_VD, regular_tags)\n",
    "\n",
    "    fold_models.append(code2model)\n",
    "    \n",
    "    # Start Training\n",
    "    print(\"Fold %i Training code\" % (fold))\n",
    "\n",
    "    model_filename = models_folder + \"/\" + \"%i_%s__%s\" % (fold, \"power_set\", str(randint(0, 9999999)))\n",
    "    \n",
    "    model = CRFTagger(feature_func = get_word_features, verbose=False)\n",
    "    model.train(td_sents, model_filename)\n",
    "    \n",
    "    td_predictions = model.tag_sents(to_sentences(td_sents))\n",
    "    vd_predictions = model.tag_sents(to_sentences(vd_sents))\n",
    "\n",
    "    # for evaluation - binary tags\n",
    "    # YS (ACTUAL)\n",
    "    wd_td_ys_bytag = to_flattened_binary_tags_by_code(td_sents, regular_tags)\n",
    "    wd_vd_ys_bytag = to_flattened_binary_tags_by_code(vd_sents, regular_tags)\n",
    "\n",
    "    # YS (PREDICTED)\n",
    "    td_wd_predictions_by_code = to_flattened_binary_tags_by_code(td_predictions, regular_tags)\n",
    "    vd_wd_predictions_by_code = to_flattened_binary_tags_by_code(vd_predictions, regular_tags)\n",
    "    \n",
    "    # merge results for fold\n",
    "    merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "    merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "    \n",
    "    os.remove(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGGING\n",
      "\n",
      "TAG:       1                     \n",
      "f1:        0.780710276339        0.757344940152        \n",
      "recall:    0.748086470832        0.719900703351        \n",
      "precision: 0.816309255079        0.798898071625        \n",
      "accuracy:  0.987898013284        0.986715515444        \n",
      "sentences:                       4834                  \n",
      "\n",
      "TAG:       2                     \n",
      "f1:        0.660530421217        0.621079046424        \n",
      "recall:    0.551302083333        0.515625              \n",
      "precision: 0.82373540856         0.780757097792        \n",
      "accuracy:  0.996759300628        0.996401870551        \n",
      "sentences:                       960                   \n",
      "\n",
      "TAG:       3                     \n",
      "f1:        0.734990550952        0.710194506891        \n",
      "recall:    0.664796633941        0.636921458626        \n",
      "precision: 0.821757503522        0.802518223989        \n",
      "accuracy:  0.983710124207        0.982336996992        \n",
      "sentences:                       5704                  \n",
      "\n",
      "TAG:       4                     \n",
      "f1:        0.776338175631        0.752941176471        \n",
      "recall:    0.712638154733        0.691975012013        \n",
      "precision: 0.852543834435        0.825688073394        \n",
      "accuracy:  0.994909599976        0.994370476276        \n",
      "sentences:                       2081                  \n",
      "\n",
      "TAG:       5                     \n",
      "f1:        0.317017643711        0.213173652695        \n",
      "recall:    0.203879941435        0.130307467057        \n",
      "precision: 0.712276214834        0.585526315789        \n",
      "accuracy:  0.996425699223        0.996086140649        \n",
      "sentences:                       683                   \n",
      "\n",
      "TAG:       6                     \n",
      "f1:        0.840025493945        0.811127379209        \n",
      "recall:    0.786664392906        0.755798090041        \n",
      "precision: 0.901152568861        0.875197472354        \n",
      "accuracy:  0.997383313973        0.996926101331        \n",
      "sentences:                       1466                  \n",
      "\n",
      "TAG:       7                     \n",
      "f1:        0.727160255171        0.707064461277        \n",
      "recall:    0.630294906166        0.613136729223        \n",
      "precision: 0.859205116492        0.834976268711        \n",
      "accuracy:  0.989490066422        0.988711166711        \n",
      "sentences:                       3730                  \n",
      "\n",
      "TAG:       11                    \n",
      "f1:        0.878835153406        0.863376623377        \n",
      "recall:    0.810162991371        0.796740172579        \n",
      "precision: 0.960227272727        0.942176870748        \n",
      "accuracy:  0.998611979865        0.998433264826        \n",
      "sentences:                       1043                  \n",
      "\n",
      "TAG:       12                    \n",
      "f1:        0.89877675841         0.866501854141        \n",
      "recall:    0.882052821128        0.841536614646        \n",
      "precision: 0.91614713217         0.892993630573        \n",
      "accuracy:  0.999014088702        0.99871325172         \n",
      "sentences:                       833                   \n",
      "\n",
      "TAG:       13                    \n",
      "f1:        0.707349966285        0.669931972789        \n",
      "recall:    0.638077858881        0.59902676399         \n",
      "precision: 0.793494704992        0.75987654321         \n",
      "accuracy:  0.993536472761        0.992773955262        \n",
      "sentences:                       2055                  \n",
      "\n",
      "TAG:       14                    \n",
      "f1:        0.725432307043        0.699085219066        \n",
      "recall:    0.621643948781        0.599752168525        \n",
      "precision: 0.870823086938        0.837853433353        \n",
      "accuracy:  0.993213296399        0.992553540047        \n",
      "sentences:                       2421                  \n",
      "\n",
      "TAG:       50                    \n",
      "f1:        0.875275653693        0.867670152055        \n",
      "recall:    0.843198204826        0.833357857563        \n",
      "precision: 0.909890240756        0.904929296157        \n",
      "accuracy:  0.980542400143        0.97941798469         \n",
      "sentences:                       13592                 \n",
      "\n",
      "TAG:       5b                    \n",
      "f1:        0.116187989556        0.0719794344473       \n",
      "recall:    0.0685670261941       0.0431432973806       \n",
      "precision: 0.380341880342        0.217054263566        \n",
      "accuracy:  0.995966997289        0.995698924731        \n",
      "sentences:                       649                   \n",
      "\n",
      "TAG:       MEAN                  \n",
      "f1:        0.695279280412        0.662420801461        \n",
      "recall:    0.627797341118        0.598247795           \n",
      "precision: 0.816761863054        0.773726581636        \n",
      "accuracy:  0.992881642529        0.992241476095        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       MEAN_CONCEPT_CODES    \n",
      "f1:        0.695279280412        0.662420801461        \n",
      "recall:    0.627797341118        0.598247795           \n",
      "precision: 0.816761863054        0.773726581636        \n",
      "accuracy:  0.992881642529        0.992241476095        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       WEIGHTED_MEAN         \n",
      "f1:        0.779597295796        0.758500484529        \n",
      "recall:    0.723209158323        0.701655389379        \n",
      "precision: 0.857115048501        0.835480942105        \n",
      "accuracy:  0.987274493681        0.986328716443        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "TAG:       WEIGHTED_MEAN_CONCEPT_CODES  \n",
      "f1:        0.779597295796        0.758500484529        \n",
      "recall:    0.723209158323        0.701655389379        \n",
      "precision: 0.857115048501        0.835480942105        \n",
      "accuracy:  0.987274493681        0.986328716443        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "TAG:       MICRO_F1              \n",
      "f1:        0.788550932763        0.768496615847        \n",
      "recall:    0.723209158323        0.701655389379        \n",
      "precision: 0.866872671226        0.849413613831        \n",
      "accuracy:  0.992881642529        0.992241476095        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "Macro F1:  0.709920264217        0.674765111184        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training completed\")\n",
    "\n",
    "\"\"\" Persist Results to Mongo DB \"\"\"\n",
    "\n",
    "wd_algo = \"CRF_lbl_powerset\"\n",
    "SUFFIX = \"_CRF_LBL_POWERSET_TEST\"\n",
    "CB_TAGGING_TD, CB_TAGGING_VD, CB_SENT_TD, CB_SENT_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX, \"CB_SENT_TD\" + SUFFIX, \"CB_SENT_VD\" + SUFFIX\n",
    "parameters = dict(config)\n",
    "#parameters[\"extractors\"] = map(lambda fn: fn.func_name, extractors)\n",
    "parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "\n",
    "wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "\n",
    "# This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\n",
    "\n",
    "print processor.results_to_string(wd_td_objectid,   CB_TAGGING_TD,  wd_vd_objectid,     CB_TAGGING_VD,  \"TAGGING\")\n",
    "logger.info(\"Results Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Most Frequent Label Tagger On Single Word Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Training code\n",
      "Fold 1 Training code\n",
      "Fold 2 Training code\n",
      "Fold 3 Training code\n",
      "Fold 4 Training code\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from numpy.random import randint\n",
    "import os\n",
    "\n",
    "fold_models = []\n",
    "#TD, act vs pred\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "#VD, act vs pred\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "folds = cross_validation(tagged_essays, CV_FOLDS)\n",
    "for fold,(essays_TD, essays_VD) in enumerate(folds):\n",
    "        \n",
    "    # For training\n",
    "    td_sents = to_most_common_code_tagged_sentences(essays_TD, regular_tags, code_freq)\n",
    "    vd_sents = to_most_common_code_tagged_sentences(essays_VD, regular_tags, code_freq)\n",
    "\n",
    "    fold_models.append(code2model)\n",
    "    \n",
    "    # Start Training\n",
    "    print(\"Fold %i Training code\" % (fold))\n",
    "\n",
    "    model_filename = models_folder + \"/\" + \"%i_%s__%s\" % (fold, \"most_freq_code\", str(randint(0, 9999999)))\n",
    "    \n",
    "    model = CRFTagger(feature_func = get_word_features, verbose=False)\n",
    "    model.train(td_sents, model_filename)\n",
    "    \n",
    "    td_predictions = model.tag_sents(to_sentences(td_sents))\n",
    "    vd_predictions = model.tag_sents(to_sentences(vd_sents))\n",
    "\n",
    "    # for evaluation - binary tags\n",
    "    # YS (ACTUAL)\n",
    "    td_sents_pset = to_label_powerset_tagged_sentences(essays_TD, regular_tags)\n",
    "    vd_sents_pset = to_label_powerset_tagged_sentences(essays_VD, regular_tags)\n",
    "    \n",
    "    wd_td_ys_bytag = to_flattened_binary_tags_by_code(td_sents_pset, regular_tags)\n",
    "    wd_vd_ys_bytag = to_flattened_binary_tags_by_code(vd_sents_pset, regular_tags)\n",
    "\n",
    "    # YS (PREDICTED)\n",
    "    td_wd_predictions_by_code = to_flattened_binary_tags_by_code(td_predictions, regular_tags)\n",
    "    vd_wd_predictions_by_code = to_flattened_binary_tags_by_code(vd_predictions, regular_tags)\n",
    "    \n",
    "    # merge results for fold\n",
    "    merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "    merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "    \n",
    "    os.remove(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGGING\n",
      "\n",
      "TAG:       1                     \n",
      "f1:        0.780532549168        0.75814863103         \n",
      "recall:    0.748138187836        0.721762515515        \n",
      "precision: 0.815859229598        0.798398169336        \n",
      "accuracy:  0.987884609657        0.986739344116        \n",
      "sentences:                       4834                  \n",
      "\n",
      "TAG:       2                     \n",
      "f1:        0.658981149712        0.626582278481        \n",
      "recall:    0.55078125            0.515625              \n",
      "precision: 0.820085304382        0.798387096774        \n",
      "accuracy:  0.996739939833        0.996485270902        \n",
      "sentences:                       960                   \n",
      "\n",
      "TAG:       3                     \n",
      "f1:        0.735076706818        0.710711493354        \n",
      "recall:    0.664665147265        0.63744740533         \n",
      "precision: 0.822174030903        0.803003533569        \n",
      "accuracy:  0.983720549251        0.982366782831        \n",
      "sentences:                       5704                  \n",
      "\n",
      "TAG:       4                     \n",
      "f1:        0.775414021077        0.753721598329        \n",
      "recall:    0.711556943777        0.693416626622        \n",
      "precision: 0.851862505393        0.825514874142        \n",
      "accuracy:  0.99489023918         0.994382390612        \n",
      "sentences:                       2081                  \n",
      "\n",
      "TAG:       5                     \n",
      "f1:        0.323712507074        0.216346153846        \n",
      "recall:    0.209370424597        0.1317715959          \n",
      "precision: 0.713216957606        0.604026845638        \n",
      "accuracy:  0.996440592142        0.996115926489        \n",
      "sentences:                       683                   \n",
      "\n",
      "TAG:       6                     \n",
      "f1:        0.840471189846        0.811147781445        \n",
      "recall:    0.784788540246        0.754433833561        \n",
      "precision: 0.904658934539        0.877081681205        \n",
      "accuracy:  0.997398206892        0.996932058499        \n",
      "sentences:                       1466                  \n",
      "\n",
      "TAG:       7                     \n",
      "f1:        0.727708607223        0.707663782447        \n",
      "recall:    0.630697050938        0.613941018767        \n",
      "precision: 0.859989033084        0.83515681984         \n",
      "accuracy:  0.989512405802        0.988729038215        \n",
      "sentences:                       3730                  \n",
      "\n",
      "TAG:       11                    \n",
      "f1:        0.878524100299        0.863376623377        \n",
      "recall:    0.810402684564        0.796740172579        \n",
      "precision: 0.95914893617         0.942176870748        \n",
      "accuracy:  0.998607511989        0.998433264826        \n",
      "sentences:                       1043                  \n",
      "\n",
      "TAG:       12                    \n",
      "f1:        0.898395721925        0.864228146311        \n",
      "recall:    0.882352941176        0.836734693878        \n",
      "precision: 0.915032679739        0.89358974359         \n",
      "accuracy:  0.999009620826        0.998695380216        \n",
      "sentences:                       833                   \n",
      "\n",
      "TAG:       13                    \n",
      "f1:        0.70577117786         0.662844662845        \n",
      "recall:    0.63600973236         0.590754257908        \n",
      "precision: 0.792721758908        0.754975124378        \n",
      "accuracy:  0.993508176213        0.992642897566        \n",
      "sentences:                       2055                  \n",
      "\n",
      "TAG:       14                    \n",
      "f1:        0.728214393255        0.69768563163         \n",
      "recall:    0.624328789756        0.597686906237        \n",
      "precision: 0.873573183066        0.837869137232        \n",
      "accuracy:  0.993278825246        0.992529711375        \n",
      "sentences:                       2421                  \n",
      "\n",
      "TAG:       50                    \n",
      "f1:        0.875231424045        0.867879716529        \n",
      "recall:    0.843437316068        0.833431430253        \n",
      "precision: 0.90951644254         0.905298489571        \n",
      "accuracy:  0.980528996515        0.979453727698        \n",
      "sentences:                       13592                 \n",
      "\n",
      "TAG:       5b                    \n",
      "f1:        0.112315270936        0.0719794344473       \n",
      "recall:    0.0658705701079       0.0431432973806       \n",
      "precision: 0.380846325167        0.217054263566        \n",
      "accuracy:  0.995974443749        0.995698924731        \n",
      "sentences:                       649                   \n",
      "\n",
      "TAG:       MEAN                  \n",
      "f1:        0.695411447634        0.662485841082        \n",
      "recall:    0.627876890669        0.597452981071        \n",
      "precision: 0.816821947776        0.776348665353        \n",
      "accuracy:  0.992884162869        0.992246516775        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       MEAN_CONCEPT_CODES    \n",
      "f1:        0.695411447634        0.662485841082        \n",
      "recall:    0.627876890669        0.597452981071        \n",
      "precision: 0.816821947776        0.776348665353        \n",
      "accuracy:  0.992884162869        0.992246516775        \n",
      "sentences:                       13.0                  \n",
      "\n",
      "TAG:       WEIGHTED_MEAN         \n",
      "f1:        0.779677887882        0.75852981882         \n",
      "recall:    0.723296546903        0.701455644054        \n",
      "precision: 0.857171192555        0.836191732341        \n",
      "accuracy:  0.987273642036        0.98634443643         \n",
      "sentences:                       40051.0               \n",
      "\n",
      "TAG:       WEIGHTED_MEAN_CONCEPT_CODES  \n",
      "f1:        0.779677887882        0.75852981882         \n",
      "recall:    0.723296546903        0.701455644054        \n",
      "precision: 0.857171192555        0.836191732341        \n",
      "accuracy:  0.987273642036        0.98634443643         \n",
      "sentences:                       40051.0               \n",
      "\n",
      "TAG:       MICRO_F1              \n",
      "f1:        0.788630114611        0.768561580128        \n",
      "recall:    0.723296546903        0.701455644054        \n",
      "precision: 0.866938500673        0.849865384034        \n",
      "accuracy:  0.992884162869        0.992246516775        \n",
      "sentences:                       40051.0               \n",
      "\n",
      "Macro F1:  0.70999382176         0.67525297509         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training completed\")\n",
    "\n",
    "\"\"\" Persist Results to Mongo DB \"\"\"\n",
    "\n",
    "wd_algo = \"CRF_MOST_FREQUENT_TAG\"\n",
    "SUFFIX = \"_CRF_LBL_POWERSET_TEST\"\n",
    "CB_TAGGING_TD, CB_TAGGING_VD, CB_SENT_TD, CB_SENT_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX, \"CB_SENT_TD\" + SUFFIX, \"CB_SENT_VD\" + SUFFIX\n",
    "parameters = dict(config)\n",
    "#parameters[\"extractors\"] = map(lambda fn: fn.func_name, extractors)\n",
    "parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "\n",
    "wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "\n",
    "# This outputs 0's for MEAN CONCEPT CODES as we aren't including those in the outputs\n",
    "\n",
    "print processor.results_to_string(wd_td_objectid,   CB_TAGGING_TD,  wd_vd_objectid,     CB_TAGGING_VD,  \"TAGGING\")\n",
    "logger.info(\"Results Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Feature Extractor\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the        ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:the   ', 'WD1:cat   ', 'WD2:sat   ', 'WD3:on    ']\n",
      "cat        ['WD-3:START', 'WD-2:START', 'WD-1:the  ', 'WD0:cat   ', 'WD1:sat   ', 'WD2:on    ', 'WD3:the   ']\n",
      "sat        ['WD-3:START', 'WD-2:the  ', 'WD-1:cat  ', 'WD0:sat   ', 'WD1:on    ', 'WD2:the   ', 'WD3:mat   ']\n",
      "on         ['WD-3:the  ', 'WD-2:cat  ', 'WD-1:sat  ', 'WD0:on    ', 'WD1:the   ', 'WD2:mat   ', 'WD3:END   ']\n",
      "the        ['WD-3:cat  ', 'WD-2:sat  ', 'WD-1:on   ', 'WD0:the   ', 'WD1:mat   ', 'WD2:END   ', 'WD3:END   ']\n",
      "mat        ['WD-3:sat  ', 'WD-2:on   ', 'WD-1:the  ', 'WD0:mat   ', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n",
      "coral      ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:coral ', 'WD1:bleach', 'WD2:END   ', 'WD3:END   ']\n",
      "bleaching  ['WD-3:START', 'WD-2:START', 'WD-1:coral', 'WD0:bleach', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n",
      "president  ['WD-3:START', 'WD-2:START', 'WD-1:START', 'WD0:presid', 'WD1:obama ', 'WD2:approach', 'WD3:the   ']\n",
      "obama      ['WD-3:START', 'WD-2:START', 'WD-1:presid', 'WD0:obama ', 'WD1:approach', 'WD2:the   ', 'WD3:senate,']\n",
      "approached ['WD-3:START', 'WD-2:presid', 'WD-1:obama', 'WD0:approach', 'WD1:the   ', 'WD2:senate,', 'WD3:...   ']\n",
      "the        ['WD-3:presid', 'WD-2:obama', 'WD-1:approach', 'WD0:the   ', 'WD1:senate,', 'WD2:...   ', 'WD3:END   ']\n",
      "senate,    ['WD-3:obama', 'WD-2:approach', 'WD-1:the  ', 'WD0:senate,', 'WD1:...   ', 'WD2:END   ', 'WD3:END   ']\n",
      "...        ['WD-3:approach', 'WD-2:the  ', 'WD-1:senate,', 'WD0:...   ', 'WD1:END   ', 'WD2:END   ', 'WD3:END   ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comparator(kvp):\n",
    "    k,v = kvp\n",
    "    num = (k[2:]).split(\":\")[0]\n",
    "    return int(num)\n",
    "\n",
    "def test_feature_extractor_on_sentence(extractor, sent):\n",
    "    sent = sent.split(\" \")\n",
    "    for i in range(len(sent)):\n",
    "        print sent[i].ljust(10),\n",
    "        s = sorted(extractor(sent, i).items(), key = comparator)\n",
    "        print map(lambda item: str(item).ljust(10),zip(*s)[0])\n",
    "\n",
    "sent1 = \"the cat sat on the mat\"\n",
    "sent2 = \"coral bleaching\"\n",
    "sent3 = \"president obama approached the senate, ...\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent1)\n",
    "print \"\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent2)\n",
    "print \"\"\n",
    "test_feature_extractor_on_sentence(extract_features, sent3)\n",
    "print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
