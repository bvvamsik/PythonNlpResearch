{
 "metadata": {
  "name": "",
  "signature": "sha256:204f245a81f657b5627386f9fb964ef18233ce3c6ffab97e5eee4b881b74c077"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from XmlEssay import essay_xml_loader\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 3\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 5\n",
      "PCT_VALIDATION  = 0.2\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "\n",
      "essays = essay_xml_loader()\n",
      "wd_sent_freq = defaultdict(int)\n",
      "all_codes = set()\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wdsInSent = set()\n",
      "        for w, tags in sentence:\n",
      "            if w not in wdsInSent:\n",
      "                wdsInSent.add(w)\n",
      "                all_codes.update(tags)\n",
      "                wd_sent_freq[w] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Results Dir: /Users/simon.hughes/Dropbox/Phd/Results/\n",
        "Data Dir:    /Users/simon.hughes/Dropbox/Phd/Data/\n",
        "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
        "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
        "105 files found\n",
        "Processing Essay: EBA1_PRE_CB_Baldwin_9_2_essay_03464_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Baldwin_9_2_Essays_02770_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Baldwin_9_2_Essays_03466_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Baldwin_9_2_Essays_03468_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_4_essay_03808_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_4_Essay_03810_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_4_Essay_03811_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_4_Essay_03812_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_4_essays_3826_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_5_Essay_03801_F.xml\n",
        "Processing Essay: EBA1_Pre_CB_Letizia_7_5_Essay_03803_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_5_essay_03807_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Letizia_7_5_Essay_03830_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_1_essay_03490_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_1_Essays_03489_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_1_Essays_03494_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_1_Essays_03495_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_1_Essays_03496_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_2_essay_3509_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_2_essay_3515_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_2_Essays_03513_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_2_Essays_03516_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_McIntyre_6_2_Essays_03517_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_essay_03280_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_essay_3291_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03278_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03279_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03282_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03283_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03286_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03287_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03288_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03296_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03298_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_316_Essays_03299_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_essay_03235_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_essay_03248_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_essay_03253_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_essay_03256_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_essay_03258_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03237_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03240_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03244_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03245_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03247_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03252_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_317_Essays_03257_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_essay_03300_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_essay_03302_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03301_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03303_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03305_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03308_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03310_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03311_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03312_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarb_6_321_Essays_03313_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_5_essay_03339_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_5_essay_03342_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_5_essay_03348_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_5_Essays_03344_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_5_Essays_03345_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_essay_03363_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03355_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03356_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03357_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03358_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03359_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03360_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sarna_9_7_Essays_03362_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_3_7_essay_03434_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03399_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_essay_03400_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03402_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03405_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_essay_03406_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03407_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03409_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03410_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03412_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_2_Essay_03415_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_3_Essay_03418_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_3_Essay_03426_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_3_Essay_03428_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_3_essay_03429_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03377_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_Essay_03378_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03380_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03382_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_Essay_03383_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03385_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03388_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_Essay_03391_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_essay_03394_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Sjoholm_7_4_Essay_03396_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_essay_03444_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_essay_03451_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_essay_03487_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03448_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03452_F.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03453_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03456_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03458_F.xml\n",
        "Processing Essay: EBA1_PRE_CB_Stites_11_2_Essays_03459_F.xml\n",
        "Processing Essay: EBA1_Pre_CB_Stites_11_8_Essays_03442_F.xml\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "essays[0].tagged_sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[('water', set()),\n",
        " ('temps', set()),\n",
        " ('is', set()),\n",
        " ('low', set()),\n",
        " ('weaker', {'1'}),\n",
        " ('trade', {'1'}),\n",
        " ('winds', {'1'}),\n",
        " ('in', set()),\n",
        " ('1998', set()),\n",
        " ('.', set())]"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "    assert all(map(lambda win: all(map(lambda (wd, tags): len(tags) <=1, win)), windows )), \\\n",
      "            \"More than one tag per word\"        \n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "sentences = []\n",
      "\n",
      "i = 0\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size= WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        sentences.append(sentence)\n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        i += 1\n",
      "\n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract Features \"\"\"\n",
      "from WindowFeatures import extract_positional_word_features, extract_word_features\n",
      "\n",
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "idgen = IdGenerator()\n",
      "def extract_features(words):\n",
      "    #Extract features for words\n",
      "    features = extract_positional_word_features(words, MID_IX, feature_val=1)\n",
      "    word_features = extract_word_features(words, feature_val=1)\n",
      "    #Merge dicts\n",
      "    features.update(word_features)    \n",
      "    return[ (idgen.get_id(feat), val) for feat, val in features.items()]\n",
      "\n",
      "def extract_ys_by_code(tags, ysByCode):\n",
      "    for code in all_codes:\n",
      "        ysByCode[code].append(1 if code in tags else 0 )    \n",
      "\n",
      "ix2xs = {}\n",
      "ix2ys = {}\n",
      "\n",
      "for i, windows in ix2windows.items():\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    \n",
      "    ix2xs[i] = xs\n",
      "    ix2ys[i] = ysByCode\n",
      "    for window in windows:\n",
      "        # Get the words minus tags\n",
      "        words, tags = zip(*window)                \n",
      "        x = extract_features(words)\n",
      "        xs.append(x)\n",
      "        \n",
      "        #Tags for middle word (target)\n",
      "        tags4word = tags[MID_IX]\n",
      "        extract_ys_by_code(tags4word, ysByCode)\n",
      "    assert len(windows) == len(xs)\n",
      "    assert all(map(lambda (k,v): len(v) == len(xs), ysByCode.items()))\n",
      "        \n",
      "num_features = idgen.max_id() + 1\n",
      "print \"Number of features:\", num_features\n",
      "\n",
      "\"\"\" Convert to numpy \"\"\"\n",
      "for i in ix2xs.keys():\n",
      "    xs = ix2xs[i]\n",
      "    xs = np.array([matutils.sparse2full(x, num_features) for x in xs])        \n",
      "    ix2xs[i] = xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 1647\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_window(win):\n",
      "    def set2str(st):\n",
      "        return \"{\" + str(t)[5:-2] + \"}\"\n",
      "    \n",
      "    w, tg = zip(*win)\n",
      "    lens = [max(len(wd),len(set2str(t))) for wd,t in win]\n",
      "    \n",
      "    for i, wd in enumerate(w):\n",
      "        print wd.ljust(lens[i]) , \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "    for i, t in enumerate(tg):\n",
      "        print set2str(t).ljust(lens[i]), \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "def extract_features(window, feat_vals):\n",
      "    feats = [idgen.get_key(i) for i,val in enumerate(feat_vals) if val]\n",
      "    \n",
      "    wd_feats = []\n",
      "    for win in window:\n",
      "        wd, tgs = win\n",
      "        match = filter(lambda feat: \" \" + wd + \" \" in \" \" + feat + \" \", feats)\n",
      "        wd_feats.append((wd, match))\n",
      "    return wd_feats\n",
      "\n",
      "def print_features(wf):\n",
      "    w_f = wf\n",
      "    for w,ft in w_f:\n",
      "        print w.ljust(10), map(lambda s:s.ljust(10), sorted(ft, key=lambda s:len(s)))\n",
      "    print \"\"\n",
      "\n",
      "sentence_no = 101\n",
      "print \"Tagged Windows\"\n",
      "for win in ix2windows[sentence_no][:5]:\n",
      "    print_window(win)\n",
      "print \"\"    \n",
      "\n",
      "print \"Features\"\n",
      "def prn_sent_features(sentence_num):\n",
      "    win = ix2windows[sentence_num]\n",
      "    for i in range(len(win)):\n",
      "        wf = extract_features(win[i], ix2xs[sentence_num][i])\n",
      "        print_features(wf)\n",
      "\n",
      "prn_sent_features(sentence_no)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tagged Windows\n",
        "<START> | coral  | bleaching | \n",
        "{}      | {'50'} | {'50'}    | \n",
        "coral  | bleaching | rates  | \n",
        "{'50'} | {'50'}    | {'50'} | \n",
        "bleaching | rates  | vary   | \n",
        "{'50'}    | {'50'} | {'50'} | \n",
        "rates  | vary   | at | \n",
        "{'50'} | {'50'} | {} | \n",
        "vary   | at | different | \n",
        "{'50'} | {} | {}        | \n",
        "\n",
        "Features\n",
        "<START>    ['<START>   ', 'WD:-1 <START>']\n",
        "coral      ['coral     ', 'WD:0 coral']\n",
        "bleaching  ['bleaching ', 'WD:1 bleaching']\n",
        "\n",
        "coral      ['coral     ', 'WD:-1 coral']\n",
        "bleaching  ['bleaching ', 'WD:0 bleaching']\n",
        "rates      ['rates     ', 'WD:1 rates']\n",
        "\n",
        "bleaching  ['bleaching ', 'WD:-1 bleaching']\n",
        "rates      ['rates     ', 'WD:0 rates']\n",
        "vary       ['vary      ', 'WD:1 vary ']\n",
        "\n",
        "rates      ['rates     ', 'WD:-1 rates']\n",
        "vary       ['vary      ', 'WD:0 vary ']\n",
        "at         ['at        ', 'WD:1 at   ']\n",
        "\n",
        "vary       ['vary      ', 'WD:-1 vary']\n",
        "at         ['at        ', 'WD:0 at   ']\n",
        "different  ['different ', 'WD:1 different']\n",
        "\n",
        "at         ['at        ', 'WD:-1 at  ']\n",
        "different  ['different ', 'WD:0 different']\n",
        "times      ['times     ', 'WD:1 times']\n",
        "\n",
        "different  ['different ', 'WD:-1 different']\n",
        "times      ['times     ', 'WD:0 times']\n",
        "because    ['because   ', 'WD:1 because']\n",
        "\n",
        "times      ['times     ', 'WD:-1 times']\n",
        "because    ['because   ', 'WD:0 because']\n",
        "sometimes  ['sometimes ', 'WD:1 sometimes']\n",
        "\n",
        "because    ['because   ', 'WD:-1 because']\n",
        "sometimes  ['sometimes ', 'WD:0 sometimes']\n",
        "they       ['they      ', 'WD:1 they ']\n",
        "\n",
        "sometimes  ['sometimes ', 'WD:-1 sometimes']\n",
        "they       ['they      ', 'WD:0 they ']\n",
        "don        ['don       ', 'WD:1 don  ']\n",
        "\n",
        "they       ['they      ', 'WD:-1 they']\n",
        "don        ['don       ', 'WD:0 don  ']\n",
        "t          ['t         ', 'WD:1 t    ']\n",
        "\n",
        "don        ['don       ', 'WD:-1 don ']\n",
        "t          ['t         ', 'WD:0 t    ']\n",
        "have       ['have      ', 'WD:1 have ']\n",
        "\n",
        "t          ['t         ', 'WD:-1 t   ']\n",
        "have       ['have      ', 'WD:0 have ']\n",
        "what       ['what      ', 'WD:1 what ']\n",
        "\n",
        "have       ['have      ', 'WD:-1 have']\n",
        "what       ['what      ', 'WD:0 what ']\n",
        "they       ['they      ', 'WD:1 they ']\n",
        "\n",
        "what       ['what      ', 'WD:-1 what']\n",
        "they       ['they      ', 'WD:0 they ']\n",
        "need       ['need      ', 'WD:1 need ']\n",
        "\n",
        "they       ['they      ', 'WD:-1 they']\n",
        "need       ['need      ', 'WD:0 need ']\n",
        "to         ['to        ', 'WD:1 to   ']\n",
        "\n",
        "need       ['need      ', 'WD:-1 need']\n",
        "to         ['to        ', 'WD:0 to   ']\n",
        "stay       ['stay      ', 'WD:1 stay ']\n",
        "\n",
        "to         ['to        ', 'WD:-1 to  ']\n",
        "stay       ['stay      ', 'WD:0 stay ']\n",
        "alive      ['alive     ', 'WD:1 alive']\n",
        "\n",
        "stay       ['stay      ', 'WD:-1 stay']\n",
        "alive      ['alive     ', 'WD:0 alive']\n",
        ".          ['.         ', 'WD:1 .    ']\n",
        "\n",
        "alive      ['alive     ', 'WD:-1 alive']\n",
        ".          ['.         ', 'WD:0 .    ']\n",
        "<END>      ['<END>     ', 'WD:1 <END>']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split the Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_xs_ys(ixs):\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    for i in ixs:\n",
      "        xs_tmp = ix2xs[i]\n",
      "        xs.extend(xs_tmp)\n",
      "        ysByCode_tmp = ix2ys[i]\n",
      "        for code in all_codes:\n",
      "            ysByCode[code].extend(ysByCode_tmp[code])\n",
      "    return (np.array(xs), ysByCode)\n",
      "\n",
      "PCT_VALIDATION = 0.5\n",
      "num_train = int(len(sentences) * (1.0 - PCT_VALIDATION))\n",
      "\n",
      "ixtest  = ix2sents.keys()[:num_train]\n",
      "ixvalid = ix2sents.keys()[num_train:]\n",
      "\n",
      "x_t, yByCode_t = extract_xs_ys(ixtest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_codes = sorted(all_codes, key= lambda s :(len(s), s))\n",
      "all_codes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['1', '2', '3', '4', '5', '6', '7', '11', '12', '13', '50']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train\n",
      "====="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TRAIN \"\"\"\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "map_svm = lambda y: -1 if y < 0 else 1\n",
      "map_reg = lambda y: y\n",
      "\n",
      "#map_y = map_svm\n",
      "map_y = map_reg\n",
      "\n",
      "code2cls = {}\n",
      "for code in all_codes:\n",
      "    print \"Training for :\", code\n",
      "    cls = DecisionTreeClassifier()\n",
      "    #cls = LogisticRegression()\n",
      "    #cls = LinearSVC()\n",
      "    #cls = SVC()\n",
      "    #cls = RandomForestClassifier()\n",
      "    code2cls[code] = cls\n",
      "    ys = np.asarray(yByCode_t[code])\n",
      "    ys = map(map_y, ys)\n",
      "    cls.fit(x_t, ys)\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classify\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_for_code(code, ixs):\n",
      "    cls = code2cls[code]        \n",
      "    \n",
      "    act_ys  = []\n",
      "    pred_ys = []\n",
      "    for ix in ixs:\n",
      "        xs = ix2xs[ix]\n",
      "        ysByCode = ix2ys[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        act_ys.append(max(ys))\n",
      "        pred_ys.append(max(pred))\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    print \"code:      \", code\n",
      "    print \"recall:    \", r\n",
      "    print \"precision: \", p\n",
      "    print \"f1:        \", f1\n",
      "    print \"accuracy:  \", a\n",
      "    print \"sentences: \", num_codes\n",
      "    print \"\"\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "print \"\"\n",
      "print \"total sent:\", len(ixvalid)\n",
      "print \"\"\n",
      "\n",
      "metrics = []\n",
      "for c in all_codes:\n",
      "    metrics.append(test_for_code(c, ixvalid))\n",
      "\n",
      "wt_mn_prfa = weighted_mean_rpfa(metrics)\n",
      "print wt_mn_prfa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "total sent: 525\n",
        "\n",
        "code:       1\n",
        "recall:     0.772727272727\n",
        "precision:  0.809523809524\n",
        "f1:         0.790697674419\n",
        "accuracy:   0.982857142857\n",
        "sentences:  22\n",
        "\n",
        "code:       2\n",
        "recall:     0.6\n",
        "precision:  0.75\n",
        "f1:         0.666666666667\n",
        "accuracy:   0.994285714286\n",
        "sentences:  5\n",
        "\n",
        "code:       3\n",
        "recall:     0.757575757576\n",
        "precision:  0.446428571429\n",
        "f1:         0.561797752809\n",
        "accuracy:   0.925714285714\n",
        "sentences:  33\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.833333333333\n",
        "precision:  0.833333333333\n",
        "f1:         0.833333333333\n",
        "accuracy:   0.992380952381\n",
        "sentences:  12\n",
        "\n",
        "code:       5\n",
        "recall:     0.4\n",
        "precision:  0.4\n",
        "f1:         0.4\n",
        "accuracy:   0.988571428571\n",
        "sentences:  5\n",
        "\n",
        "code:       6\n",
        "recall:     0.7\n",
        "precision:  0.7\n",
        "f1:         0.7\n",
        "accuracy:   0.988571428571\n",
        "sentences:  10\n",
        "\n",
        "code:       7\n",
        "recall:     0.407407407407\n",
        "precision:  0.52380952381\n",
        "f1:         0.458333333333\n",
        "accuracy:   0.950476190476\n",
        "sentences:  27\n",
        "\n",
        "code:       11\n",
        "recall:     0.875\n",
        "precision:  1.0\n",
        "f1:         0.933333333333\n",
        "accuracy:   0.998095238095\n",
        "sentences:  8\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     1.0\n",
        "precision:  0.666666666667\n",
        "f1:         0.8\n",
        "accuracy:   0.99619047619\n",
        "sentences:  4\n",
        "\n",
        "code:       13\n",
        "recall:     0.727272727273\n",
        "precision:  0.888888888889\n",
        "f1:         0.8\n",
        "accuracy:   0.992380952381\n",
        "sentences:  11\n",
        "\n",
        "code:       50\n",
        "recall:     0.942408376963\n",
        "precision:  0.873786407767\n",
        "f1:         0.906801007557\n",
        "accuracy:   0.929523809524\n",
        "sentences:  191\n",
        "\n",
        "Recall: 0.8354, Precision: 0.7828, F1: 0.8028, Accuracy: 0.9450, Codes:   328\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "DT:     Recall: 0.8702, Precision: 0.7774, F1: 0.8155, Accuracy: 0.9473, Codes:   131  \n",
      "LR:     Recall: 0.7786, Precision: 0.8429, F1: 0.7945, Accuracy: 0.9420, Codes:   131  \n",
      "LinSVM: Recall: 0.8702, Precision: 0.7434, F1: 0.7889, Accuracy: 0.9318, Codes:   131  \n",
      "RF:     Recall: 0.7939, Precision: 0.8534, F1: 0.8075, Accuracy: 0.9435, Codes:   131"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}