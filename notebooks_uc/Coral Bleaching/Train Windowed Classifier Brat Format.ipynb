{
 "metadata": {
  "name": "",
  "signature": "sha256:92f6f9f6a21aa0443693a009634c80c06527e886d201335f665596e8275c4caf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train a Window Based Classier on the Coral Bleaching Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup:\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Imports \"\"\"\n",
      "from collections import defaultdict\n",
      "\n",
      "import numpy as np\n",
      "from gensim import matutils\n",
      "from numpy import random\n",
      "import pylibfm \n",
      "\n",
      "from Metrics import rpf1a\n",
      "from Rpfa import rpfa, weighted_mean_rpfa\n",
      "from BrattEssay import load_bratt_essays\n",
      "from WindowSplitter import split_into_windows\n",
      "\n",
      "from IdGenerator import IdGenerator\n",
      "from IterableFP import flatten\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "from stanford_parser import parser\n",
      "\n",
      "\"\"\" TODO \n",
      "    Try dependency parse features from this python dependency parser: https://github.com/syllog1sm/redshift\n",
      "\"\"\"\n",
      "\n",
      "\"\"\" Settings \"\"\"\n",
      "\"\"\" Start Script \"\"\"\n",
      "WINDOW_SIZE = 7 #7 is best\n",
      "MID_IX = int(round(WINDOW_SIZE / 2.0) - 1)\n",
      "\n",
      "MIN_SENTENCE_FREQ = 2\n",
      "MIN_FEAT_FREQ = 15     #15 best so far, and faster also\n",
      "PCT_VALIDATION = 0.25\n",
      "\n",
      "SENTENCE_START = \"<START>\"\n",
      "SENTENCE_END   = \"<END>\"\n",
      "STEM = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/jpype/_pykeywords.py:18: DeprecationWarning: the sets module is deprecated\n",
        "  import sets\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the Essays\n",
      "---------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\" Load Essays \"\"\"\n",
      "essays = load_bratt_essays(\"/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/BrattData/Merged/\")\n",
      "\n",
      "all_codes = set()\n",
      "all_words = []\n",
      "\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        for w, tags in sentence:\n",
      "            all_words.append(w)\n",
      "            all_codes.update(tags)\n",
      "                \n",
      "# Correct miss-spellings\n",
      "from SpellingCorrector import SpellingCorrector\n",
      "\n",
      "corrector = SpellingCorrector(all_words)\n",
      "corrections = defaultdict(int)\n",
      "\n",
      "for essay in essays:\n",
      "    for i, sentence in enumerate(essay.tagged_sentences):\n",
      "        for j, (w, tags) in enumerate(sentence):\n",
      "            # common error is ..n't and ..nt\n",
      "            w = w.lower()\n",
      "            if w.endswith(\"n't\") or w.endswith(\"n'\"):\n",
      "                cw = w[:-3] + \"nt\"\n",
      "            elif w.endswith(\"'s\"):\n",
      "                cw = w[:-2]\n",
      "            elif w == \"&\":\n",
      "                cw = \"and\"\n",
      "            else:\n",
      "                cw = corrector.correct(w).lower()\n",
      "            if cw != w:\n",
      "                corrections[(w,cw)] += 1\n",
      "            sentence[j] = (cw, tags)            \n",
      "            \n",
      "wd_sent_freq = defaultdict(int)\n",
      "for essay in essays:\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        wds, tag_list = zip(*sentence)\n",
      "        unique_wds = set(wds)\n",
      "        for w in unique_wds: \n",
      "            wd_sent_freq[w] += 1\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "297 files found\n",
        "297 essays processed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\" \".join(zip(*essays[0].tagged_sentences[0])[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'the coral bleaching is a different type they are bleached and coral bleaching .'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "cor_srtd = sort_by_value(corrections, reverse = True)\n",
      "cor_srtd[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[((\"it's\", 'it'), 52),\n",
        " (('zox', 'zo'), 41),\n",
        " ((\"don't\", 'dont'), 31),\n",
        " ((\"that's\", 'that'), 29),\n",
        " (('algea', 'algae'), 26),\n",
        " ((\"world's\", 'world'), 20),\n",
        " (('&', 'and'), 17),\n",
        " ((\"can't\", 'cant'), 14),\n",
        " (('bleaches', 'bleached'), 13),\n",
        " (('cloral', 'coral'), 11),\n",
        " ((\"they're\", 'there'), 11),\n",
        " ((\"coral's\", 'coral'), 11),\n",
        " ((\"isn't\", 'isnt'), 11),\n",
        " (('tempeture', 'temperature'), 9),\n",
        " ((\"won't\", 'wont'), 9),\n",
        " (('alge', 'algae'), 9),\n",
        " (('tiems', 'times'), 8),\n",
        " ((\"doesn't\", 'doesnt'), 8),\n",
        " (('tempature', 'temperature'), 8),\n",
        " (('varys', 'vary'), 7)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Single char words \"\"\"\n",
      "wds = [(w,f) for w,f in wd_sent_freq.items() if len(w.strip()) == 1 and not w[0].isalpha()]\n",
      "print \"\\n\".join(map(str,sorted(wds, key = lambda (w,f): -f)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('.', 2623)\n",
        "(',', 681)\n",
        "('-', 105)\n",
        "('\"', 105)\n",
        "('?', 90)\n",
        "('(', 43)\n",
        "(')', 42)\n",
        "('3', 40)\n",
        "('%', 36)\n",
        "('\\xc2', 28)\n",
        "('\\xb0', 28)\n",
        "('\\x80', 17)\n",
        "('\\xe2', 17)\n",
        "('1', 17)\n",
        "('\\\\', 16)\n",
        "('5', 15)\n",
        "(';', 12)\n",
        "(':', 9)\n",
        "('\\x99', 8)\n",
        "('+', 7)\n",
        "('2', 7)\n",
        "('!', 7)\n",
        "('\\x93', 7)\n",
        "(\"'\", 6)\n",
        "('0', 4)\n",
        "('6', 4)\n",
        "('8', 2)\n",
        "('9', 2)\n",
        "('\\xa6', 2)\n",
        "('=', 2)\n",
        "('7', 1)\n",
        "('4', 1)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create Windows\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Creating Windows \"\"\"\n",
      "def filter2min_word_freq(sentence):\n",
      "    return filter(lambda (w, tags4word): wd_sent_freq[w] >= MIN_SENTENCE_FREQ, sentence)\n",
      "\n",
      "VALID_CHARS = {\".\", \"?\", \"!\", \"=\", \"/\", \":\", \";\", \"&\", \"+\",  \"-\", \"=\",  \"%\", \"'\", \",\", \"\\\\\", \"(\", \")\", \"\\\"\"}\n",
      "\"\"\" Remove bad chars (see above - e.g. '\\x93') \"\"\"\n",
      "removed = set()\n",
      "def valid_wd(wd):\n",
      "    wd = wd.strip()\n",
      "    if len(wd) != 1:\n",
      "        return True\n",
      "    if wd in removed:\n",
      "        return False\n",
      "    if wd.isalpha() or wd.isdigit() or wd in VALID_CHARS:\n",
      "        return True\n",
      "    removed.add(wd)\n",
      "    return False\n",
      "    \n",
      "def filterout_punctuation(sentence):\n",
      "    return filter(lambda (w, tags4word): valid_wd(w), sentence)\n",
      "\n",
      "def bookend(sentence):\n",
      "    for i in range(MID_IX):\n",
      "        modified_sentence.insert(0, (SENTENCE_START,    set()))\n",
      "        modified_sentence.append(   (SENTENCE_END,      set()))\n",
      "\n",
      "def assert_windows_correct(windows):\n",
      "    lens = map(len, windows)\n",
      "    assert min(lens) == max(lens) == WINDOW_SIZE, \\\n",
      "            \"Windows are not all the correct size\"\n",
      "   \n",
      "ix2windows = {}\n",
      "ix2sents = {}\n",
      "# maps sentence index to essay\n",
      "ix2essays = {}\n",
      "sentences = []\n",
      "tokenized_sentences = []\n",
      "\n",
      "i = 0\n",
      "for e_ix, essay in enumerate(essays):\n",
      "    for sentence in essay.tagged_sentences:\n",
      "        \n",
      "        modified_sentence = filter2min_word_freq(sentence)\n",
      "        modified_sentence = filterout_punctuation(modified_sentence)\n",
      "        if len(modified_sentence) == 0:\n",
      "            continue\n",
      "        \n",
      "        bookend(modified_sentence)        \n",
      "        new_windows = split_into_windows(modified_sentence, window_size= WINDOW_SIZE)        \n",
      "        assert_windows_correct(new_windows)       \n",
      "        \n",
      "        # tagged words\n",
      "        sentences.append(sentence)\n",
      "        # words only\n",
      "        tokenized_sentences.append(zip(*sentence)[0])\n",
      "        \n",
      "        ix2windows[i] = new_windows\n",
      "        ix2sents[i] = modified_sentence\n",
      "        ix2essays[i] = e_ix\n",
      "        i += 1\n",
      "        \n",
      "\"\"\" Assert tags set correctly \"\"\"\n",
      "print \"Windows loaded correctly!\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Windows loaded correctly!\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentence Level Features\n",
      "-----------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from gensim import matutils\n",
      "\n",
      "def filter_words(wd):\n",
      "    return wd.isalnum()\n",
      "\n",
      "docs = map(lambda sen : \" \".join(filter(filter_words,sen)),tokenized_sentences)\n",
      "\n",
      "#Vectorize\n",
      "vectorizer = TfidfVectorizer(use_idf = False, ngram_range = (1, 1), min_df = 5, binary=True)\n",
      "sentence_vectors = vectorizer.fit_transform(docs)\n",
      "sentence_vectors = sentence_vectors.todense()\n",
      "sentence_vectors = map(lambda s: s.tolist()[0], sentence_vectors)\n",
      "ix2vector = dict(enumerate(sentence_vectors))\n",
      "print len(ix2vector[0]), \"features\"\n",
      "#ix2vector[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "662 features\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Removed Characters\n",
      "------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"\\n\".join(sorted(removed))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract Features\n",
      "----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Extract Features \"\"\"\n",
      "from WindowFeatures import extract_positional_word_features, extract_word_features\n",
      "from NgramGenerator import compute_ngrams\n",
      "\n",
      "def extract_skip_b4_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[:mid_ix]:\n",
      "        feats[\"_BEFORE: \" + wd + \"|\" + target] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_skip_after_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for wd in window[mid_ix+1:]:\n",
      "        feats[\"_AFTER: \" + target + \"|\" + wd] = feature_val\n",
      "    return feats\n",
      "\n",
      "def extract_positional_bigram_features(window, mid_ix, feature_val = 1):\n",
      "    bi_grams = compute_ngrams(window, max_len = 2, min_len = 2)\n",
      "    d = {}\n",
      "    for i, bi_gram in enumerate(bi_grams):\n",
      "        d[\"BI\" + \":\" + str(-mid_ix + i) + \" \" + bi_gram[0] + \" | \" + bi_gram[1]] = feature_val\n",
      "    return d\n",
      "\n",
      "def extract_positional_skip_word_features(window, mid_ix, feature_val = 1):\n",
      "    feats = {}\n",
      "    target = window[mid_ix]\n",
      "    for i, wd in enumerate(window):\n",
      "        if i == mid_ix:\n",
      "            continue\n",
      "        a,b = wd,target\n",
      "        if i > mid_ix:\n",
      "            a,b = b,a\n",
      "        feats[\"SKIP:\" + str(-mid_ix + i) + \" \" + a + \" | \" + b] = feature_val\n",
      "    return feats\n",
      "\n",
      "\"\"\" TODO:\n",
      "        Extract features for numbers\n",
      "        Extract features for years\n",
      "        Extract features that are temperatures (look for degree\\degrees in subsequent words, along with C or F)\n",
      "\"\"\"\n",
      "idgen = IdGenerator()\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def extract_features(words):\n",
      "    \n",
      "    if STEM:\n",
      "        words = [stemmer.stem(w) for w in words]\n",
      "    #Extract features for words\n",
      "    \n",
      "    \"\"\" Try only middle word \"\"\"\n",
      "    features = {}\n",
      "    ###\n",
      "    pos_features  = extract_positional_word_features(words, MID_IX, feature_val=1)    \n",
      "    word_features = extract_word_features(words, feature_val=1)\n",
      "    \n",
      "    #DO NOT HELP\n",
      "    #b4_features    = extract_skip_b4_word_features(words, MID_IX, feature_val=1)\n",
      "    #after_features = extract_skip_after_word_features(words, MID_IX, feature_val=1)\n",
      "    #pos_skip_grams = extract_positional_skip_word_features(words, MID_IX,  feature_val = 1)\n",
      "    pos_bi_grams = extract_positional_bigram_features(words, MID_IX, feature_val = 1)\n",
      "\n",
      "    features.update(pos_features)\n",
      "    features.update(word_features)\n",
      "    #features.update(b4_features)\n",
      "    #features.update(after_features)\n",
      "    #features.update(pos_skip_grams)\n",
      "    features.update(pos_bi_grams)\n",
      "    return features.items()\n",
      "\n",
      "def extract_ys_by_code(tags, ysByCode):\n",
      "    for code in all_codes:\n",
      "        ysByCode[code].append(1 if code in tags else 0 )    \n",
      "\n",
      "ix2ys = {}\n",
      "ix2feats = {}\n",
      "feat_counts = defaultdict(int)\n",
      "def tally_features(feats):\n",
      "    for k,v in feats:\n",
      "        feat_counts[k] += 1\n",
      "\n",
      "for i, windows in ix2windows.items():\n",
      "    feats = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    \n",
      "    ix2feats[i] = feats\n",
      "    ix2ys[i] = ysByCode\n",
      "    for window in windows:\n",
      "        # Get the words minus tags\n",
      "        words, tags = zip(*window)                \n",
      "        feat = extract_features(words)\n",
      "        tally_features(feat)\n",
      "        feats.append(feat)\n",
      "        \n",
      "        #Tags for middle word (target)\n",
      "        tags4word = tags[MID_IX]\n",
      "        extract_ys_by_code(tags4word, ysByCode)\n",
      "    assert len(windows) == len(feats)\n",
      "    assert all(map(lambda (k,v): len(v) == len(feats), ysByCode.items()))\n",
      "        \n",
      "\"\"\" Convert sparse dictionary features to sparse arrays \"\"\"\n",
      "ix2xs = {}\n",
      "for i, feature_lists in ix2feats.items():\n",
      "    xs = []\n",
      "    ix2xs[i] = xs\n",
      "    for feats in feature_lists:\n",
      "        x = [(idgen.get_id(f),v) \n",
      "                 for f,v in feats \n",
      "                 if feat_counts[f] >= MIN_FEAT_FREQ or f.startswith(\"WD:0\" )]\n",
      "        xs.append(x)        \n",
      "\n",
      "num_features = idgen.max_id() + 1\n",
      "print \"Number of features:\", num_features\n",
      "\n",
      "\"\"\" Convert to dense numpy arrays \"\"\"\n",
      "for i in ix2xs.keys():\n",
      "    xs = ix2xs[i]\n",
      "    xs = np.array([matutils.sparse2full(x, num_features) for x in xs])        \n",
      "    ix2xs[i] = xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 6004\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DictionaryHelper import *\n",
      "\n",
      "def count_above(ft_counts, threshold):\n",
      "    above = [ v for k,v in ft_counts.items() if v >= threshold]\n",
      "    return (sum(above), len(above))\n",
      "\n",
      "total_all, cnt_all = count_above(feat_counts, 0)\n",
      "total_above, cnt_above = count_above(feat_counts, MIN_FEAT_FREQ)\n",
      "\n",
      "print \"Counts\"\n",
      "print \"all:     \", cnt_all\n",
      "print \"above:   \", cnt_above\n",
      "print \"% above: \", str(100.0 * cnt_above / float(cnt_all))+ \"%\"\n",
      "\n",
      "print \"\\nTotal Frequency\"\n",
      "print \"all:     \", total_all\n",
      "print \"above:   \", total_above\n",
      "print \"% above: \", str(100.0 * total_above / float(total_all))+ \"%\"\n",
      "\n",
      "#srtd = sort_by_value(feat_counts, reverse = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counts\n",
        "all:      68809\n",
        "above:    5486\n",
        "% above:  7.97279425657%\n",
        "\n",
        "Total Frequency\n",
        "all:      846975\n",
        "above:    695992\n",
        "% above:  82.1738540099%\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_window(win):\n",
      "    def set2str(st):\n",
      "        return \"{\" + str(t)[5:-2] + \"}\"\n",
      "    \n",
      "    w, tg = zip(*win)\n",
      "    lens = [max(len(wd),len(set2str(t))) for wd,t in win]\n",
      "    \n",
      "    for i, wd in enumerate(w):\n",
      "        print wd.ljust(lens[i]) , \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "    for i, t in enumerate(tg):\n",
      "        print set2str(t).ljust(lens[i]), \"|\",\n",
      "    print \"\"\n",
      "    \n",
      "def extract_features(window, feat_vals):\n",
      "    feats = [idgen.get_key(i) for i,val in enumerate(feat_vals) if val]\n",
      "    \n",
      "    wd_feats = []\n",
      "    for win in window:\n",
      "        wd, tgs = win\n",
      "        if STEM:\n",
      "            match = filter(lambda feat: \" \" + stemmer.stem(wd) + \" \" in \" \" + feat + \" \", feats)\n",
      "        else:\n",
      "            match = filter(lambda feat: \" \" + wd + \" \" in \" \" + feat + \" \", feats)\n",
      "        wd_feats.append((wd, match))\n",
      "    return wd_feats\n",
      "\n",
      "def print_features(wf):\n",
      "    w_f = wf\n",
      "    for w,ft in w_f:\n",
      "        print w.ljust(10), map(lambda s:s.ljust(10), sorted(ft, key=lambda s:(len(s),s)))\n",
      "    print \"\"\n",
      "\n",
      "#uncomment to verify code output\n",
      "\n",
      "sentence_no = 101\n",
      "print \"Tagged Windows\"\n",
      "for win in ix2windows[sentence_no][:5]:\n",
      "    print_window(win)\n",
      "print \"\"    \n",
      "\n",
      "print \"Features\"\n",
      "def prn_sent_features(sentence_num):\n",
      "    win = ix2windows[sentence_num]\n",
      "    for i in range(len(win)):\n",
      "        print \"[%s]\" % str(i)\n",
      "        wf = extract_features(win[i], ix2xs[sentence_num][i])\n",
      "        print_features(wf)\n",
      "\n",
      "prn_sent_features(sentence_no)\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tagged Windows\n",
        "<START> | <START> | <START> | so | they | need | to | \n",
        "{}      | {}      | {}      | {} | {}   | {}   | {} | \n",
        "<START> | <START> | so | they | need | to | have | \n",
        "{}      | {}      | {} | {}   | {}   | {} | {}   | \n",
        "<START> | so | they | need | to | have | light | \n",
        "{}      | {} | {}   | {}   | {} | {}   | {}    | \n",
        "so | they | need | to | have | light | to | \n",
        "{} | {}   | {}   | {} | {}   | {}    | {} | \n",
        "they | need | to | have | light | to | eat | \n",
        "{}   | {}   | {} | {}   | {}    | {} | {}  | \n",
        "\n",
        "Features\n",
        "[0]\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-1 <START>', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-1 <START> | so', u'BI:-2 <START> | <START>', u'BI:-3 <START> | <START>']\n",
        "so         [u'so        ', u'WD:0 so   ', u'BI:-1 <START> | so']\n",
        "they       [u'they      ', u'WD:1 they ', u'BI:1 they | need']\n",
        "need       [u'need      ', u'WD:2 need ', u'BI:2 need | to', u'BI:1 they | need']\n",
        "to         [u'to        ', u'WD:3 to   ', u'BI:2 need | to']\n",
        "\n",
        "[1]\n",
        "<START>    [u'<START>   ', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-2 <START> | so', u'BI:-3 <START> | <START>']\n",
        "<START>    [u'<START>   ', u'WD:-2 <START>', u'WD:-3 <START>', u'BI:-2 <START> | so', u'BI:-3 <START> | <START>']\n",
        "so         [u'so        ', u'WD:-1 so  ', u'BI:-2 <START> | so']\n",
        "they       [u'they      ', u'WD:0 they ', u'BI:0 they | need']\n",
        "need       [u'need      ', u'WD:1 need ', u'BI:1 need | to', u'BI:0 they | need']\n",
        "to         [u'to        ', u'WD:2 to   ', u'BI:1 need | to']\n",
        "have       [u'have      ', u'WD:3 have ']\n",
        "\n",
        "[2]\n",
        "<START>    [u'<START>   ', u'WD:-3 <START>', u'BI:-3 <START> | so']\n",
        "so         [u'so        ', u'WD:-2 so  ', u'BI:-3 <START> | so']\n",
        "they       [u'they      ', u'WD:-1 they', u'BI:-1 they | need']\n",
        "need       [u'need      ', u'WD:0 need ', u'BI:0 need | to', u'BI:-1 they | need']\n",
        "to         [u'to        ', u'WD:1 to   ', u'BI:0 need | to']\n",
        "have       [u'have      ', u'WD:2 have ']\n",
        "light      [u'light     ', u'WD:3 light']\n",
        "\n",
        "[3]\n",
        "so         [u'so        ', u'WD:-3 so  ']\n",
        "they       [u'they      ', u'WD:-2 they', u'BI:-2 they | need']\n",
        "need       [u'need      ', u'WD:-1 need', u'BI:-1 need | to', u'BI:-2 they | need']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:3 to   ', u'BI:-1 need | to']\n",
        "have       [u'have      ', u'WD:1 have ']\n",
        "light      [u'light     ', u'WD:2 light']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:3 to   ', u'BI:-1 need | to']\n",
        "\n",
        "[4]\n",
        "they       [u'they      ', u'WD:-3 they', u'BI:-3 they | need']\n",
        "need       [u'need      ', u'WD:-2 need', u'BI:-2 need | to', u'BI:-3 they | need']\n",
        "to         [u'to        ', u'WD:2 to   ', u'WD:-1 to  ', u'BI:-2 need | to']\n",
        "have       [u'have      ', u'WD:0 have ']\n",
        "light      [u'light     ', u'WD:1 light']\n",
        "to         [u'to        ', u'WD:2 to   ', u'WD:-1 to  ', u'BI:-2 need | to']\n",
        "eat        [u'eat       ', u'WD:3 eat  ']\n",
        "\n",
        "[5]\n",
        "need       [u'need      ', u'WD:-3 need', u'BI:-3 need | to']\n",
        "to         [u'to        ', u'WD:1 to   ', u'WD:-2 to  ', u'BI:-3 need | to']\n",
        "have       [u'have      ', u'WD:-1 have']\n",
        "light      [u'light     ', u'WD:0 light']\n",
        "to         [u'to        ', u'WD:1 to   ', u'WD:-2 to  ', u'BI:-3 need | to']\n",
        "eat        [u'eat       ', u'WD:2 eat  ']\n",
        ".          [u'.         ', u'WD:3 .    ']\n",
        "\n",
        "[6]\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:-3 to  ']\n",
        "have       [u'have      ', u'WD:-2 have']\n",
        "light      [u'light     ', u'WD:-1 light']\n",
        "to         [u'to        ', u'WD:0 to   ', u'WD:-3 to  ']\n",
        "eat        [u'eat       ', u'WD:1 eat  ']\n",
        ".          [u'.         ', u'WD:2 .    ', u'BI:2 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:3 <END>', u'BI:2 . | <END>']\n",
        "\n",
        "[7]\n",
        "have       [u'have      ', u'WD:-3 have']\n",
        "light      [u'light     ', u'WD:-2 light']\n",
        "to         [u'to        ', u'WD:-1 to  ']\n",
        "eat        [u'eat       ', u'WD:0 eat  ']\n",
        ".          [u'.         ', u'WD:1 .    ', u'BI:1 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:2 <END>', u'WD:3 <END>', u'BI:1 . | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:2 <END>', u'WD:3 <END>', u'BI:1 . | <END>', u'BI:2 <END> | <END>']\n",
        "\n",
        "[8]\n",
        "light      [u'light     ', u'WD:-3 light']\n",
        "to         [u'to        ', u'WD:-2 to  ']\n",
        "eat        [u'eat       ', u'WD:-1 eat ']\n",
        ".          [u'.         ', u'WD:0 .    ', u'BI:0 . | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "<END>      [u'<END>     ', u'WD:1 <END>', u'WD:2 <END>', u'WD:3 <END>', u'BI:0 . | <END>', u'BI:1 <END> | <END>', u'BI:2 <END> | <END>']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split the Data\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_xs_ys(ixs, ixTOxs, ixTOys, codes):\n",
      "    xs = []\n",
      "    ysByCode = defaultdict(list)\n",
      "    for i in ixs:\n",
      "        xs_tmp = ixTOxs[i]\n",
      "        xs.extend(xs_tmp)\n",
      "        ysByCode_tmp = ixTOys[i]\n",
      "        for code in codes:\n",
      "            ysByCode[code].extend(ysByCode_tmp[code])\n",
      "    return (np.array(xs), ysByCode)\n",
      "\n",
      "num_train = int(len(sentences) * (1.0 - PCT_VALIDATION))\n",
      "\n",
      "ixtest  = ix2sents.keys()[:num_train]\n",
      "ixvalid = ix2sents.keys()[num_train:]\n",
      "\n",
      "# Extract flattened windows for training data as xs and ys\n",
      "x_t, yByCode_t = extract_xs_ys(ixtest,ix2xs, ix2ys, all_codes)\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"#Sentences : \" + str(len(sentences))\n",
      "print \"\"\n",
      "\n",
      "all_codes = sorted(all_codes, key= lambda s :(len(s), s))\n",
      "for code in all_codes:\n",
      "    print code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#Sentences : 2779\n",
        "\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "11\n",
        "12\n",
        "13\n",
        "14\n",
        "50\n",
        "5b\n",
        "it\n",
        "other\n",
        "Causer\n",
        "Result\n",
        "Anaphor\n",
        "explicit\n",
        "rhetorical\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train\n",
      "====="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class factMach(object):\n",
      "    def __init__(self):\n",
      "        self.fm = pylibfm.FM(num_factors=50, num_iter=10, task=\"regression\", shuffle_training=True)\n",
      "        \n",
      "    def __to_sparse_(self, xs):\n",
      "        from scipy import sparse\n",
      "        return sparse.csr_matrix(np.asarray(xs, dtype=np.double))\n",
      "    \n",
      "    def fit(self, xs, ys):\n",
      "        return self.fm.fit(self.__to_sparse_(xs), np.asarray(ys, dtype=np.double))\n",
      "    \n",
      "    def predict(self, xs):\n",
      "        return np.round(self.fm.predict(self.__to_sparse_(xs)))\n",
      "        #return self.fm.predict(self.__to_sparse_(xs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" TRAIN \"\"\"\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import RidgeClassifier\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "map_svm = lambda y: -1 if y < 0 else 1\n",
      "map_reg = lambda y: y\n",
      "\n",
      "#map_y = map_svm\n",
      "map_y = map_reg\n",
      "\n",
      "def make_cls():\n",
      "    #cls = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, criterion=\"entropy\")\n",
      "    #cls = DecisionTreeClassifier(criterion=\"entropy\")\n",
      "    cls = LogisticRegression()\n",
      "    #cls = RidgeClassifier()\n",
      "    #cls = KNeighborsClassifier(n_neighbors=5) # TOO SLOW!\n",
      "    #cls = LDA()\n",
      "    #cls = SVC()\n",
      "    #cls = RandomForestClassifier(n_jobs=-1, max_depth=100, n_estimators=10)\n",
      "    #cls = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, max_depth=1)\n",
      "    #cls = Ridge()\n",
      "    #cls = LinearSVC()\n",
      "    #cls = factMach()\n",
      "    return cls\n",
      "\n",
      "print \"Starting Training\"\n",
      "reg_codes = [c for c in all_codes if c.isdigit() or c == \"explicit\"]\n",
      "\n",
      "def train(codes, xs, yByCode, fn_create_cls):\n",
      "    code2classifier = {}\n",
      "    for code in codes:\n",
      "        print \"Training for :\", code   \n",
      "        cls = fn_create_cls()\n",
      "        code2classifier[code] = cls\n",
      "        ys = np.asarray(yByCode[code])    \n",
      "        ys = map(map_y, ys)\n",
      "        cls.fit(xs, ys)\n",
      "    return code2classifier\n",
      "\n",
      "code2cls = train(all_codes, x_t, yByCode_t, make_cls)\n",
      "print make_cls()\n",
      "print \"Done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting Training\n",
        "Training for : 1\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classify\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Get sentence level classification performance \"\"\"\n",
      "def test_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    cls = codeToClassifier[code]\n",
      "    \n",
      "    try:\n",
      "        cls.n_jobs = 1\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    act_ys  = []\n",
      "    pred_ys = []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys.append(max(ys))\n",
      "        pred_ys.append(max(pred))\n",
      "    \n",
      "    num_codes = len([y for y in act_ys if y == 1])\n",
      "    r,p,f1,a = rpf1a(act_ys, pred_ys)\n",
      "    print \"code:      \", code\n",
      "    print \"recall:    \", r\n",
      "    print \"precision: \", p\n",
      "    print \"f1:        \", f1\n",
      "    print \"accuracy:  \", a\n",
      "    print \"sentences: \", num_codes\n",
      "    print \"\"\n",
      "    return rpfa(r,p,f1,a,num_codes)\n",
      "\n",
      "print \"\"\n",
      "print \"total sent:\", len(ixvalid)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "total sent: 695\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training Data Performance\n",
      "-------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(codes, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "    td_metrics = []\n",
      "    for c in codes:\n",
      "        cls = codeToClassifier[c]\n",
      "        td_metrics.append(test_for_code(c, ixs, ixToXs, ixToYs, codeToClassifier))\n",
      "    td_wt_mn_prfa = weighted_mean_rpfa(td_metrics)\n",
      "    print type(cls), td_wt_mn_prfa\n",
      "    return td_wt_mn_prfa\n",
      "\n",
      "print \"Training Data: \"\n",
      "metrics = test(all_codes, ixtest, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.973094170404\n",
        "precision:  0.935344827586\n",
        "f1:         0.953846153846\n",
        "accuracy:   0.989923224568\n",
        "sentences:  223\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.826923076923\n",
        "precision:  0.843137254902\n",
        "f1:         0.834951456311\n",
        "accuracy:   0.991842610365\n",
        "sentences:  52\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.957746478873\n",
        "precision:  0.888888888889\n",
        "f1:         0.922033898305\n",
        "accuracy:   0.97792706334\n",
        "sentences:  284\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.958333333333\n",
        "precision:  0.938775510204\n",
        "f1:         0.948453608247\n",
        "accuracy:   0.997600767754\n",
        "sentences:  48\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.9\n",
        "precision:  0.849056603774\n",
        "f1:         0.873786407767\n",
        "accuracy:   0.987523992322\n",
        "sentences:  100\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     0.851851851852\n",
        "precision:  1.0\n",
        "f1:         0.92\n",
        "accuracy:   0.998080614203\n",
        "sentences:  27\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.914728682171\n",
        "precision:  0.900763358779\n",
        "f1:         0.907692307692\n",
        "accuracy:   0.988483685221\n",
        "sentences:  129\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.843137254902\n",
        "precision:  1.0\n",
        "f1:         0.914893617021\n",
        "accuracy:   0.996161228407\n",
        "sentences:  51\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  24\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.920634920635\n",
        "precision:  0.950819672131\n",
        "f1:         0.935483870968\n",
        "accuracy:   0.996161228407\n",
        "sentences:  63\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     1.0\n",
        "precision:  0.785714285714\n",
        "f1:         0.88\n",
        "accuracy:   0.997120921305\n",
        "sentences:  22\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.966580976864\n",
        "precision:  0.967824967825\n",
        "f1:         0.967202572347\n",
        "accuracy:   0.975527831094\n",
        "sentences:  778\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5b\n",
        "recall:     0.833333333333\n",
        "precision:  1.0\n",
        "f1:         0.909090909091\n",
        "accuracy:   0.999040307102\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " it\n",
        "recall:     1.0\n",
        "precision:  1.0\n",
        "f1:         1.0\n",
        "accuracy:   1.0\n",
        "sentences:  1\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " other\n",
        "recall:     0.0879120879121\n",
        "precision:  1.0\n",
        "f1:         0.161616161616\n",
        "accuracy:   0.920345489443\n",
        "sentences:  182\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Causer\n",
        "recall:     0.774418604651\n",
        "precision:  0.814180929095\n",
        "f1:         0.793802145411\n",
        "accuracy:   0.916986564299\n",
        "sentences:  430\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Result\n",
        "recall:     0.651480637813\n",
        "precision:  0.824207492795\n",
        "f1:         0.727735368957\n",
        "accuracy:   0.897312859885\n",
        "sentences:  439\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Anaphor\n",
        "recall:     0.205607476636\n",
        "precision:  1.0\n",
        "f1:         0.341085271318\n",
        "accuracy:   0.959213051823\n",
        "sentences:  107\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.628635346756\n",
        "precision:  0.912337662338\n",
        "f1:         0.744370860927\n",
        "accuracy:   0.907389635317\n",
        "sentences:  447\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rhetorical\n",
        "recall:     0.618055555556\n",
        "precision:  0.855769230769\n",
        "f1:         0.717741935484\n",
        "accuracy:   0.96641074856\n",
        "sentences:  144\n",
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.7707, Precision: 0.9060, F1: 0.8052, Accuracy: 0.9502, Codes:  3563\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validation Data Performance\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data: \"\n",
      "test(reg_codes, ixvalid, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "recall:     0.89247311828\n",
        "precision:  0.790476190476\n",
        "f1:         0.838383838384\n",
        "accuracy:   0.953956834532\n",
        "sentences:  93\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "recall:     0.5\n",
        "precision:  0.882352941176\n",
        "f1:         0.63829787234\n",
        "accuracy:   0.975539568345\n",
        "sentences:  30\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "recall:     0.806451612903\n",
        "precision:  0.729927007299\n",
        "f1:         0.766283524904\n",
        "accuracy:   0.912230215827\n",
        "sentences:  124\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "recall:     0.913043478261\n",
        "precision:  0.875\n",
        "f1:         0.893617021277\n",
        "accuracy:   0.992805755396\n",
        "sentences:  23\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "recall:     0.681818181818\n",
        "precision:  0.697674418605\n",
        "f1:         0.689655172414\n",
        "accuracy:   0.961151079137\n",
        "sentences:  44\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n",
        "recall:     1.0\n",
        "precision:  0.666666666667\n",
        "f1:         0.8\n",
        "accuracy:   0.995683453237\n",
        "sentences:  6\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7\n",
        "recall:     0.707692307692\n",
        "precision:  0.754098360656\n",
        "f1:         0.730158730159\n",
        "accuracy:   0.951079136691\n",
        "sentences:  65\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11\n",
        "recall:     0.733333333333\n",
        "precision:  1.0\n",
        "f1:         0.846153846154\n",
        "accuracy:   0.994244604317\n",
        "sentences:  15\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12\n",
        "recall:     0.75\n",
        "precision:  1.0\n",
        "f1:         0.857142857143\n",
        "accuracy:   0.995683453237\n",
        "sentences:  12\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13\n",
        "recall:     0.769230769231\n",
        "precision:  0.769230769231\n",
        "f1:         0.769230769231\n",
        "accuracy:   0.98273381295\n",
        "sentences:  26\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14\n",
        "recall:     0.866666666667\n",
        "precision:  0.8125\n",
        "f1:         0.838709677419\n",
        "accuracy:   0.992805755396\n",
        "sentences:  15\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50\n",
        "recall:     0.957746478873\n",
        "precision:  0.947735191638\n",
        "f1:         0.952714535902\n",
        "accuracy:   0.961151079137\n",
        "sentences:  284\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.524390243902\n",
        "precision:  0.614285714286\n",
        "f1:         0.565789473684\n",
        "accuracy:   0.810071942446\n",
        "sentences:  164\n",
        "\n",
        "<class 'sklearn.linear_model.logistic.LogisticRegression'> Recall: 0.7902, Precision: 0.8029, F1: 0.7927, Accuracy: 0.9291, Codes:   901\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Recall: 0.7902, Precision: 0.8029, F1: 0.7927, Accuracy: 0.9291, Codes:   901"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Window - 5, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LDA:      Recall: 0.8927, Precision: 0.6974, F1: 0.7675, Accuracy: 0.8823, Codes:   792\n",
      "LinSVC:   Recall: 0.7601, Precision: 0.7655, F1: 0.7563, Accuracy: 0.9048, Codes:   792\n",
      "DT:       Recall: 0.7462, Precision: 0.6890, F1: 0.7063, Accuracy: 0.8766, Codes:   792\n",
      "RidgeClf: Recall: 0.6843, Precision: 0.8359, F1: 0.6874, Accuracy: 0.9036, Codes:   795\n",
      "\n",
      "LinSVC\n",
      "Window - 7, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "LinSVC: Recall: 0.7937, Precision: 0.7522, F1: 0.7677, Accuracy: 0.9037, Codes:   795\n",
      "\n",
      "Window - 9, Min sent freq - 6, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.7887, Precision: 0.7338, F1: 0.7555, Accuracy: 0.8929, Codes:   795\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "Recall: 0.8058, Precision: 0.7559, F1: 0.7756, Accuracy: 0.9046, Codes:   798\n",
      "\n",
      "-- Starting adding new features, messing with feature freq\n",
      "Window - 5, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8008, Precision: 0.7369, F1: 0.7625, Accuracy: 0.9008, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 10\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8120, Precision: 0.7535, F1: 0.7779, Accuracy: 0.9066, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - ***15***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8145, Precision: 0.7460, F1: 0.7744, Accuracy: 0.9040, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 20\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7982, Precision: 0.7496, F1: 0.7685, Accuracy: 0.9025, Codes:   798\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 25\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.8070, Precision: 0.7485, F1: 0.7732, Accuracy: 0.9047, Codes:   798\n",
      "\n",
      "\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 5 ***\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "    + Positional BI-GRAMS ****\n",
      "Recall: 0.8145, Precision: 0.7642, F1: 0.7831, Accuracy: 0.9070, Codes:   798\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7306, Precision: 0.8298, F1: 0.7672, Accuracy: 0.9150, Codes:   798\n",
      "\n",
      "Window - 9, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7206, Precision: 0.8232, F1: 0.7580, Accuracy: 0.9121, Codes:   798\n",
      "\n",
      "RF\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit')\n",
      "Recall: 0.6028, Precision: 0.8071, F1: 0.6570, Accuracy: 0.8978, Codes:   798\n",
      "\n",
      "*** - New Data + Sentence Segmentation Fixes\n",
      "Logistic Regression\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 5\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7902, Precision: 0.8029, F1: 0.7927, Accuracy: 0.9291, Codes:   901\n",
      "***\n",
      "Logistic Regression\n",
      "Window - 7, Min sent freq - 2, 'reg codes (numeric and 'explicit') Min Feature Freq - 15\n",
      "    + ALL WDS IN WINDOW (ingoring posn)\n",
      "Recall: 0.7869, Precision: 0.8050, F1: 0.7913, Accuracy: 0.9294, Codes:   901\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train Stacked Classifier\n",
      "========================"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def codes_2_features(set_codes):\n",
      "    return [1 if code in set_codes else 0 for code in all_codes]\n",
      "\n",
      "print codes_2_features({\"50\", \"5\"})\n",
      "print codes_2_features({})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Create Data Using Previous Classifier \"\"\"\n",
      "ix2newxs = {}\n",
      "ix2newys = {} #dict to dict to list\n",
      "\n",
      "CAUSAL_REL = \"CRel\"\n",
      "RESULT_REL = \"RRel\"\n",
      "CAUSE_RESULT = \"C->R\"\n",
      "\n",
      "cr_codes = [CAUSAL_REL, RESULT_REL, CAUSE_RESULT]\n",
      "tally = defaultdict(lambda: defaultdict(int))\n",
      "codes_per_row = []\n",
      "str_codes = []\n",
      "\n",
      "essay_ix = ix2essays[0]\n",
      "codes4essay = []\n",
      "maxpredCodes4essay = []\n",
      "\n",
      "# for each sentence\n",
      "for i,xs in ix2xs.items():\n",
      "    # i = sentence index\n",
      "    # xs = windows in sentence\n",
      "    if ix2essays[i] != essay_ix:\n",
      "        # if new essay, reset history of tags for essay\n",
      "        essay_ix = ix2essays[i]\n",
      "        codes4essay = []\n",
      "        maxpredCodes4essay = []\n",
      "\n",
      "    # COMPUTE XS\n",
      "    tmp_xs = []\n",
      "    tmp_ys = []\n",
      "    tmp_ys_by_code = defaultdict(list)\n",
      "\n",
      "    # add BOW features\n",
      "    #tmp_xs.extend(ix2vector[i])\n",
      "    \n",
      "    un_codes = set() # correct labels - YS\n",
      "    un_pred_codes = set() # predicted ys\n",
      "    s_codes = \"|\"\n",
      "    maxes = []\n",
      "    \n",
      "    for code in all_codes:\n",
      "        cls = code2cls[code]\n",
      "        pred = cls.decision_function(xs)\n",
      "        # add min and max values\n",
      "        mx = max(pred)\n",
      "        mn = min(pred)\n",
      "        diff = mx - mn\n",
      "        yes_no = max(cls.predict(xs))\n",
      "        \n",
      "        tmp_xs.append(mx)\n",
      "        maxes.append(mx)\n",
      "        tmp_xs.append(mn)\n",
      "        #tmp_xs.append(diff)\n",
      "        tmp_xs.append(yes_no)\n",
      "        \n",
      "        y_val = max(ix2ys[i][code])\n",
      "        tmp_ys_by_code[code] = np.array([y_val])\n",
      "        if y_val > 0:\n",
      "            un_codes.add(code)\n",
      "        \n",
      "        # full prediction\n",
      "        if yes_no > 0:\n",
      "            un_pred_codes.add(code)\n",
      "            s_codes += code + \"|\"\n",
      "    \n",
      "    # look at codes for previous sentence\n",
      "    # look back at most 2 items\n",
      "    look_back = min(2, len(codes4essay))\n",
      "    for jj in range(look_back):\n",
      "        index_offset = -1 -jj\n",
      "        fts = codes_2_features(codes4essay[index_offset])\n",
      "        tmp_xs.extend(fts)\n",
      "        #tmp_xs.extend(maxpredCodes4essay[index_offset])\n",
      "\n",
      "    # if less than 2, fill with 0's\n",
      "    for jj in range(2-look_back):\n",
      "        zeros = codes_2_features(set())\n",
      "        tmp_xs.extend(zeros)\n",
      "        #tmp_xs.extend(zeros)\n",
      "    \n",
      "    #end for each code\n",
      "    codes_per_row.append(un_pred_codes)\n",
      "    str_codes.append(s_codes)\n",
      "    \n",
      "    #add 2 way feature combos\n",
      "    for a in all_codes:\n",
      "        for b in all_codes:\n",
      "            if b < a:\n",
      "                if a in un_pred_codes and b in un_pred_codes:\n",
      "                    tmp_xs.append(1)\n",
      "                else:\n",
      "                    tmp_xs.append(0)\n",
      "            #if (\"|%s|%s|\" %(a,b)) in s_codes:\n",
      "            #    tmp_xs.append(1)\n",
      "            #else:\n",
      "            #    tmp_xs.append(0)            \n",
      "    \n",
      "    if len(un_pred_codes) > 0:\n",
      "        codes4essay.append(un_pred_codes)\n",
      "    #makes it worse\n",
      "    maxpredCodes4essay.append(maxes)\n",
      "    \n",
      "    # COMPUTE YS\n",
      "    tmp_ys_by_code[CAUSAL_REL]   = np.array([ 1 if  \"Causer\" in un_codes and \"explicit\" in un_codes else 0 ])\n",
      "    tmp_ys_by_code[RESULT_REL]   = np.array([ 1 if  \"Result\" in un_codes and \"explicit\" in un_codes else 0 ])\n",
      "    tmp_ys_by_code[CAUSE_RESULT] = np.array([ 1 if (\"Result\" in un_codes and \"explicit\" in un_codes and \"Causer\" in un_codes) else 0 ])\n",
      "    \n",
      "    for k,v in tmp_ys_by_code.items():\n",
      "        tally[k][max(v)] += 1\n",
      "        \n",
      "    ix2newxs[i] = np.array([tmp_xs])\n",
      "    ix2newys[i] = tmp_ys_by_code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals = ix2newxs.values()\n",
      "rlens = map(lambda a: a.shape[0], vals)\n",
      "clens = map(lambda a: a.shape[1], vals)\n",
      "assert min(clens) == max(clens), \"All xs' should be the same length\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Visualize how common multiple codes co-occur with C-R's \"\"\"\n",
      "for i,s in enumerate(str_codes):\n",
      "    print str(i).ljust(5), s\n",
      "    if i > 200:\n",
      "        break\n",
      "#print ix2newxs[15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0     |50|\n",
        "1     |50|\n",
        "2     |\n",
        "3     |\n",
        "4     |\n",
        "5     |1|\n",
        "6     |\n",
        "7     |\n",
        "8     |1|\n",
        "9     |\n",
        "10    |\n",
        "11    |3|\n",
        "12    |\n",
        "13    |4|\n",
        "14    |\n",
        "15    |7|50|Causer|Result|explicit|\n",
        "16    |50|\n",
        "17    |1|\n",
        "18    |2|\n",
        "19    |\n",
        "20    |\n",
        "21    |\n",
        "22    |\n",
        "23    |\n",
        "24    |\n",
        "25    |7|50|Causer|Result|\n",
        "26    |50|\n",
        "27    |7|50|Causer|Result|explicit|\n",
        "28    |50|\n",
        "29    |50|\n",
        "30    |7|50|Causer|\n",
        "31    |50|explicit|\n",
        "32    |50|\n",
        "33    |3|\n",
        "34    |\n",
        "35    |\n",
        "36    |\n",
        "37    |50|Causer|Result|\n",
        "38    |\n",
        "39    |\n",
        "40    |\n",
        "41    |\n",
        "42    |\n",
        "43    |50|\n",
        "44    |\n",
        "45    |\n",
        "46    |50|\n",
        "47    |3|\n",
        "48    |1|50|Causer|Result|explicit|\n",
        "49    |1|50|Causer|Result|explicit|\n",
        "50    |1|3|50|Causer|Result|explicit|\n",
        "51    |3|\n",
        "52    |2|3|Causer|Result|explicit|\n",
        "53    |1|3|Causer|\n",
        "54    |3|50|Causer|explicit|\n",
        "55    |50|explicit|\n",
        "56    |\n",
        "57    |explicit|\n",
        "58    |1|Causer|\n",
        "59    |\n",
        "60    |1|rhetorical|\n",
        "61    |3|Causer|\n",
        "62    |\n",
        "63    |1|50|\n",
        "64    |1|\n",
        "65    |\n",
        "66    |\n",
        "67    |3|50|Causer|Result|explicit|\n",
        "68    |50|\n",
        "69    |\n",
        "70    |50|\n",
        "71    |50|\n",
        "72    |50|\n",
        "73    |50|\n",
        "74    |50|\n",
        "75    |50|\n",
        "76    |50|\n",
        "77    |50|\n",
        "78    |50|\n",
        "79    |\n",
        "80    |\n",
        "81    |\n",
        "82    |\n",
        "83    |\n",
        "84    |7|50|Causer|\n",
        "85    |\n",
        "86    |7|50|Causer|Result|explicit|\n",
        "87    |\n",
        "88    |\n",
        "89    |\n",
        "90    |\n",
        "91    |50|\n",
        "92    |50|\n",
        "93    |\n",
        "94    |\n",
        "95    |\n",
        "96    |\n",
        "97    |\n",
        "98    |50|\n",
        "99    |\n",
        "100   |\n",
        "101   |\n",
        "102   |1|\n",
        "103   |50|Causer|Result|explicit|\n",
        "104   |\n",
        "105   |50|\n",
        "106   |7|50|Causer|Result|explicit|\n",
        "107   |\n",
        "108   |50|\n",
        "109   |\n",
        "110   |50|\n",
        "111   |\n",
        "112   |7|50|Result|explicit|\n",
        "113   |50|Result|\n",
        "114   |\n",
        "115   |\n",
        "116   |\n",
        "117   |\n",
        "118   |3|4|Causer|Result|\n",
        "119   |7|50|Result|\n",
        "120   |3|Causer|explicit|\n",
        "121   |1|\n",
        "122   |\n",
        "123   |\n",
        "124   |50|\n",
        "125   |50|Result|\n",
        "126   |50|\n",
        "127   |7|50|Causer|Result|explicit|\n",
        "128   |7|50|Causer|Result|explicit|\n",
        "129   |\n",
        "130   |\n",
        "131   |\n",
        "132   |50|\n",
        "133   |50|\n",
        "134   |\n",
        "135   |\n",
        "136   |\n",
        "137   |\n",
        "138   |50|\n",
        "139   |\n",
        "140   |50|\n",
        "141   |\n",
        "142   |3|\n",
        "143   |\n",
        "144   |\n",
        "145   |\n",
        "146   |50|\n",
        "147   |\n",
        "148   |\n",
        "149   |50|\n",
        "150   |\n",
        "151   |50|Result|\n",
        "152   |1|explicit|\n",
        "153   |3|\n",
        "154   |50|\n",
        "155   |1|3|\n",
        "156   |1|\n",
        "157   |3|\n",
        "158   |\n",
        "159   |4|50|Causer|explicit|\n",
        "160   |4|50|Causer|\n",
        "161   |50|rhetorical|\n",
        "162   |\n",
        "163   |\n",
        "164   |\n",
        "165   |\n",
        "166   |\n",
        "167   |\n",
        "168   |\n",
        "169   |50|\n",
        "170   |\n",
        "171   |3|50|Result|explicit|\n",
        "172   |\n",
        "173   |50|\n",
        "174   |50|\n",
        "175   |1|50|\n",
        "176   |50|\n",
        "177   |\n",
        "178   |1|50|Causer|Result|\n",
        "179   |\n",
        "180   |\n",
        "181   |\n",
        "182   |1|\n",
        "183   |\n",
        "184   |\n",
        "185   |50|Result|\n",
        "186   |\n",
        "187   |\n",
        "188   |50|\n",
        "189   |50|Result|explicit|\n",
        "190   |50|\n",
        "191   |50|\n",
        "192   |50|\n",
        "193   |50|\n",
        "194   |50|\n",
        "195   |50|\n",
        "196   |50|\n",
        "197   |\n",
        "198   |\n",
        "199   |\n",
        "200   |\n",
        "201   |50|\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Search for good C values\n",
      "\n",
      "dct = {}\n",
      "codes = cr_codes + [\"explicit\"] #[CAUSE_RESULT]\n",
      "for c in [1.0, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5]:\n",
      "    print \"C\", c\n",
      "    new_code2cls = train(codes, newx_t, yByCode_t, lambda : LinearSVC(C = float(c)))\n",
      "    dct[c] = test(codes, ixvalid, ix2newxs, ix2newys, new_code2cls)\n",
      "\n",
      "print \"\"\n",
      "for k,v in sorted(dct.items()):\n",
      "    print \"C\", str(k).ljust(5), \"Metric:\",v\n",
      "    \n",
      "\"\"\"\n",
      "None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_plus_cr = list(all_codes) + cr_codes\n",
      "newx_t, yByCode_t = extract_xs_ys(ixtest, ix2newxs, ix2newys, all_plus_cr)\n",
      "print newx_t[0].shape, \"features\"\n",
      "\n",
      "xsa = []\n",
      "ysa = []\n",
      "class platts_scaler(object):\n",
      "    def __init__(self, model):\n",
      "        self.model = model()\n",
      "        self.scaler = LogisticRegression()\n",
      "    \n",
      "    def fit(self, xs, ys):\n",
      "        self.model.fit(xs,ys)\n",
      "        pred = self.model.predict_proba(xs)\n",
      "        #self.scaler.fit(pred.reshape((len(pred),1)), ys)\n",
      "        self.scaler.fit(pred, ys)\n",
      "        \n",
      "    def predict(self, xs):\n",
      "        pred = self.model.predict_proba(xs)\n",
      "        #return self.scaler.predict(pred.reshape((len(pred),1)))\n",
      "        return self.scaler.predict(pred)\n",
      "    \n",
      "# SVM Is best when using joint features\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, \n",
      "                     #lambda: GradientBoostingClassifier(subsample=1.0, max_depth=3))\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda: RandomForestClassifier())\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, factMach)\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, LDA)\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda : LinearSVC(C=1.8))\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, LogisticRegression)\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, DecisionTreeClassifier)\n",
      "\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda : platts_scaler(lambda : LinearSVC(C=1.8)))\n",
      "#new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda : platts_scaler(lambda : RandomForestClassifier()))\n",
      "new_code2cls = train(cr_codes + [\"explicit\"], newx_t, yByCode_t, lambda : platts_scaler(lambda : GradientBoostingClassifier()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(290,) features\n",
        "Training for : CRel\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RRel\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "Training for :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = platts_scaler(lambda : LinearSVC(C=1.8))\n",
      "x = newx_t\n",
      "y = yByCode_t[\"50\"]\n",
      "\n",
      "#p.fit(x,y)\n",
      "x.shape, np.asarray(y).shape\n",
      "p.model.fit(x,y)\n",
      "pred = p.model.predict(x)\n",
      "p.scaler.fit(pred.reshape((len(pred),1)),y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test Data #2: \"\n",
      "metrics = test(cr_codes + [\"explicit\"], ixtest, ix2newxs, ix2newys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test Data #2: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CRel\n",
        "recall:     0.966981132075\n",
        "precision:  0.97619047619\n",
        "f1:         0.971563981043\n",
        "accuracy:   0.988483685221\n",
        "sentences:  424\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RRel\n",
        "recall:     0.962441314554\n",
        "precision:  0.980861244019\n",
        "f1:         0.971563981043\n",
        "accuracy:   0.988483685221\n",
        "sentences:  426\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "recall:     0.963503649635\n",
        "precision:  0.975369458128\n",
        "f1:         0.969400244798\n",
        "accuracy:   0.988003838772\n",
        "sentences:  411\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.968680089485\n",
        "precision:  0.977426636569\n",
        "f1:         0.973033707865\n",
        "accuracy:   0.988483685221\n",
        "sentences:  447\n",
        "\n",
        "<class '__main__.platts_scaler'> Recall: 0.9655, Precision: 0.9775, F1: 0.9714, Accuracy: 0.9884, Codes:  1708\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Validation Data #2: \"\n",
      "metrics = test(cr_codes + [\"explicit\"], ixvalid, ix2newxs, ix2newys, new_code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Validation Data #2: \n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CRel\n",
        "recall:     0.746835443038\n",
        "precision:  0.61780104712\n",
        "f1:         0.676217765043\n",
        "accuracy:   0.837410071942\n",
        "sentences:  158\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RRel\n",
        "recall:     0.748427672956\n",
        "precision:  0.623036649215\n",
        "f1:         0.68\n",
        "accuracy:   0.838848920863\n",
        "sentences:  159\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " C->R\n",
        "recall:     0.745222929936\n",
        "precision:  0.625668449198\n",
        "f1:         0.68023255814\n",
        "accuracy:   0.841726618705\n",
        "sentences:  157\n",
        "\n",
        "code:      "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " explicit\n",
        "recall:     0.774390243902\n",
        "precision:  0.622549019608\n",
        "f1:         0.690217391304\n",
        "accuracy:   0.835971223022\n",
        "sentences:  164\n",
        "\n",
        "<class '__main__.platts_scaler'> Recall: 0.7539, Precision: 0.6223, F1: 0.6817, Accuracy: 0.8385, Codes:   638\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Using Min\\Max Distance from Decision Plane\n",
      "  Single Features\n",
      "***\n",
      "<class 'LDA'> Recall: 0.6797, Precision: 0.6151, F1: 0.6458, Accuracy: 0.7784, Codes:   637\n",
      "***\n",
      "<class 'LinearSVC'> Recall: 0.6562, Precision: 0.6069, F1: 0.6297, Accuracy: 0.7704, Codes:   637\n",
      "<class 'logistic.LogisticRegression'> Recall: 0.6578, Precision: 0.6207, F1: 0.6379, Accuracy: 0.7784, Codes:   637\n",
      "<class 'GradientBoostingClassifier'> Recall: 0.6185, Precision: 0.6458, F1: 0.6288, Accuracy: 0.7835, Codes:   637\n",
      "<class 'DecisionTreeClassifier'> Recall: 0.5950, Precision: 0.6334, F1: 0.6118, Accuracy: 0.7755, Codes:   637\n",
      "    \n",
      "  Joint Features\n",
      "<class 'sklearn.lda.LDA'> Recall: 0.5667, Precision: 0.6260, F1: 0.5934, Accuracy: 0.7700, Codes:   637\n",
      "-- PREVIOUS BEST\n",
      "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.6703, Precision: 0.6323, F1: 0.6502, Accuracy: 0.7853, Codes:   637\n",
      "code:       C->R\n",
      "recall:     0.677215189873\n",
      "precision:  0.685897435897\n",
      "f1:         0.68152866242\n",
      "accuracy:   0.813432835821\n",
      "sentences:  158\n",
      "<class 'LogisticRegression'> Recall: 0.6342, Precision: 0.6539, F1: 0.6424, Accuracy: 0.7900, Codes:   637\n",
      "<class 'GradientBoostingClassifier'> Recall: 0.6075, Precision: 0.6396, F1: 0.6196, Accuracy: 0.7793, Codes:   637\n",
      "<class 'DecisionTreeClassifier'> Recall: 0.5651, Precision: 0.6236, F1: 0.5896, Accuracy: 0.7672, Codes:   637\n",
      "    \n",
      "<<<< New Data + Improved Sentence Parsing Logic in BrattEssay <<<<\n",
      "JOINT FEATURES + previous sentence's predicted labels (predictions not probs)\n",
      "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.6912, Precision: 0.6020, F1: 0.6396, Accuracy: 0.8206, Codes:   638\n",
      "code:       C->R\n",
      "recall:     0.732484076433\n",
      "precision:  0.605263157895\n",
      "f1:         0.662824207493\n",
      "accuracy:   0.831654676259\n",
      "sentences:  157\n",
      "    \n",
      "JOINT FEATURES + previous sentence's predicted probabilities (MUCH WORSE)\n",
      "<class 'sklearn.svm.classes.LinearSVC'> Recall: 0.6097, Precision: 0.6642, F1: 0.5974, Accuracy: 0.8214, Codes:   638\n",
      "code:       C->R\n",
      "recall:     0.324840764331\n",
      "precision:  0.822580645161\n",
      "f1:         0.465753424658\n",
      "accuracy:   0.831654676259\n",
      "sentences:  157\n",
      "\n",
      "************************************************************************************************\n",
      "JOINT FEATURES + previous sentence's predicted labels (predictions not probs) - default params\n",
      "<class 'GradientBoostingClassifier'> Recall: 0.7539, Precision: 0.6207, F1: 0.6808, Accuracy: 0.8377, Codes:   638\n",
      "************************************************************************************************"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*** TODO *** - Peter\\Simon 5.14.2014\n",
      "Match Explicit + Cause or Result (or both)\n",
      "Try reading this: http://ceur-ws.org/Vol-1109/paper4.pdf\n",
      "Read Peter's paper\n",
      "\n",
      "1. <s>Try removing commas and '\"''s and other punctuation</s>\n",
      "2. <s>Try skip gram features (Peter)</s>\n",
      "3. Dependency parse \n",
      "    - span of words for the explicit and the concept\n",
      "    - find any depencencies tha join those two groups\n",
      "    - dependency type\n",
      "        - NSUBJ or PREP_TO or CONJ_AND\n",
      "        - OR advmod, conj_and, dobj, prep_of, prep_in\n",
      "        - OR acomp,  advmod\n",
      "4. <s>Read Peter's latest paper </s>\n",
      "5. Read related papers - Semeval 2007 (or close), Rink et al, Girju et al, etc\n",
      "5. <s>Try training a second classifier based on the output of the first including the predictions for the previous and next word</s>\n",
      "6. <s>Add in sentence level features, such as BOW</s>\n",
      "7. <s>Do some cross validation</s>\n",
      "8. **Add in predicted codes from previous \\ next sentence** - recent sentence parsing improvements have decreased F1 score. Suggests that this information helps\n",
      "9. Try blending multiple window based classifiers (Log R, SVM, LDA, DT) using a regression model\n",
      "10. Try an ensemble of multiple classifiers trained on the same data, or subsets of it (think Kaggle)\n",
      "\"\"\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hits_misses_for_code(code, ixs, ixToXs, ixToYs, codeToClassifier):\n",
      "\n",
      "    cls = codeToClassifier[code]    \n",
      "    try:\n",
      "        cls.n_jobs = 1\n",
      "    except:\n",
      "        pass\n",
      "    \n",
      "    tp, tn, fp, fn = [], [], [], []\n",
      "    for ix in ixs:\n",
      "        xs = ixToXs[ix]\n",
      "        ysByCode = ixToYs[ix]\n",
      "        \n",
      "        ys = np.asarray(ysByCode[code])\n",
      "        ys = map(map_y, ys)\n",
      "        pred = cls.predict(xs)\n",
      "        \n",
      "        # Flatten predictions to sentence level by taking the max values\n",
      "        # over all windows\n",
      "        act_ys  = round(max(ys))\n",
      "        pred_ys = round(max(pred))\n",
      "        \n",
      "        sent = ix2sents[ix]\n",
      "        if pred_ys == 1.0:\n",
      "            if act_ys == pred_ys:\n",
      "                tp.append(sent)\n",
      "            else:\n",
      "                fp.append(sent)\n",
      "        else: # negative prediction\n",
      "            if act_ys == pred_ys:\n",
      "                tn.append(sent)\n",
      "            else:\n",
      "                fn.append(sent)\n",
      "            \n",
      "    return (tp, tn, fp, fn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examine some of the Errors Made in Tagging the Test Set Sentences\n",
      "-----------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tp, tn, fp, fn = hits_misses_for_code(CAUSE_RESULT, ixvalid, ix2newxs, ix2newys, new_code2cls)\n",
      "tp, tn, fp, fn = hits_misses_for_code(\"5\", ixvalid, ix2xs, ix2ys, code2cls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentence_text(sentence):\n",
      "    wds, codes = zip(*sentence)\n",
      "    return \" \".join([wd for wd in wds if wd != SENTENCE_START and wd != SENTENCE_END])\n",
      "\n",
      "print \"tp\"\n",
      "for i, sentence in enumerate(tp[:5]):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\"\n",
      "\n",
      "print \"fn\"\n",
      "for i, sentence in enumerate(fn):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\"\n",
      "\n",
      "print \"fp\"\n",
      "for i, sentence in enumerate(fp):\n",
      "    print str(i).ljust(3), get_sentence_text(sentence)\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tp\n",
        "0   coral and zooxanthellae algae have a very strong relationship with each other with that because in tropical oceans there are not enough nutrients to allow photosynthesis .\n",
        "1   as water temperature increases , the decrease in carbon dioxide creates a disruption in the process of photosynthesis\n",
        "2   when water temperatures increase coral can not do the process of photo synthesis . photo synthesis only in temperatures of 70 - 85f .\n",
        "3   the rates of coral bleaching vary from the amount light it recieve if a coral does not have enough light it will not be able to do photosynthesis .\n",
        "4   if it does not do photosynthesis it will not be able to get glucose to survive .\n",
        "\n",
        "fn\n",
        "0   coral live in shallow waters because they need light for photosynthesis for the zooxanthallae , when water temperature increases , the solubility or co2 in water decreases .\n",
        "1   this disruption threatens the delicate balance of photosynthesis .\n",
        "2   coral reefs need to go through photosynthesis to help survive .\n",
        "3   corals need temperatures between 70 - 85f in order to go through photosynthesis .\n",
        "4   if photosynthesis is not , the coral will bleach and die out .\n",
        "5   next , coral bleaching rates vary at different times because of photosynthesis for example during photosynthesis , zooxanthallae use energy from sunlight to combine carbon dioxide in the ocean with water .\n",
        "6   it also needs to be in clear shallow water that is o between 70 - 85 of , or else the photosynthesis will be unbalanced .\n",
        "7   the zo algae helps the coral go through photosynthesis .\n",
        "8   if the water is not clear then the algae , zo live in the coral tissues and also need light for photosynthesis .\n",
        "9   most coral feed off sugars made by algae that live among the corals polyps so they create sugars from sunlight .\n",
        "10  this disruption threatens the delicate balance required to keep coral healthy . \"\n",
        "11  coral bleaching happens by changes in temperature or not having zooxanthallae for a long time .\n",
        "12  this disruption threatens the delicate balance required to keep corals healthy .\n",
        "13  without photosynthesis , the corals would loose it color .\n",
        "\n",
        "fp\n",
        "0   the pass some of the food they make from the sun s energy to the coral .\n",
        "1   corals lose their color because of the ejection or death of the zooxanthellae algae , which passes food made by the sun energy to the coral .\n",
        "2   it is dependent on this because the zooxanthellae passes some of the food they make from the suns energy .\n",
        "3   since the decrease of carbon dioxide causes disruption in coral , the coral does not get enough nutrients to allow photosynthesis with zooxanthelle and when zooxanthelle is absent , it the coral from color and white , and eventually results in death .\n",
        "4   corals receive 50 - 95 % of their energy from photosynthesis .\n",
        "5   the zooxanthallae passes some of the food they make from the suns energy to the coral .\n",
        "6   because of the high temperature change zooxantheallae isn able to work properly and give the coral the sugar and oxygen needed .\n",
        "7   i believe coral bleaching rates vary at different times because of the environment its in , photosynthesis and zooxanthallae .\n",
        "8   during this process , carbon is passed from the algae to the coral in the form of glycerol or glucose .\n",
        "9   zo , according to the text , give \" some of the food they make from the sun energy to the coral \" , as well as giving corals their color .\n",
        "10  but , since the shift happened it changes the temperature and that up their photosynthesis process .\n",
        "11  the coral turns white because the zo passed some of the food energy made from the sun to the corals .\n",
        "12  coral bleaching rates vary at different times , such as in 1998 , when the coral breaking was at its highest , wind currents or shifting winds , the ability to create photosynthesis , and zooxanthallae algae , all contribute to why and how coral bleaching varies .\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Coral Bleaching Codelist\n",
      "\n",
      "1  \u2013 Decrease in trade winds\n",
      "2  \u2013 Warm water moving east\n",
      "3  \u2013 Increase in water temperature\n",
      "4  \u2013 Decrease in the solubility of CO2\n",
      "5  \u2013 Decrease/Disruption in the process of Photosynthesis\n",
      "6  \u2013 Coral stress\n",
      "7  \u2013 Ejection/Death of Algae\n",
      "11 \u2013 Storms/Rainfall\n",
      "12 \u2013 Increase in Freshwater\n",
      "13 \u2013 Decrease in Salinity\n",
      "50 \u2013 Coral Bleaching\n",
      "\n",
      "Causal Model\n",
      "1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> 50\n",
      "\n",
      "                      -> 6\n",
      "11 -> 12 -> 13"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    }
   ],
   "metadata": {}
  }
 ]
}